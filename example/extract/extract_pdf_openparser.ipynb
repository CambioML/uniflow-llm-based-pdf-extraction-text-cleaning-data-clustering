{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a content from an PDF into Markdown Format\n",
    "\n",
    "Below it's a simple example of using OpenParser to accurately extract content from a pdf file into markdown format.\n",
    "\n",
    "### 1. Load the libraries\n",
    "\n",
    "If you have install `open_parser`, uncomment the below line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install python-dotenv\n",
    "# !pip3 install --upgrade open_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up your OpenParser API key\n",
    "\n",
    "To set up your `CAMBIO_API_KEY` API key, you will:\n",
    "\n",
    "1. create a `.env` file in your root folder;\n",
    "2. add the following one line to your `.env file:\n",
    "    ```\n",
    "    CAMBIO_API_KEY=17b************************\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uniflow.flow.client import ExtractClient\n",
    "from uniflow.flow.config import ExtractPDFConfig\n",
    "from uniflow.op.model.model_config import OpenParserModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()\n",
    "pdf_file = \"1408.5882_page-1.pdf\"\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload response: 204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"filename\": input_file},\n",
    "]\n",
    "\n",
    "config = ExtractPDFConfig(\n",
    "    model_config=OpenParserModelConfig(\n",
    "        model_name = \"CambioML/open-parser\",\n",
    "    ),\n",
    ")\n",
    "openparser_client = ExtractClient(config)\n",
    "\n",
    "output = openparser_client.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'output': [{'text': ['# Convolutional Neural Networks for Sentence Classification',\n",
       "     'Yoon Kim\\nNew York University\\nyhk255@nyu.edu',\n",
       "     '## Abstract',\n",
       "     'We report on a series of experiments with\\nconvolutional neural networks (CNN)\\ntrained on top of pre-trained word vec-\\ntors for sentence-level classification tasks.\\nWe show that a simple CNN with lit-\\ntle hyperparameter tuning and static vec-\\ntors achieves excellent results on multi-\\nple benchmarks. Learning task-specific\\nvectors through fine-tuning offers further\\ngains in performance. We additionally\\npropose a simple modification to the ar-\\nchitecture to allow for the use of both\\ntask-specific and static vectors. The CNN\\nmodels discussed herein improve upon the\\nstate of the art on 4 out of 7 tasks, which\\ninclude sentiment analysis and question\\nclassification.',\n",
       "     '## 1 Introduction',\n",
       "     'Deep learning models have achieved remarkable\\nresults in computer vision (Krizhevsky et al.,\\n2012) and speech recognition (Graves et al., 2013)\\nin recent years. Within natural language process-\\ning, much of the work with deep learning meth-\\nods has involved learning word vector representa-\\ntions through neural language models (Bengio et\\nal., 2003; Yih et al., 2011; Mikolov et al., 2013)\\nand performing composition over the learned word\\nvectors for classification (Collobert et al., 2011).\\nWord vectors, wherein words are projected from a\\nsparse, 1-of-V encoding (here V is the vocabulary\\nsize) onto a lower dimensional vector space via a\\nhidden layer, are essentially feature extractors that\\nencode semantic features of words in their dimen-\\nsions. In such dense representations, semantically\\nclose words are likewise close-in euclidean or\\ncosine distance-in the lower dimensional vector\\nspace.',\n",
       "     'Convolutional neural networks (CNN) utilize\\nlayers with convolving filters that are applied to',\n",
       "     'local features (LeCun et al., 1998). Originally\\ninvented for computer vision, CNN models have\\nsubsequently been shown to be effective for NLP\\nand have achieved excellent results in semantic\\nparsing (Yih et al., 2014), search query retrieval\\n(Shen et al., 2014), sentence modeling (Kalch-\\nbrenner et al., 2014), and other traditional NLP\\ntasks (Collobert et al., 2011).',\n",
       "     \"In the present work, we train a simple CNN with\\none layer of convolution on top of word vectors\\nobtained from an unsupervised neural language\\nmodel. These vectors were trained by Mikolov et\\nal. (2013) on 100 billion words of Google News,\\nand are publicly available. We initially keep the\\nword vectors static and learn only the other param-\\neters of the model. Despite little tuning of hyper-\\nparameters, this simple model achieves excellent\\nresults on multiple benchmarks, suggesting that\\nthe pre-trained vectors are 'universal' feature ex-\\ntractors that can be utilized for various classifica-\\ntion tasks. Learning task-specific vectors through\\nfine-tuning results in further improvements. We\\nfinally describe a simple modification to the archi-\\ntecture to allow for the use of both pre-trained and\\ntask-specific vectors by having multiple channels.\",\n",
       "     'Our work is philosophically similar to Razavian\\net al. (2014) which showed that for image clas-\\nsification, feature extractors obtained from a pre-\\ntrained deep learning model perform well on a va-\\nriety of tasks-including tasks that are very dif-\\nferent from the original task for which the feature\\nextractors were trained.',\n",
       "     '## 2 Model',\n",
       "     'The model architecture, shown in figure 1, is a\\nslight variant of the CNN architecture of Collobert\\net al. (2011). Let X E Rk be the k-dimensional\\nword vector corresponding to the i-th word in the\\nsentence. A sentence of length n (padded where',\n",
       "     'https://code.google.com/p/word2vec/']}],\n",
       "  'root': <uniflow.node.Node at 0x2448f70b1c0>}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
