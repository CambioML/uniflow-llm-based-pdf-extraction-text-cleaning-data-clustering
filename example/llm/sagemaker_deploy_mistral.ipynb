{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Amazon SageMaker to deploy Mistral-7B-Instruct-v0.2 from the Hugging Face Hub\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need a valid [AWS CLI profile](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html) to run the code. You can set up the profile by running `aws configure --profile <profile_name>` in your terminal. You will need to provide your AWS Access Key ID and AWS Secret Access Key. You can find your AWS Access Key ID and AWS Secret Access Key in the [Security Credentials](https://console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials) section of the AWS console.\n",
    "\n",
    "```bash\n",
    "$ aws configure --profile <profile_name>\n",
    "$ AWS Access Key ID [None]: <your_access_key_id>\n",
    "$ AWS Secret Access Key [None]: <your_secret_access_key>\n",
    "$ Default region name [None]: us-west-2\n",
    "$ Default output format [None]: .json\n",
    "```\n",
    "\n",
    "We recommend using the default profile by executing the `aws configure` command. This notebook will utilize the default profile. Make sure to set `Default output format` to `.json`.\n",
    "\n",
    "> Note: If you don't have AWS CLI installed, you will get a `command not found: aws` error. You can follow the instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on how to deploy a model on Amazon SageMaker, you can refer to this document:\n",
    "\n",
    "https://huggingface.co/docs/sagemaker/inference#deploy-a-model-from-the--hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Extra Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -q boto3\n",
    "!{sys.executable} -m pip install -q huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependency\n",
    "First, we import libraries and create a boto3 session. We will use the default profile here, but you can also specify a profile name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(profile_name='default')\n",
    "region = \"us-west-2\"\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "datetime_timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "datetime_day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "s3_client = session.client(\"s3\", region_name=region)\n",
    "sm_client = session.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = session.client(\"sagemaker-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model\n",
    "We will download the model from the Hugging Face Hub. You can find the model in the [model hub](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2). For this example, we will use the `mistralai/Mistral-7B-Instruct-v0.2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/file_extraction/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_model_path = Path(\"./Mistral_7B_Instruct_model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_commit_hash = \"b70aa86578567ba3301b21c8a27bea4e8f6d6d61\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|██████████| 16/16 [00:00<00:00, 144631.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(repo_id=model_name, revision=model_commit_hash, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model to Amazon S3\n",
    "We will create a bucket in Amazon S3 and upload the model to the bucket. You can find more details on how to create a bucket in Amazon S3 [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket(bucket_name):\n",
    "    try:\n",
    "        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def upload_files_to_s3(local_model_directory, s3_bucket_name, s3_directory_prefix):\n",
    "    \"\"\"\n",
    "    Uploads all files in a local directory to an S3 bucket with a specified directory prefix.\n",
    "\n",
    "    Args:\n",
    "        local_model_directory (str): The local directory containing the files to be uploaded.\n",
    "        s3_bucket_name (str): The name of the S3 bucket.\n",
    "        s3_directory_prefix (str): The directory prefix to be added to the S3 file paths.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_files_paths = glob.glob(local_model_directory + \"/**/*\", recursive=True)\n",
    "\n",
    "    for file_path in all_files_paths:\n",
    "        if Path(file_path).is_dir():\n",
    "            continue\n",
    "\n",
    "        relative_file_path = file_path.replace(f\"{local_model_directory}/\", \"\")\n",
    "        s3_file_path = os.path.join(s3_directory_prefix, str(relative_file_path))\n",
    "\n",
    "        print(f\"Uploading {file_path} to {s3_file_path}\")\n",
    "        s3_client.upload_file(file_path, s3_bucket_name, s3_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_code_prefix: Uniflow/LLM/Mistral_7B_Instruct_code\n",
      "model_snapshot_path: Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61\n"
     ]
    }
   ],
   "source": [
    "s3_model_prefix = \"Uniflow/LLM/Mistral_7B_Instruct_model\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_code_prefix = \"Uniflow/LLM/Mistral_7B_Instruct_code\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/special_tokens_map.json to Uniflow/LLM/Mistral_7B_Instruct_model/special_tokens_map.json\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/pytorch_model.bin.index.json to Uniflow/LLM/Mistral_7B_Instruct_model/pytorch_model.bin.index.json\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/generation_config.json to Uniflow/LLM/Mistral_7B_Instruct_model/generation_config.json\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/config.json to Uniflow/LLM/Mistral_7B_Instruct_model/config.json\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/pytorch_model-00001-of-00003.bin to Uniflow/LLM/Mistral_7B_Instruct_model/pytorch_model-00001-of-00003.bin\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/README.md to Uniflow/LLM/Mistral_7B_Instruct_model/README.md\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/tokenizer_config.json to Uniflow/LLM/Mistral_7B_Instruct_model/tokenizer_config.json\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/pytorch_model-00002-of-00003.bin to Uniflow/LLM/Mistral_7B_Instruct_model/pytorch_model-00002-of-00003.bin\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/tokenizer.model to Uniflow/LLM/Mistral_7B_Instruct_model/tokenizer.model\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/model.safetensors.index.json to Uniflow/LLM/Mistral_7B_Instruct_model/model.safetensors.index.json\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/pytorch_model-00003-of-00003.bin to Uniflow/LLM/Mistral_7B_Instruct_model/pytorch_model-00003-of-00003.bin\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/model-00003-of-00003.safetensors to Uniflow/LLM/Mistral_7B_Instruct_model/model-00003-of-00003.safetensors\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/tokenizer.json to Uniflow/LLM/Mistral_7B_Instruct_model/tokenizer.json\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/model-00002-of-00003.safetensors to Uniflow/LLM/Mistral_7B_Instruct_model/model-00002-of-00003.safetensors\n",
      "Uploading Mistral_7B_Instruct_model/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61/model-00001-of-00003.safetensors to Uniflow/LLM/Mistral_7B_Instruct_model/model-00001-of-00003.safetensors\n"
     ]
    }
   ],
   "source": [
    "s3_bucket_name = f\"uniflow-llm-{account_id}-{region}\"\n",
    "create_s3_bucket(s3_bucket_name)\n",
    "upload_files_to_s3(str(model_snapshot_path), s3_bucket_name, s3_model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create role\n",
    "We will create an execution role that will be used by SageMaker to access AWS resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_role(role_name):\n",
    "    \"\"\"\n",
    "    Creates an IAM role for SageMaker deployment.\n",
    "\n",
    "    Parameters:\n",
    "    role_name (str): The name of the IAM role to be created.\n",
    "\n",
    "    Returns:\n",
    "    str: The ARN (Amazon Resource Name) of the created IAM role.\n",
    "    \"\"\"\n",
    "    iam_client = session.client(\"iam\")\n",
    "\n",
    "    # Check if role already exists\n",
    "    try:\n",
    "        get_role_response = iam_client.get_role(RoleName=role_name)\n",
    "        print(f\"IAM Role '{role_name}' already exists. Skipping creation.\")\n",
    "        return get_role_response[\"Role\"][\"Arn\"]\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        pass\n",
    "\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    create_role_response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "    )\n",
    "\n",
    "    attach_policy_response = iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n",
    "    )\n",
    "\n",
    "    attach_policy_response = iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\",\n",
    "    )\n",
    "\n",
    "    print(f\"IAM Role '{role_name}' created successfully!\")\n",
    "\n",
    "    role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "\n",
    "    return role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We name the role `UniflowSageMakerEndpointRole-v1` in this notebook. You can change it to your own role name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM Role 'UniflowSageMakerEndpointRole-v1' created successfully!\n"
     ]
    }
   ],
   "source": [
    "role_name = f\"UniflowSageMakerEndpointRole-v1\"\n",
    "role_arn = create_role(role_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model\n",
    "Next, we deploy the model to an endpoint. There will be 3 steps to this process:\n",
    "\n",
    "First, we create a model in SageMaker. This will be a reference to the model artifacts in S3.\n",
    "\n",
    "Second, we create an endpoint configuration. This will be a reference to the model in SageMaker.\n",
    "\n",
    "Third, we create an endpoint. SageMaker will spin up an instance to host the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deploying the model, we need to create model artifacts and inference code. We will create a `model.tar.gz` file that contains the inference code, requirements and serving properties. We will then upload it to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p Mistral_7B_Instruct_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Mistral_7B_Instruct_code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Mistral_7B_Instruct_code/model.py\n",
    "from djl_python import Input, Output\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_location, trust_remote_code=True)\n",
    "    model = model.eval().half()\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "generator = None\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    input_sentence = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    "    history = data.get(\"history\", [])\n",
    "    max_new_tokens = params.get(\"max_new_tokens\", 1000)\n",
    "    do_sample = params.get(\"do_sample\", True)\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": input_sentence})\n",
    "\n",
    "    if history:\n",
    "        messages.extend(history)\n",
    "    \n",
    "    outputs = Output()\n",
    "\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = encodeds.to(device)\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, do_sample=do_sample)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    result = {\"outputs\": decoded[0]}\n",
    "\n",
    "    outputs.add_as_json(result)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = [\n",
    "    \"transformers==4.36.2\",\n",
    "    \"accelerate==0.25.0\"\n",
    "]\n",
    "\n",
    "with open(\"Mistral_7B_Instruct_code/requirements.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(requirements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_properties = [\n",
    "    \"engine=Python\",\n",
    "    \"option.tensor_parallel_degree=1\",\n",
    "    f\"option.s3url=s3://{s3_bucket_name}/Uniflow/LLM/Mistral_7B_Instruct_model/\",\n",
    "]\n",
    "\n",
    "with open(\"Mistral_7B_Instruct_code/serving.properties\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(serving_properties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral_7B_Instruct_code/\n",
      "Mistral_7B_Instruct_code/requirements.txt\n",
      "Mistral_7B_Instruct_code/model.py\n",
      "Mistral_7B_Instruct_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz Mistral_7B_Instruct_code\n",
    "\n",
    "s3_client.upload_file(\"model.tar.gz\", s3_bucket_name, f\"{s3_code_prefix}/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading the model artifacts to Amazon S3, we will create a SageMaker model. We will then create an endpoint configuration and deploy the model to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = f\"Mistral-7B-Instruct-{datetime_timestamp}\"\n",
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    ")\n",
    "s3_code_artifact = f\"s3://{s3_bucket_name}/{s3_code_prefix}/model.tar.gz\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role_arn,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will create a SageMaker Endpoint Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.4xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 10*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(f\"Created Endpoint Config: {endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create a SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_endpoint_creation(sm_client, endpoint_name):\n",
    "    while True:\n",
    "        resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = resp[\"EndpointStatus\"]\n",
    "        \n",
    "        print(f\"Status: {status}\")\n",
    "        \n",
    "        if status != \"Creating\":\n",
    "            break\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    print(f\"Arn: {resp['EndpointArn']}\")\n",
    "    print(f\"Status: {status}\")\n",
    "\n",
    "wait_for_endpoint_creation(sm_client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke endpoint\n",
    "Finally, we invoke the endpoint with a sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint(endpoint_name, input_text):\n",
    "    \"\"\"\n",
    "    Invokes the SageMaker endpoint.\n",
    "\n",
    "    Args:\n",
    "        endpoint_name (str): The name of the SageMaker endpoint.\n",
    "        input_text (str): The input text to be processed by the endpoint.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the SageMaker endpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 128,\n",
    "    }\n",
    "\n",
    "    prompt = f\"{input_text}\"\n",
    "\n",
    "    payload = json.dumps({\"inputs\": prompt, \"parameters\": parameters})\n",
    "\n",
    "    response = sm_runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=payload\n",
    "    )\n",
    "\n",
    "    return json.loads(response[\"Body\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': \"<s> [INST] Tell me about Amazon SageMaker [/INST] Amazon SageMaker is a fully managed platform provided by Amazon Web Services (AWS) that allows developers and data scientists to build, train, and deploy machine learning models quickly and at scale. It provides various tools, algorithms, and pre-built templates to help you get started with your machine learning projects. Here are some key features and benefits of Amazon SageMaker:\\n\\n1. Fully managed: You don't need to manage any infrastructure, such as servers, storage, or networking, as Amazon SageMaker manages all of that for you.\\n2. Bring your own data: Sage\"}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Tell me about Amazon SageMaker\"\n",
    "response = invoke_endpoint(endpoint_name, input_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of this notebook. You can find more details on how to deploy a model on Amazon SageMaker [here](https://huggingface.co/docs/sagemaker/inference#deploy-a-model-from-the--hub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to delete the endpoint after you are done with this notebook. SageMaker endpoints are billed by the hour so you will incur charges if you forget to delete the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "file_extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
