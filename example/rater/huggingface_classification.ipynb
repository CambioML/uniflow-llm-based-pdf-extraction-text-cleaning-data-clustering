{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Evaluate Answer Completeness and Accuracy for Given Questions using Huggingface Open Source Models\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM)? In this example, we demonstrate how to use AutoRater for verifying the correctness of an answer to a specific question and its context, using open-source Huggingface models.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths, install and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "!{sys.executable} -m pip install -q transformers accelerate bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import (\n",
    "    RaterForClassificationHuggingfaceConfig,\n",
    "    HuggingfaceModelConfig,\n",
    ")\n",
    "from uniflow.op.prompt import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three example raw inputs. Each one is a tuple consisting of context, question, and answer to be labeled. The ground truth label of the first one is 'correct', and the others are 'incorrect'. Then, we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\",\n",
    "     \"What is the largest ocean on Earth?\",\n",
    "     \"The largest ocean on Earth is the Pacific Ocean.\"), # correct\n",
    "    (\"Shakespeare, a renowned English playwright and poet, wrote 39 plays during his lifetime. His works include famous plays like 'Hamlet' and 'Romeo and Juliet'.\",\n",
    "     \"How many plays did Shakespeare write?\",\n",
    "     \"Shakespeare wrote 31 plays.\"), # incorrect\n",
    "    (\"The human brain is an intricate organ responsible for intelligence, memory, and emotions. It is made up of approximately 86 billion neurons.\",\n",
    "     \"What is the human brain responsible for?\",\n",
    "     \"The human brain is responsible for physical movement.\"), # incorrect\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], answer=c[2])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using Mistral-7B-Instruct-v0.2\n",
    "\n",
    "In this example, we will use the Mistral-Instruct-7b model as the default LLM. If you want to use open-source models, you can replace with Huggingface models.\n",
    "\n",
    "We use the default `guided_prompt` in `RaterForClassificationHuggingfaceConfig`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the huggingface model. Configuration for the huggingeface model. Includes model_name(\"mistralai/Mistral-7B-Instruct-v0.2\"), model_server (\"HuggingfaceModelServer\"), batch_size (1), neuron (False), load_in_4bit (False), load_in_8bit (True), responese_start_key(\"exaplanation\"), response_format({\"type\": \"json_object\"})\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"Yes\": 1.0, \"No\": 0.0}.\n",
    "- `prompt_template` (GuidedPrompt): Template for guided prompts used in rating. Includes instructions for rating, along with examples that detail the context, question, answer, label, and explanation for each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: In `model_config`, `response_format` decides whether model generates a plain text or a json object. `response_start_key` is what you want model to first generate. Because we are using chain of thoughts (CoT) prompt in default, so the first generate field is `explanation` (see default `few_shot_prompt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'model_server': 'HuggingfaceModelServer', 'batch_size': 1, 'neuron': False, 'load_in_4bit': False, 'load_in_8bit': True, 'max_new_tokens': 768, 'do_sample': False, 'temperature': 0.0, 'num_beams': 1, 'num_return_sequences': 1, 'repetition_penalty': 1.2, 'response_start_key': 'explanation', 'response_format': {'type': 'json_object'}}, label2score={'Yes': 1.0, 'No': 0.0}, prompt_template=PromptTemplate(instruction=\"Evaluate if a given answer is appropriate based on the question and the context.\\n            Follow the format of the examples below, consisting of context, question, answer, explanation and label (you must choose one from ['Yes', 'No']).\", few_shot_prompt=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The answer is consistency to the fact that Eiffel Tower was constructed in 1889 mentioned in context, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells but not mitochondria indicated by answer, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "config = RaterForClassificationHuggingfaceConfig(\n",
    "    model_config=HuggingfaceModelConfig(\n",
    "        response_start_key=\"explanation\", \n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        batch_size=1\n",
    "    )\n",
    ")\n",
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 1.0,\n",
      "                            'majority_vote': 'yes',\n",
      "                            'samples': [{'answer': 'The largest ocean on Earth '\n",
      "                                                   'is the Pacific Ocean.',\n",
      "                                         'context': 'The Pacific Ocean is the '\n",
      "                                                    'largest and deepest of '\n",
      "                                                    \"Earth's oceanic \"\n",
      "                                                    'divisions. It extends '\n",
      "                                                    'from the Arctic Ocean in '\n",
      "                                                    'the north to the Southern '\n",
      "                                                    'Ocean in the south.',\n",
      "                                         'explanation': 'The answer is '\n",
      "                                                        'consistent with the '\n",
      "                                                        'fact stated in the '\n",
      "                                                        'context that the '\n",
      "                                                        'Pacific Ocean is the '\n",
      "                                                        'largest ocean on '\n",
      "                                                        'Earth, so the answer '\n",
      "                                                        'is correct.',\n",
      "                                         'label': 'Yes',\n",
      "                                         'question': 'What is the largest '\n",
      "                                                     'ocean on Earth?'}],\n",
      "                            'scores': [1.0],\n",
      "                            'votes': ['yes']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa14deb6020>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'no',\n",
      "                            'samples': [{'answer': 'Shakespeare wrote 31 '\n",
      "                                                   'plays.',\n",
      "                                         'context': 'Shakespeare, a renowned '\n",
      "                                                    'English playwright and '\n",
      "                                                    'poet, wrote 39 plays '\n",
      "                                                    'during his lifetime. His '\n",
      "                                                    'works include famous '\n",
      "                                                    \"plays like 'Hamlet' and \"\n",
      "                                                    \"'Romeo and Juliet'.\",\n",
      "                                         'explanation': 'The answer is '\n",
      "                                                        'inconsistent with the '\n",
      "                                                        'fact stated in the '\n",
      "                                                        'context that '\n",
      "                                                        'Shakespeare wrote 39 '\n",
      "                                                        'plays, so the answer '\n",
      "                                                        'is incorrect.',\n",
      "                                         'label': 'No',\n",
      "                                         'question': 'How many plays did '\n",
      "                                                     'Shakespeare write?'}],\n",
      "                            'scores': [0.0],\n",
      "                            'votes': ['no']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa27009f970>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'no',\n",
      "                            'samples': [{'answer': 'The human brain is '\n",
      "                                                   'responsible for physical '\n",
      "                                                   'movement.',\n",
      "                                         'context': 'The human brain is an '\n",
      "                                                    'intricate organ '\n",
      "                                                    'responsible for '\n",
      "                                                    'intelligence, memory, and '\n",
      "                                                    'emotions. It is made up '\n",
      "                                                    'of approximately 86 '\n",
      "                                                    'billion neurons.',\n",
      "                                         'explanation': 'The answer is '\n",
      "                                                        'inconsistent with the '\n",
      "                                                        'context which states '\n",
      "                                                        'that the human brain '\n",
      "                                                        'is responsible for '\n",
      "                                                        'intelligence, memory, '\n",
      "                                                        'and emotions, so the '\n",
      "                                                        'answer is incorrect.',\n",
      "                                         'label': 'No',\n",
      "                                         'question': 'What is the human brain '\n",
      "                                                     'responsible for?'}],\n",
      "                            'scores': [0.0],\n",
      "                            'votes': ['no']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa26184a440>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_score': 1.0,\n",
      " 'majority_vote': 'yes',\n",
      " 'samples': [{'answer': 'The largest ocean on Earth is the Pacific Ocean.',\n",
      "              'context': 'The Pacific Ocean is the largest and deepest of '\n",
      "                         \"Earth's oceanic divisions. It extends from the \"\n",
      "                         'Arctic Ocean in the north to the Southern Ocean in '\n",
      "                         'the south.',\n",
      "              'explanation': 'The answer is consistent with the fact stated in '\n",
      "                             'the context that the Pacific Ocean is the '\n",
      "                             'largest ocean on Earth, so the answer is '\n",
      "                             'correct.',\n",
      "              'label': 'Yes',\n",
      "              'question': 'What is the largest ocean on Earth?'}],\n",
      " 'scores': [1.0],\n",
      " 'votes': ['yes']}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31myes\u001b[0m and score \u001b[34m1.0\u001b[0m\n",
      "data 1 has label \u001b[31mno\u001b[0m and score \u001b[34m0.0\u001b[0m\n",
      "data 2 has label \u001b[31mno\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['response'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['response'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using Mistral-7B-Instruct-v0.2\n",
    "\n",
    "Following the previous settings, but we will change the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'model_server': 'HuggingfaceModelServer', 'batch_size': 1, 'neuron': False, 'load_in_4bit': False, 'load_in_8bit': True, 'max_new_tokens': 768, 'do_sample': False, 'temperature': 0.0, 'num_beams': 1, 'num_return_sequences': 1, 'repetition_penalty': 1.2, 'response_start_key': 'explanation', 'response_format': {'type': 'text'}}, label2score={'Yes': 1.0, 'No': 0.0}, prompt_template=PromptTemplate(instruction=\"Evaluate if a given answer is appropriate based on the question and the context.\\n            Follow the format of the examples below, consisting of context, question, answer, explanation and label (you must choose one from ['Yes', 'No']).\", few_shot_prompt=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The answer is consistency to the fact that Eiffel Tower was constructed in 1889 mentioned in context, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells but not mitochondria indicated by answer, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterForClassificationHuggingfaceConfig(\n",
    "    model_config=HuggingfaceModelConfig(\n",
    "        response_start_key=\"explanation\", \n",
    "        response_format={\"type\": \"text\"},\n",
    "        batch_size=1,\n",
    "    )\n",
    ")\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 1.0,\n",
      "                            'majority_vote': 'yes',\n",
      "                            'samples': ['instruction: Evaluate if a given '\n",
      "                                        'answer is appropriate based on the '\n",
      "                                        'question and the context.\\n'\n",
      "                                        '            Follow the format of the '\n",
      "                                        'examples below, consisting of '\n",
      "                                        'context, question, answer, '\n",
      "                                        'explanation and label (you must '\n",
      "                                        \"choose one from ['Yes', 'No']).\\n\"\n",
      "                                        'context: The Eiffel Tower, located in '\n",
      "                                        'Paris, France, is one of the most '\n",
      "                                        'famous landmarks in the world. It was '\n",
      "                                        'constructed in 1889 and stands at a '\n",
      "                                        'height of 324 meters.\\n'\n",
      "                                        'question: When was the Eiffel Tower '\n",
      "                                        'constructed?\\n'\n",
      "                                        'answer: The Eiffel Tower was '\n",
      "                                        'constructed in 1889.\\n'\n",
      "                                        'explanation: The answer is '\n",
      "                                        'consistency to the fact that Eiffel '\n",
      "                                        'Tower was constructed in 1889 '\n",
      "                                        'mentioned in context, so the answer '\n",
      "                                        'is correct.\\n'\n",
      "                                        'label: Yes\\n'\n",
      "                                        'context: Photosynthesis is a process '\n",
      "                                        'used by plants to convert light '\n",
      "                                        'energy into chemical energy. This '\n",
      "                                        'process primarily occurs in the '\n",
      "                                        'chloroplasts of plant cells.\\n'\n",
      "                                        'question: Where does photosynthesis '\n",
      "                                        'primarily occur in plant cells?\\n'\n",
      "                                        'answer: Photosynthesis primarily '\n",
      "                                        'occurs in the mitochondria of plant '\n",
      "                                        'cells.\\n'\n",
      "                                        'explanation: The context mentions '\n",
      "                                        'that photosynthesis primarily occurs '\n",
      "                                        'in the chloroplasts of plant cells '\n",
      "                                        'but not mitochondria indicated by '\n",
      "                                        'answer, so the answer is incorrect.\\n'\n",
      "                                        'label: No\\n'\n",
      "                                        'context: The Pacific Ocean is the '\n",
      "                                        \"largest and deepest of Earth's \"\n",
      "                                        'oceanic divisions. It extends from '\n",
      "                                        'the Arctic Ocean in the north to the '\n",
      "                                        'Southern Ocean in the south.\\n'\n",
      "                                        'question: What is the largest ocean '\n",
      "                                        'on Earth?\\n'\n",
      "                                        'answer: The largest ocean on Earth is '\n",
      "                                        'the Pacific Ocean. \\n'\n",
      "                                        'explanation: The answer is consistent '\n",
      "                                        'with the fact stated in the context '\n",
      "                                        'that the Pacific Ocean is the largest '\n",
      "                                        'ocean on Earth, so the answer is '\n",
      "                                        'correct.\\n'\n",
      "                                        '\\n'\n",
      "                                        'label: Yes'],\n",
      "                            'scores': [1.0],\n",
      "                            'votes': ['yes']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa0e02a6b60>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'no',\n",
      "                            'samples': ['instruction: Evaluate if a given '\n",
      "                                        'answer is appropriate based on the '\n",
      "                                        'question and the context.\\n'\n",
      "                                        '            Follow the format of the '\n",
      "                                        'examples below, consisting of '\n",
      "                                        'context, question, answer, '\n",
      "                                        'explanation and label (you must '\n",
      "                                        \"choose one from ['Yes', 'No']).\\n\"\n",
      "                                        'context: The Eiffel Tower, located in '\n",
      "                                        'Paris, France, is one of the most '\n",
      "                                        'famous landmarks in the world. It was '\n",
      "                                        'constructed in 1889 and stands at a '\n",
      "                                        'height of 324 meters.\\n'\n",
      "                                        'question: When was the Eiffel Tower '\n",
      "                                        'constructed?\\n'\n",
      "                                        'answer: The Eiffel Tower was '\n",
      "                                        'constructed in 1889.\\n'\n",
      "                                        'explanation: The answer is '\n",
      "                                        'consistency to the fact that Eiffel '\n",
      "                                        'Tower was constructed in 1889 '\n",
      "                                        'mentioned in context, so the answer '\n",
      "                                        'is correct.\\n'\n",
      "                                        'label: Yes\\n'\n",
      "                                        'context: Photosynthesis is a process '\n",
      "                                        'used by plants to convert light '\n",
      "                                        'energy into chemical energy. This '\n",
      "                                        'process primarily occurs in the '\n",
      "                                        'chloroplasts of plant cells.\\n'\n",
      "                                        'question: Where does photosynthesis '\n",
      "                                        'primarily occur in plant cells?\\n'\n",
      "                                        'answer: Photosynthesis primarily '\n",
      "                                        'occurs in the mitochondria of plant '\n",
      "                                        'cells.\\n'\n",
      "                                        'explanation: The context mentions '\n",
      "                                        'that photosynthesis primarily occurs '\n",
      "                                        'in the chloroplasts of plant cells '\n",
      "                                        'but not mitochondria indicated by '\n",
      "                                        'answer, so the answer is incorrect.\\n'\n",
      "                                        'label: No\\n'\n",
      "                                        'context: Shakespeare, a renowned '\n",
      "                                        'English playwright and poet, wrote 39 '\n",
      "                                        'plays during his lifetime. His works '\n",
      "                                        \"include famous plays like 'Hamlet' \"\n",
      "                                        \"and 'Romeo and Juliet'.\\n\"\n",
      "                                        'question: How many plays did '\n",
      "                                        'Shakespeare write?\\n'\n",
      "                                        'answer: Shakespeare wrote 31 plays. \\n'\n",
      "                                        'explanation: The answer is '\n",
      "                                        'inconsistent with the fact stated in '\n",
      "                                        'the context that Shakespeare wrote 39 '\n",
      "                                        'plays, so the answer is incorrect.\\n'\n",
      "                                        '\\n'\n",
      "                                        'label: No'],\n",
      "                            'scores': [0.0],\n",
      "                            'votes': ['no']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa0e02ec490>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'no',\n",
      "                            'samples': ['instruction: Evaluate if a given '\n",
      "                                        'answer is appropriate based on the '\n",
      "                                        'question and the context.\\n'\n",
      "                                        '            Follow the format of the '\n",
      "                                        'examples below, consisting of '\n",
      "                                        'context, question, answer, '\n",
      "                                        'explanation and label (you must '\n",
      "                                        \"choose one from ['Yes', 'No']).\\n\"\n",
      "                                        'context: The Eiffel Tower, located in '\n",
      "                                        'Paris, France, is one of the most '\n",
      "                                        'famous landmarks in the world. It was '\n",
      "                                        'constructed in 1889 and stands at a '\n",
      "                                        'height of 324 meters.\\n'\n",
      "                                        'question: When was the Eiffel Tower '\n",
      "                                        'constructed?\\n'\n",
      "                                        'answer: The Eiffel Tower was '\n",
      "                                        'constructed in 1889.\\n'\n",
      "                                        'explanation: The answer is '\n",
      "                                        'consistency to the fact that Eiffel '\n",
      "                                        'Tower was constructed in 1889 '\n",
      "                                        'mentioned in context, so the answer '\n",
      "                                        'is correct.\\n'\n",
      "                                        'label: Yes\\n'\n",
      "                                        'context: Photosynthesis is a process '\n",
      "                                        'used by plants to convert light '\n",
      "                                        'energy into chemical energy. This '\n",
      "                                        'process primarily occurs in the '\n",
      "                                        'chloroplasts of plant cells.\\n'\n",
      "                                        'question: Where does photosynthesis '\n",
      "                                        'primarily occur in plant cells?\\n'\n",
      "                                        'answer: Photosynthesis primarily '\n",
      "                                        'occurs in the mitochondria of plant '\n",
      "                                        'cells.\\n'\n",
      "                                        'explanation: The context mentions '\n",
      "                                        'that photosynthesis primarily occurs '\n",
      "                                        'in the chloroplasts of plant cells '\n",
      "                                        'but not mitochondria indicated by '\n",
      "                                        'answer, so the answer is incorrect.\\n'\n",
      "                                        'label: No\\n'\n",
      "                                        'context: The human brain is an '\n",
      "                                        'intricate organ responsible for '\n",
      "                                        'intelligence, memory, and emotions. '\n",
      "                                        'It is made up of approximately 86 '\n",
      "                                        'billion neurons.\\n'\n",
      "                                        'question: What is the human brain '\n",
      "                                        'responsible for?\\n'\n",
      "                                        'answer: The human brain is '\n",
      "                                        'responsible for physical movement. \\n'\n",
      "                                        'explanation: The answer is '\n",
      "                                        'inconsistent with the context which '\n",
      "                                        'states that the human brain is '\n",
      "                                        'responsible for intelligence, memory, '\n",
      "                                        'and emotions, so the answer is '\n",
      "                                        'incorrect.\\n'\n",
      "                                        'label: No'],\n",
      "                            'scores': [0.0],\n",
      "                            'votes': ['no']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa0e02a66b0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31myes\u001b[0m and score \u001b[34m1.0\u001b[0m\n",
      "data 1 has label \u001b[31mno\u001b[0m and score \u001b[34m0.0\u001b[0m\n",
      "data 2 has label \u001b[31mno\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['response'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['response'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
