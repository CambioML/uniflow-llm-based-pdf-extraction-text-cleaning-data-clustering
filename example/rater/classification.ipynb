{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Evaluate Answer Completeness and Accuracy for Given Questions\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM)? In this example, we demonstrate how to use AutoRater for verifying the correctness of an answer to a specific question and its context.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to create a `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import (\n",
    "    RaterForClassificationOpenAIGPT4Config,\n",
    "    RaterForClassificationOpenAIGPT3p5Config\n",
    ")\n",
    "from uniflow.op.prompt import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three example raw inputs. Each one is a tuple consisting of context, question, and answer to be labeled. The ground truth label of the first one is 'correct', and the others are 'incorrect'. Then, we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\",\n",
    "     \"What is the largest ocean on Earth?\",\n",
    "     \"The largest ocean on Earth is the Pacific Ocean.\"), # correct\n",
    "    (\"Shakespeare, a renowned English playwright and poet, wrote 39 plays during his lifetime. His works include famous plays like 'Hamlet' and 'Romeo and Juliet'.\",\n",
    "     \"How many plays did Shakespeare write?\",\n",
    "     \"Shakespeare wrote 31 plays.\"), # incorrect\n",
    "    (\"The human brain is an intricate organ responsible for intelligence, memory, and emotions. It is made up of approximately 86 billion neurons.\",\n",
    "     \"What is the human brain responsible for?\",\n",
    "     \"The human brain is responsible for physical movement.\"), # incorrect\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], answer=c[2])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using GPT4\n",
    "\n",
    "In this example, we will use the OpenAI GPT4 Model as the default LLM. If you want to use open-source models, you can replace with Huggingface models in the Uniflow.\n",
    "\n",
    "We use the default `guided_prompt` in `RaterForClassificationOpenAIGPT4Config`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the GPT-4 model. Includes model name (\"gpt-4\"), the server (\"OpenAIModelServer\"), number of calls (1), temperature (0), and the response format (plain text).\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"Yes\": 1.0, \"No\": 0.0}.\n",
    "- `prompt_template` (PromptTemplate): Template for guided prompts used in rating. Includes instructions for rating, along with examples that detail the context, question, answer, label, and explanation for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterForClassificationOpenAIGPT4Config(flow_name='RaterFlow',\n",
      "                                       model_config=OpenAIModelConfig(model_name='gpt-4',\n",
      "                                                                      model_server='OpenAIModelServer',\n",
      "                                                                      num_call=1,\n",
      "                                                                      temperature=0,\n",
      "                                                                      response_format={'type': 'text'}),\n",
      "                                       label2score={'No': 0.0, 'Yes': 1.0},\n",
      "                                       prompt_template=PromptTemplate(instruction=\"\\n            Evaluate the appropriateness of a given answer based on the question and the context.\\n            There are few examples below, consisting of context, question, answer, explanation and label.\\n            If answer is appropriate, you should give a label representing higher score and vise versa. Check label to score dictionary: [('Yes', 1.0), ('No', 0.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['Yes', 'No']).\\n            \", few_shot_prompt=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]),\n",
      "                                       num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterForClassificationOpenAIGPT4Config()\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the response format to be JSON, we need to update two aspects of the default config:\n",
    "1. Change the `model_name` to \"gpt-4-1106-preview\", which is the only GPT-4 model that supports the JSON format.\n",
    "1. Change the `response_format` to a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_config.model_name = \"gpt-4-1106-preview\"\n",
    "config.model_config.response_format = {\"type\": \"json_object\"}\n",
    "config.model_config.num_call = 1\n",
    "config.model_config.temperature = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 1, 'temperature': 0.0, 'response_format': {'type': 'json_object'}}, label2score={'Yes': 1.0, 'No': 0.0}, prompt_template=PromptTemplate(instruction=\"\\n            Evaluate the appropriateness of a given answer based on the question and the context.\\n            There are few examples below, consisting of context, question, answer, explanation and label.\\n            If answer is appropriate, you should give a label representing higher score and vise versa. Check label to score dictionary: [('Yes', 1.0), ('No', 0.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['Yes', 'No']).\\n            \", few_shot_prompt=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:12<00:00,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 1.0,\n",
      "                            'majority_vote': 'yes',\n",
      "                            'samples': [{'explanation': 'The context provided '\n",
      "                                                        'states that the '\n",
      "                                                        'Pacific Ocean is the '\n",
      "                                                        'largest and deepest '\n",
      "                                                        \"of Earth's oceanic \"\n",
      "                                                        'divisions, which '\n",
      "                                                        'directly answers the '\n",
      "                                                        'question. Therefore, '\n",
      "                                                        'the answer given is '\n",
      "                                                        'correct.',\n",
      "                                         'label': 'Yes'}],\n",
      "                            'scores': [1.0],\n",
      "                            'votes': ['yes']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f65989fa200>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'no',\n",
      "                            'samples': [{'explanation': 'The context states '\n",
      "                                                        'that Shakespeare '\n",
      "                                                        'wrote 39 plays during '\n",
      "                                                        'his lifetime, but the '\n",
      "                                                        'answer provided '\n",
      "                                                        'incorrectly states '\n",
      "                                                        'that he wrote 31 '\n",
      "                                                        'plays. Therefore, the '\n",
      "                                                        'answer is not '\n",
      "                                                        'appropriate.',\n",
      "                                         'label': 'No'}],\n",
      "                            'scores': [0.0],\n",
      "                            'votes': ['no']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f65989f9e70>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'no',\n",
      "                            'samples': [{'explanation': 'The context describes '\n",
      "                                                        'the human brain as '\n",
      "                                                        'responsible for '\n",
      "                                                        'intelligence, memory, '\n",
      "                                                        'and emotions. It does '\n",
      "                                                        'not mention physical '\n",
      "                                                        'movement, which is '\n",
      "                                                        'generally controlled '\n",
      "                                                        'by the motor cortex '\n",
      "                                                        'and other parts of '\n",
      "                                                        'the nervous system. '\n",
      "                                                        'Therefore, the answer '\n",
      "                                                        'is incorrect as it '\n",
      "                                                        'does not accurately '\n",
      "                                                        'reflect the functions '\n",
      "                                                        'listed in the '\n",
      "                                                        'context.',\n",
      "                                         'label': 'No'}],\n",
      "                            'scores': [0.0],\n",
      "                            'votes': ['no']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f65989fa440>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_score': 1.0,\n",
      " 'majority_vote': 'yes',\n",
      " 'samples': [{'explanation': 'The context provided states that the Pacific '\n",
      "                             \"Ocean is the largest and deepest of Earth's \"\n",
      "                             'oceanic divisions, which directly answers the '\n",
      "                             'question. Therefore, the answer given is '\n",
      "                             'correct.',\n",
      "              'label': 'Yes'}],\n",
      " 'scores': [1.0],\n",
      " 'votes': ['yes']}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only sample LLM once so the majority vote is the only label for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31myes\u001b[0m and score \u001b[34m1.0\u001b[0m\n",
      "data 1 has label \u001b[31mno\u001b[0m and score \u001b[34m0.0\u001b[0m\n",
      "data 2 has label \u001b[31mno\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['response'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['response'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using GPT3.5\n",
    "\n",
    "Following the previous settings, we will keep the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. Furthermore, we will change `num_call` to 3. This means the model will perform inference on each example three times, allowing us to take the majority vote of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'Yes': 1.0, 'No': 0.0}, prompt_template=PromptTemplate(instruction=\"\\n            # Task: Evaluate the appropriateness of a given answer based on a provided context and question.\\n            ## Input:\\n            1. context: A brief text containing key information.\\n            2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n            3. answer: A response to the question.\\n            ## Evaluation Criteria: If answer is appropriate, you should give a label representing higher score and vise versa. Check label to score dictionary: [('Yes', 1.0), ('No', 0.0)].\\n            ## Response Format: Your response should only include two fields below:\\n            1. explanation: Reasoning behind your judgment, explaining why the answer is appropriate or not.\\n            2. label: Your judgment (one of ['Yes', 'No']).\\n            ## Note: Use the below example only for demonstration, do not include in the final response.\\n            \", few_shot_prompt=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterForClassificationOpenAIGPT3p5Config()\n",
    "config2.model_config.num_call = 3\n",
    "config2.model_config.temperature = 0.9\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. The label is determined by taking the majority vote from three samples of the LLM's output, which improves stability and self-consistency compared to generating a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 1.0,\n",
      "                            'majority_vote': 'yes',\n",
      "                            'samples': ['explanation: The answer correctly '\n",
      "                                        'identifies the Pacific Ocean as the '\n",
      "                                        'largest ocean on Earth, which is '\n",
      "                                        'explicitly stated in the context.\\n'\n",
      "                                        'label: Yes',\n",
      "                                        'explanation: The context directly '\n",
      "                                        'states that the Pacific Ocean is the '\n",
      "                                        'largest ocean on Earth, so the answer '\n",
      "                                        'is correct.\\n'\n",
      "                                        'label: Yes',\n",
      "                                        'explanation: The answer correctly '\n",
      "                                        'identifies the Pacific Ocean as the '\n",
      "                                        'largest ocean on Earth, which is '\n",
      "                                        'explicitly stated in the context.\\n'\n",
      "                                        'label: Yes'],\n",
      "                            'scores': [1.0, 1.0, 1.0],\n",
      "                            'votes': ['yes', 'yes', 'yes']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f658372a260>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'no',\n",
      "                            'samples': ['\\n'\n",
      "                                        'explanation: The context explicitly '\n",
      "                                        'states that Shakespeare wrote 39 '\n",
      "                                        'plays, so the answer is incorrect.\\n'\n",
      "                                        'label: No',\n",
      "                                        'explanation: The context explicitly '\n",
      "                                        'states that Shakespeare wrote 39 '\n",
      "                                        'plays, so the answer is incorrect.\\n'\n",
      "                                        'label: No',\n",
      "                                        'explanation: The context clearly '\n",
      "                                        'states that Shakespeare wrote 39 '\n",
      "                                        'plays, so the answer is incorrect.\\n'\n",
      "                                        'label: No'],\n",
      "                            'scores': [0.0, 0.0, 0.0],\n",
      "                            'votes': ['no', 'no', 'no']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f6583688ee0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'no',\n",
      "                            'samples': ['explanation: The context states that '\n",
      "                                        'the human brain is responsible for '\n",
      "                                        'intelligence, memory, and emotions, '\n",
      "                                        'not just physical movement. '\n",
      "                                        'Therefore, the answer is incorrect.\\n'\n",
      "                                        'label: No',\n",
      "                                        'explanation: The context mentions '\n",
      "                                        'that the human brain is responsible '\n",
      "                                        'for intelligence, memory, and '\n",
      "                                        'emotions, not just physical movement. '\n",
      "                                        'The answer is not comprehensive and '\n",
      "                                        'therefore inappropriate.\\n'\n",
      "                                        'label: No',\n",
      "                                        '\\n'\n",
      "                                        'explanation: The context mentions '\n",
      "                                        'that the human brain is responsible '\n",
      "                                        'for intelligence, memory, and '\n",
      "                                        'emotions, but not specifically for '\n",
      "                                        'physical movement, so the answer is '\n",
      "                                        'incorrect.\\n'\n",
      "                                        'label: No'],\n",
      "                            'scores': [0.0, 0.0, 0.0],\n",
      "                            'votes': ['no', 'no', 'no']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f658372a680>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has major vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has major vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has major vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['response'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['response'][0]['average_score']\n",
    "    print(f\"data {idx} has major vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
