{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Compare Answers to Given Questions\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM) compared to a pre-fomulated answer? In this example, we demonstrate how to use AutoRater for verifying the correctness of a generated answers compared to the grounding answer in relation to given question and context.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import (\n",
    "    RaterForGeneratedAnswerOpenAIGPT4Config,\n",
    "    RaterForGeneratedAnswerOpenAIGPT3p5Config\n",
    ")\n",
    "from uniflow.op.prompt import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three sample raw inputs. Each one is a tuple consisting of context, question, ground truth answer and generated answer to be labeled. Then we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"Reddit is an American social news aggregation, content rating, and discussion website. Registered users submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members.\",\n",
    "     \"What type of content can users submit on Reddit?\",\n",
    "     \"Users can only post text on Reddit.\",\n",
    "     \"Users on Reddit can submit various types of content including links, text posts, images, and videos.\"), # Better\n",
    "    (\"League of Legends (LoL), commonly referred to as League, is a 2009 multiplayer online battle arena video game developed and published by Riot Games. \",\n",
    "     \"When was League of Legends released?\",\n",
    "     \"League of Legends was released in 2009.\",\n",
    "     \"League of Legends was released in the early 2000s.\"), # Worse\n",
    "    (\"Vitamin C (also known as ascorbic acid and ascorbate) is a water-soluble vitamin found in citrus and other fruits, berries and vegetables, also sold as a dietary supplement and as a topical serum ingredient to treat melasma (dark pigment spots) and wrinkles on the face.\",\n",
    "     \"Is Vitamin C water-soluble?\",\n",
    "     \"Yes, Vitamin C is a very water-soluble vitamin.\",\n",
    "     \"Yes, Vitamin C can be dissolved in water well.\"), # Equally good\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], grounding_answer=c[2], generated_answer=c[3])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using GPT4\n",
    "\n",
    "In this example, we will use the OpenAI GPT4 Model as the default LLM. If you want to use open-source models, you can replace with Huggingface models in the Uniflow.\n",
    "\n",
    "We use the default `prompt_template` in `RaterForGeneratedAnswerOpenAIGPT4Config`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the GPT-4 model. Includes model name (\"gpt-4\"), the server (\"OpenAIModelServer\"), number of calls (1), temperature (0), and the response format (plain text).\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"accept\": 1.0, \"equivalent\": 0.0, \"reject\": -1.0}.\n",
    "- `prompt_template` (PromptTemplate): Template for guided prompts used in rating. Includes instructions for rating, along with examples that detail the context, question, grounding answer, generated answer, label, and explanation for each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['equivalent', 'reject'] not in example label.\n",
      "RaterForGeneratedAnswerOpenAIGPT4Config(flow_name='RaterFlow',\n",
      "                                        model_config=OpenAIModelConfig(model_name='gpt-4',\n",
      "                                                                       model_server='OpenAIModelServer',\n",
      "                                                                       num_call=1,\n",
      "                                                                       temperature=0,\n",
      "                                                                       response_format={'type': 'text'}),\n",
      "                                        label2score={'accept': 1.0,\n",
      "                                                     'equivalent': 0.0,\n",
      "                                                     'reject': -1.0},\n",
      "                                        prompt_template=PromptTemplate(instruction=\"\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            There are few annotated examples below, consisting of context, question, grounding answer, generated answer, explanation and label.\\n            If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['accept', 'equivalent', 'reject']).\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators while grounding answer not. So we accept generated answer.', label='accept')]),\n",
      "                                        num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterForGeneratedAnswerOpenAIGPT4Config()\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the response format to be JSON, we need to update two aspects of the default config:\n",
    "1. Change the `model_name` to \"gpt-4-1106-preview\", which is the only GPT-4 model that supports the JSON format.\n",
    "1. Change the `response_format` to a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_config.model_name = \"gpt-4-1106-preview\"\n",
    "config.model_config.response_format = {\"type\": \"json_object\"}\n",
    "config.model_config.num_call = 1\n",
    "config.model_config.temperature = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes.\n",
    "\n",
    "NOTE: The printed information `\"The label2score label ['reject', 'equivalent'] not in example label.\"` is because we only pass one example (label=`accept`) in default `prompt_template` to reduce token consumption when using GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['equivalent', 'reject'] not in example label.\n",
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 1, 'temperature': 0.0, 'response_format': {'type': 'json_object'}}, label2score={'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0}, prompt_template=PromptTemplate(instruction=\"\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            There are few annotated examples below, consisting of context, question, grounding answer, generated answer, explanation and label.\\n            If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['accept', 'equivalent', 'reject']).\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators while grounding answer not. So we accept generated answer.', label='accept')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then, we can run the client. For each item in the raw input, the Client will generate an explanation and a final label in [`Accept`, `Equivalent`, `Reject`]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:26<00:00,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 1.0,\n",
      "                            'majority_vote': 'accept',\n",
      "                            'samples': [{'explanation': 'The grounding answer '\n",
      "                                                        'is incorrect as it '\n",
      "                                                        'states that users can '\n",
      "                                                        'only post text on '\n",
      "                                                        'Reddit, which '\n",
      "                                                        'contradicts the '\n",
      "                                                        'context provided that '\n",
      "                                                        'clearly states users '\n",
      "                                                        'can submit links, '\n",
      "                                                        'text posts, images, '\n",
      "                                                        'and videos. The '\n",
      "                                                        'generated answer '\n",
      "                                                        'accurately reflects '\n",
      "                                                        'the context by '\n",
      "                                                        'listing all the types '\n",
      "                                                        'of content that can '\n",
      "                                                        'be submitted on '\n",
      "                                                        'Reddit. Therefore, '\n",
      "                                                        'the generated answer '\n",
      "                                                        'is better.',\n",
      "                                         'label': 'accept'}],\n",
      "                            'scores': [1.0],\n",
      "                            'votes': ['accept']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f4f755e9ff0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': -1.0,\n",
      "                            'majority_vote': 'reject',\n",
      "                            'samples': [{'explanation': 'The grounding answer '\n",
      "                                                        'is better because it '\n",
      "                                                        'provides the exact '\n",
      "                                                        'release year (2009) '\n",
      "                                                        'for League of '\n",
      "                                                        'Legends, which is '\n",
      "                                                        'both specific and '\n",
      "                                                        'accurate according to '\n",
      "                                                        'the context provided. '\n",
      "                                                        'The generated answer '\n",
      "                                                        'is less precise, only '\n",
      "                                                        'stating that the game '\n",
      "                                                        'was released in the '\n",
      "                                                        \"'early 2000s,' which \"\n",
      "                                                        'could imply a range '\n",
      "                                                        'of years and is not '\n",
      "                                                        'as informative as the '\n",
      "                                                        'exact year.',\n",
      "                                         'label': 'reject'}],\n",
      "                            'scores': [-1.0],\n",
      "                            'votes': ['reject']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f4f755e9c60>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.0,\n",
      "                            'majority_vote': 'equivalent',\n",
      "                            'samples': [{'explanation': 'The generated answer '\n",
      "                                                        'is equivalent to the '\n",
      "                                                        'grounding answer as '\n",
      "                                                        'both correctly affirm '\n",
      "                                                        'that Vitamin C is '\n",
      "                                                        'water-soluble. There '\n",
      "                                                        'is no significant '\n",
      "                                                        'difference in the '\n",
      "                                                        'quality or content of '\n",
      "                                                        'the information '\n",
      "                                                        'provided.',\n",
      "                                         'label': 'equivalent'}],\n",
      "                            'scores': [0.0],\n",
      "                            'votes': ['equivalent']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f4f755ea260>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_score': 1.0,\n",
      " 'majority_vote': 'accept',\n",
      " 'samples': [{'explanation': 'The grounding answer is incorrect as it states '\n",
      "                             'that users can only post text on Reddit, which '\n",
      "                             'contradicts the context provided that clearly '\n",
      "                             'states users can submit links, text posts, '\n",
      "                             'images, and videos. The generated answer '\n",
      "                             'accurately reflects the context by listing all '\n",
      "                             'the types of content that can be submitted on '\n",
      "                             'Reddit. Therefore, the generated answer is '\n",
      "                             'better.',\n",
      "              'label': 'accept'}],\n",
      " 'scores': [1.0],\n",
      " 'votes': ['accept']}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only sample LLM once so the majority vote is the only label for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31maccept\u001b[0m and score \u001b[34m1.0\u001b[0m\n",
      "data 1 has label \u001b[31mreject\u001b[0m and score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has label \u001b[31mequivalent\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['response'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['response'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using GPT3.5\n",
    "\n",
    "Following the previous settings, we will keep the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. Furthermore, we will change `num_call` to 3. This means the model will perform inference on each example three times, allowing us to take the majority vote of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0}, prompt_template=PromptTemplate(instruction=\"\\n            # Task: Evaluate and compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            ## Input: A sample to be labeled:\\n            1. context: A brief text containing key information.\\n            2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n            3. grounding Answer: Pre-formulated, usually from human.\\n            4. generated Answer: From a language model.\\n            ## Evaluation Criteria: If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            ## Response Format: Your response should only include two fields below:\\n            1. explanatoin: Reasoning behind your judgment, detailing why the generated answer is better, equivalent or worse.\\n            2. label: Your judgment (one of ['accept', 'equivalent', 'reject']).\\n            ## Note:\\n            Only use the example below as a few shot demonstrate but not include them in the final response. Your response should only focus on the unlabeled sample.\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators.', label='accept'), Context(context='Operating systems(OS) did not exist in their modern and more complex forms until the early 1960s.', question='When did operating systems start to resemble their modern forms?', grounding_answer='Operating systems started to resemble their modern forms in the early 1960s.', generated_answer='Modern and more complex forms of operating systems began to emerge in the early 1960s.', explanation='The generated answer is as equally good as grounding answer because they both accurately pinpoint the early 1960s as the period when modern operating systems began to develop.', label='equivalent'), Context(context='Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing in the 1960s.', question='What features were added to hardware in the 1960s?', grounding_answer='Hardware in the 1960s saw the addition of features like runtime libraries and parallel processing.', generated_answer='The 1960s saw the addition of input output control and compatible timesharing capabilities in hardware.', explanation='The generated answer is worse because it inaccurately suggests the addition of capabilities of hardware in 1960s which is not supported by the context.', label='reject')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterForGeneratedAnswerOpenAIGPT3p5Config()\n",
    "config2.model_config.num_call = 3\n",
    "config2.model_config.temperature = 0.9\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the label is determined by taking the majority vote from three samples of the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 1.0,\n",
      "                            'majority_vote': 'accept',\n",
      "                            'samples': ['explanation: The generated answer is '\n",
      "                                        'better because it accurately '\n",
      "                                        'identifies the various types of '\n",
      "                                        'content that users can submit on '\n",
      "                                        'Reddit, which is consistent with the '\n",
      "                                        'information provided in the context.\\n'\n",
      "                                        'label: accept',\n",
      "                                        'explanation: The generated answer is '\n",
      "                                        'better because it accurately '\n",
      "                                        'identifies the various types of '\n",
      "                                        'content that users can submit on '\n",
      "                                        'Reddit, as described in the context.\\n'\n",
      "                                        'label: accept',\n",
      "                                        'explanation: The generated answer is '\n",
      "                                        'better because it accurately '\n",
      "                                        'identifies the different types of '\n",
      "                                        'content that users can submit on '\n",
      "                                        'Reddit, as mentioned in the context. '\n",
      "                                        'The grounding answer is incorrect in '\n",
      "                                        'stating that users can only post '\n",
      "                                        'text.\\n'\n",
      "                                        'label: accept'],\n",
      "                            'scores': [1.0, 1.0, 1.0],\n",
      "                            'votes': ['accept', 'accept', 'accept']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f4f75656f20>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': -1.0,\n",
      "                            'majority_vote': 'reject',\n",
      "                            'samples': ['explanation: The generated answer is '\n",
      "                                        'worse as it inaccurately states that '\n",
      "                                        'League of Legends was released in the '\n",
      "                                        'early 2000s, while the correct '\n",
      "                                        'release year is 2009 as provided in '\n",
      "                                        'the context.\\n'\n",
      "                                        'label: reject',\n",
      "                                        'explanation: The generated answer is '\n",
      "                                        'worse because it inaccurately states '\n",
      "                                        'that League of Legends was released '\n",
      "                                        'in the early 2000s, which contradicts '\n",
      "                                        'the provided context indicating a '\n",
      "                                        '2009 release.\\n'\n",
      "                                        'label: reject',\n",
      "                                        'explanation: The generated answer is '\n",
      "                                        'worse because it incorrectly states '\n",
      "                                        'that League of Legends was released '\n",
      "                                        'in the early 2000s, which contradicts '\n",
      "                                        'the information provided in the '\n",
      "                                        'context that it was released in '\n",
      "                                        '2009.\\n'\n",
      "                                        'label: reject'],\n",
      "                            'scores': [-1.0, -1.0, -1.0],\n",
      "                            'votes': ['reject', 'reject', 'reject']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f4f74493790>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [{'average_score': 0.6666666666666666,\n",
      "                            'majority_vote': 'accept',\n",
      "                            'samples': ['explanation: The generated answer is '\n",
      "                                        'equivalent because it correctly '\n",
      "                                        'confirms that Vitamin C is '\n",
      "                                        'water-soluble, matching the '\n",
      "                                        'information provided in the context.\\n'\n",
      "                                        'label: equivalent',\n",
      "                                        'explanation: The generated answer is '\n",
      "                                        'better because it correctly confirms '\n",
      "                                        'that Vitamin C is water-soluble, '\n",
      "                                        'which aligns with the information '\n",
      "                                        'provided in the context.\\n'\n",
      "                                        'label: accept',\n",
      "                                        'explanation: The generated answer is '\n",
      "                                        'better because it directly states '\n",
      "                                        'that Vitamin C can be dissolved in '\n",
      "                                        'water, which aligns with the fact '\n",
      "                                        'that it is a water-soluble vitamin.\\n'\n",
      "                                        'label: accept'],\n",
      "                            'scores': [0.0, 1.0, 1.0],\n",
      "                            'votes': ['equivalent', 'accept', 'accept']}]}],\n",
      "  'root': <uniflow.node.Node object at 0x7f4f75612d70>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from multiple LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has major vote \u001b[31maccept\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has major vote \u001b[31mreject\u001b[0m and average score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has major vote \u001b[31maccept\u001b[0m and average score \u001b[34m0.6666666666666666\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['response'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['response'][0]['average_score']\n",
    "    print(f\"data {idx} has major vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
