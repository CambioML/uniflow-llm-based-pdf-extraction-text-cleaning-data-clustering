{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Compare Answers to Given Questions\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM) compared to a pre-fomulated answer? In this example, we demonstrate how to use AutoRater for verifying the correctness of a generated answers compared to the grounding answer in relation to given question and context.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import (\n",
    "    RaterForGeneratedAnswerOpenAIGPT4Config,\n",
    "    RaterForGeneratedAnswerOpenAIGPT3p5Config\n",
    ")\n",
    "from uniflow.op.prompt import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three sample raw inputs. Each one is a tuple consisting of context, question, ground truth answer and generated answer to be labeled. Then we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"Reddit is an American social news aggregation, content rating, and discussion website. Registered users submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members.\",\n",
    "     \"What type of content can users submit on Reddit?\",\n",
    "     \"Users can only post text on Reddit.\",\n",
    "     \"Users on Reddit can submit various types of content including links, text posts, images, and videos.\"), # Better\n",
    "    (\"League of Legends (LoL), commonly referred to as League, is a 2009 multiplayer online battle arena video game developed and published by Riot Games. \",\n",
    "     \"When was League of Legends released?\",\n",
    "     \"League of Legends was released in 2009.\",\n",
    "     \"League of Legends was released in the early 2000s.\"), # Worse\n",
    "    (\"Vitamin C (also known as ascorbic acid and ascorbate) is a water-soluble vitamin found in citrus and other fruits, berries and vegetables, also sold as a dietary supplement and as a topical serum ingredient to treat melasma (dark pigment spots) and wrinkles on the face.\",\n",
    "     \"Is Vitamin C water-soluble?\",\n",
    "     \"Yes, Vitamin C is a very water-soluble vitamin.\",\n",
    "     \"Yes, Vitamin C can be dissolved in water well.\"), # Equally good\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], grounding_answer=c[2], generated_answer=c[3])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using GPT4\n",
    "\n",
    "In this example, we will use the OpenAI GPT4 Model as the default LLM. If you want to use open-source models, you can replace with Huggingface models in the Uniflow.\n",
    "\n",
    "We use the default `prompt_template` in `RaterForGeneratedAnswerOpenAIGPT4Config`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the GPT-4 model. Includes model name (\"gpt-4\"), the server (\"OpenAIModelServer\"), number of calls (1), temperature (0), and the response format (plain text).\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"accept\": 1.0, \"equivalent\": 0.0, \"reject\": -1.0}.\n",
    "- `prompt_template` (PromptTemplate): Template for guided prompts used in rating. Includes instructions for rating, along with examples that detail the context, question, grounding answer, generated answer, label, and explanation for each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['equivalent', 'reject'] not in example label.\n",
      "RaterForGeneratedAnswerOpenAIGPT4Config(flow_name='RaterFlow',\n",
      "                                        model_config=OpenAIModelConfig(model_name='gpt-4',\n",
      "                                                                       model_server='OpenAIModelServer',\n",
      "                                                                       num_call=1,\n",
      "                                                                       temperature=0,\n",
      "                                                                       response_format={'type': 'text'}),\n",
      "                                        label2score={'accept': 1.0,\n",
      "                                                     'equivalent': 0.0,\n",
      "                                                     'reject': -1.0},\n",
      "                                        prompt_template=PromptTemplate(instruction=\"\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            There are few annotated examples below, consisting of context, question, grounding answer, generated answer, explanation and label.\\n            If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['accept', 'equivalent', 'reject']).\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators while grounding answer not. So we accept generated answer.', label='accept')]),\n",
      "                                        num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterForGeneratedAnswerOpenAIGPT4Config()\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the response format to be JSON, we need to update two aspects of the default config:\n",
    "1. Change the `model_name` to \"gpt-4-1106-preview\", which is the only GPT-4 model that supports the JSON format.\n",
    "1. Change the `response_format` to a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_config.model_name = \"gpt-4-1106-preview\"\n",
    "config.model_config.response_format = {\"type\": \"json_object\"}\n",
    "config.model_config.num_call = 1\n",
    "config.model_config.temperature = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes.\n",
    "\n",
    "NOTE: The printed information `\"The label2score label ['reject', 'equivalent'] not in example label.\"` is because we only pass one example (label=`accept`) in default `prompt_template` to reduce token consumption when using GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['equivalent', 'reject'] not in example label.\n",
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 1, 'temperature': 0.0, 'response_format': {'type': 'json_object'}}, label2score={'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0}, prompt_template=PromptTemplate(instruction=\"\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            There are few annotated examples below, consisting of context, question, grounding answer, generated answer, explanation and label.\\n            If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['accept', 'equivalent', 'reject']).\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators while grounding answer not. So we accept generated answer.', label='accept')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then, we can run the client. For each item in the raw input, the Client will generate an explanation and a final label in [`Accept`, `Equivalent`, `Reject`]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:28<00:00,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'accept',\n",
      "              'response': [{'explanation': 'The grounding answer is incorrect '\n",
      "                                           'as it states that users can only '\n",
      "                                           'post text on Reddit, which '\n",
      "                                           'contradicts the context provided. '\n",
      "                                           'The generated answer correctly '\n",
      "                                           'lists the types of content that '\n",
      "                                           'can be submitted on Reddit, which '\n",
      "                                           'includes links, text posts, '\n",
      "                                           'images, and videos, as mentioned '\n",
      "                                           'in the context. Therefore, the '\n",
      "                                           'generated answer is better.',\n",
      "                            'label': 'accept'}],\n",
      "              'scores': [1.0],\n",
      "              'votes': ['accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f0601cf4100>},\n",
      " {'output': [{'average_score': -1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'reject',\n",
      "              'response': [{'explanation': 'The grounding answer is better '\n",
      "                                           'because it provides the exact '\n",
      "                                           'release year of League of Legends, '\n",
      "                                           'which is 2009, as stated in the '\n",
      "                                           'context. The generated answer is '\n",
      "                                           'less accurate as it vaguely states '\n",
      "                                           \"'early 2000s' which could imply \"\n",
      "                                           'any year from 2000 to 2005 and '\n",
      "                                           'does not provide the specific '\n",
      "                                           'information requested by the '\n",
      "                                           'question.',\n",
      "                            'label': 'reject'}],\n",
      "              'scores': [-1.0],\n",
      "              'votes': ['reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f061af03bb0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': [{'explanation': 'Both the grounding answer and the '\n",
      "                                           'generated answer correctly state '\n",
      "                                           'that Vitamin C is water-soluble. '\n",
      "                                           'The generated answer provides a '\n",
      "                                           \"synonymous phrase ('can be \"\n",
      "                                           \"dissolved in water well') for \"\n",
      "                                           \"'water-soluble', which conveys the \"\n",
      "                                           'same meaning as the grounding '\n",
      "                                           'answer. Therefore, the answers are '\n",
      "                                           'equivalent in terms of accuracy '\n",
      "                                           'and content.',\n",
      "                            'label': 'equivalent'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['equivalent']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f0601cf55d0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'The grounding answer is incorrect as it states that users can '\n",
      "                'only post text on Reddit, which contradicts the context '\n",
      "                'provided. The generated answer correctly lists the types of '\n",
      "                'content that can be submitted on Reddit, which includes '\n",
      "                'links, text posts, images, and videos, as mentioned in the '\n",
      "                'context. Therefore, the generated answer is better.',\n",
      " 'label': 'accept'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only sample LLM once so the majority vote is the only label for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31maccept\u001b[0m and score \u001b[34m1.0\u001b[0m\n",
      "data 1 has label \u001b[31mreject\u001b[0m and score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has label \u001b[31mequivalent\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using GPT3.5\n",
    "\n",
    "Following the previous settings, we will keep the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. Furthermore, we will change `num_call` to 3. This means the model will perform inference on each example three times, allowing us to take the majority vote of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0}, prompt_template=PromptTemplate(instruction=\"\\n            # Task: Evaluate and compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            ## Input: A sample to be labeled:\\n            1. context: A brief text containing key information.\\n            2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n            3. grounding Answer: Pre-formulated, usually from human.\\n            4. generated Answer: From a language model.\\n            ## Evaluation Criteria: If generated answer is better, you should give a label representing higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            ## Response Format: Your response should only include two fields below:\\n            1. explanatoin: Reasoning behind your judgment, detailing why the generated answer is better, equivalent or worse.\\n            2. label: Your judgment (one of ['accept', 'equivalent', 'reject']).\\n            ## Note:\\n            Only use the example below as a few shot demonstrate but not include them in the final response. Your response should only focus on the unlabeled sample.\\n            \", few_shot_prompt=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators.', label='accept'), Context(context='Operating systems(OS) did not exist in their modern and more complex forms until the early 1960s.', question='When did operating systems start to resemble their modern forms?', grounding_answer='Operating systems started to resemble their modern forms in the early 1960s.', generated_answer='Modern and more complex forms of operating systems began to emerge in the early 1960s.', explanation='The generated answer is as equally good as grounding answer because they both accurately pinpoint the early 1960s as the period when modern operating systems began to develop.', label='equivalent'), Context(context='Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing in the 1960s.', question='What features were added to hardware in the 1960s?', grounding_answer='Hardware in the 1960s saw the addition of features like runtime libraries and parallel processing.', generated_answer='The 1960s saw the addition of input output control and compatible timesharing capabilities in hardware.', explanation='The generated answer is worse because it inaccurately suggests the addition of capabilities of hardware in 1960s which is not supported by the context.', label='reject')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterForGeneratedAnswerOpenAIGPT3p5Config()\n",
    "config2.model_config.num_call = 3\n",
    "config2.model_config.temperature = 0.9\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the label is determined by taking the majority vote from three samples of the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'accept',\n",
      "              'response': ['explanation: The generated answer is better '\n",
      "                           'because it accurately identifies the various types '\n",
      "                           'of content that users can submit on Reddit, as '\n",
      "                           'described in the context.\\n'\n",
      "                           'label: accept',\n",
      "                           'explanation: The generated answer is better '\n",
      "                           'because it accurately identifies the various types '\n",
      "                           'of content that users can submit on Reddit, as '\n",
      "                           'stated in the context.\\n'\n",
      "                           'label: accept',\n",
      "                           'explanation: The generated answer is better '\n",
      "                           'because it accurately lists the various types of '\n",
      "                           'content that users can submit on Reddit, as stated '\n",
      "                           'in the context.\\n'\n",
      "                           'label: accept'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['accept', 'accept', 'accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f0600b98c40>},\n",
      " {'output': [{'average_score': -1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'reject',\n",
      "              'response': ['explanation: The generated answer is worse because '\n",
      "                           'it inaccurately states the release time of League '\n",
      "                           'of Legends as the early 2000s when the correct '\n",
      "                           'release year is 2009 as provided in the context.\\n'\n",
      "                           'label: reject',\n",
      "                           'explanation: The generated answer is worse because '\n",
      "                           'it inaccurately states the release date of League '\n",
      "                           'of Legends as the early 2000s, which contradicts '\n",
      "                           'the context explicitly stating that it was '\n",
      "                           'released in 2009.\\n'\n",
      "                           'label: reject',\n",
      "                           'explanation: The generated answer is worse because '\n",
      "                           'it inaccurately states that League of Legends was '\n",
      "                           'released in the early 2000s, which contradicts the '\n",
      "                           'provided context that clearly mentions its release '\n",
      "                           'in 2009.\\n'\n",
      "                           'label: reject'],\n",
      "              'scores': [-1.0, -1.0, -1.0],\n",
      "              'votes': ['reject', 'reject', 'reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f0601d2a020>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'reject',\n",
      "              'response': ['explanation: The grounding answer is better '\n",
      "                           'because it directly states that Vitamin C is \"very '\n",
      "                           'water-soluble,\" while the generated answer is more '\n",
      "                           'vague in saying that it \"can be dissolved in water '\n",
      "                           'well.\"\\n'\n",
      "                           'label: reject',\n",
      "                           'explanation: Both the grounding answer and the '\n",
      "                           'generated answer correctly state that Vitamin C is '\n",
      "                           'water-soluble, so they are equivalent.\\n'\n",
      "                           'label: equivalent',\n",
      "                           'explanation: The generated answer is better '\n",
      "                           'because it accurately states that Vitamin C is '\n",
      "                           'water-soluble, which aligns with the information '\n",
      "                           'provided in the context.\\n'\n",
      "                           'label: accept'],\n",
      "              'scores': [-1.0, 0.0, 1.0],\n",
      "              'votes': ['reject', 'equivalent', 'accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f0600b9a470>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from multiple LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31maccept\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mreject\u001b[0m and average score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mreject\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
