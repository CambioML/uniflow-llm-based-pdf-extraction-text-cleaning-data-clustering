{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from uniflow.transform.client import Client\n",
    "from uniflow.transform.config import TransformOpenAIConfig\n",
    "from uniflow.model.config import OpenAIModelConfig\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from uniflow.schema import Context, GuidedPrompt\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uniflow.op.transform.md_op import SplitMarkdownOp\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_file = \"README.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/uniflow/example/transform/README.md\n"
     ]
    }
   ],
   "source": [
    "dir_cur = os.getcwd()\n",
    "input_file = os.path.join(f\"{dir_cur}\", markdown_file)\n",
    "print(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = \"\"\n",
    "with open(input_file, 'r') as file:\n",
    "   markdown_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Examples\\n## Base Config\\nThe base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\n\\nHere are the default parameters for the `ModelConfig`:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\n\\n## OpenAIConfig\\nThe `OpenAIConfig` configuration runs the following default parameters:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\n\\nHere are the default parameters for the `OpenAIModelConfig`:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\n\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\n\\n## HuggingfaceConfig\\nThe `HuggingfaceConfig` configuration has the following default parameters:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\n\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\n\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\n\\n## LMQGModelConfig\\nThe `LMQGModelConfig` configuration runs with the following default parameters:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\n\\nHere are the default parameters for the `LMQGModelConfig`:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\n\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '## Base Config', 'metadata': {'Header 1': 'Examples'}},\n",
       " {'content': 'The base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\nThe base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\nThe base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\nThe base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\nHere are the default parameters for the `ModelConfig`:\\nHere are the default parameters for the `ModelConfig`:\\nHere are the default parameters for the `ModelConfig`:\\nHere are the default parameters for the `ModelConfig`:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\n## OpenAIConfig',\n",
       "  'metadata': {'Header 1': 'Examples', 'Header 2': 'Base Config'}},\n",
       " {'content': 'The `OpenAIConfig` configuration runs the following default parameters:\\nThe `OpenAIConfig` configuration runs the following default parameters:\\nThe `OpenAIConfig` configuration runs the following default parameters:\\nThe `OpenAIConfig` configuration runs the following default parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\nHere are the default parameters for the `OpenAIModelConfig`:\\nHere are the default parameters for the `OpenAIModelConfig`:\\nHere are the default parameters for the `OpenAIModelConfig`:\\nHere are the default parameters for the `OpenAIModelConfig`:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\n## HuggingfaceConfig',\n",
       "  'metadata': {'Header 1': 'Examples', 'Header 2': 'OpenAIConfig'}},\n",
       " {'content': 'The `HuggingfaceConfig` configuration has the following default parameters:\\nThe `HuggingfaceConfig` configuration has the following default parameters:\\nThe `HuggingfaceConfig` configuration has the following default parameters:\\nThe `HuggingfaceConfig` configuration has the following default parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\n## LMQGModelConfig',\n",
       "  'metadata': {'Header 1': 'Examples', 'Header 2': 'HuggingfaceConfig'}},\n",
       " {'content': 'The `LMQGModelConfig` configuration runs with the following default parameters:\\nThe `LMQGModelConfig` configuration runs with the following default parameters:\\nThe `LMQGModelConfig` configuration runs with the following default parameters:\\nThe `LMQGModelConfig` configuration runs with the following default parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\nHere are the default parameters for the `LMQGModelConfig`:\\nHere are the default parameters for the `LMQGModelConfig`:\\nHere are the default parameters for the `LMQGModelConfig`:\\nHere are the default parameters for the `LMQGModelConfig`:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.',\n",
       "  'metadata': {'Header 1': 'Examples', 'Header 2': 'LMQGModelConfig'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SplitMarkdownOp().header_splitter(markdown_str)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |  \\nHere are the default parameters for the `ModelConfig`:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |  \\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.', metadata={'Header 1': 'Examples', 'Header 2': 'Base Config'}),\n",
       " Document(page_content='The `OpenAIConfig` configuration runs the following default parameters:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |  \\nHere are the default parameters for the `OpenAIModelConfig`:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |  \\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.', metadata={'Header 1': 'Examples', 'Header 2': 'OpenAIConfig'}),\n",
       " Document(page_content='The `HuggingfaceConfig` configuration has the following default parameters:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |  \\nHere are the default parameters for the `HuggingfaceModelConfig`:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |  \\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.', metadata={'Header 1': 'Examples', 'Header 2': 'HuggingfaceConfig'}),\n",
       " Document(page_content='The `LMQGModelConfig` configuration runs with the following default parameters:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |  \\nHere are the default parameters for the `LMQGModelConfig`:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |  \\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.', metadata={'Header 1': 'Examples', 'Header 2': 'LMQGModelConfig'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_str)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# ðŸŒŠ uniflow\\n<p align=\"center\">\\n  <a href=\"/LICENSE\"><img alt=\"License Apache-2.0\" src=\"https://img.shields.io/github/license/cambioml/uniflow?style=flat-square\"></a>\\n  <a href=\"https://pypi.org/project/uniflow\"><img src=\"https://img.shields.io/pypi/v/uniflow.svg\" alt=\"pypi_status\" /></a>\\n  <a href=\"https://github.com/cambioml/uniflow/graphs/commit-activity\"><img alt=\"Commit activity\" src=\"https://img.shields.io/github/commit-activity/m/cambioml/uniflow?style=flat-square\"/></a>\\n  <a href=\"https://join.slack.com/t/cambiomlworkspace/shared_invite/zt-1zes33rmt-20Rag043uvExUaUdvt5_xQ\"><img src=\"https://badgen.net/badge/Join/Community/cyan?icon=slack\" alt=\"Slack\" /></a>\\n</p>\\n\\n`uniflow` is a unified interface to solve data augmentation problem for LLM training. It enables use of different LLMs, including [OpenAI](https://openai.com/product), [Huggingface](https://huggingface.co/mistralai/Mistral-7B-v0.1), and [LMQG](https://huggingface.co/lmqg) with a single interface. Using `uniflow`, you can easily run different LLMs to generate questions and answers, chunk text, summarize text, and more.\\n\\nBuilt by [CambioML](https://www.cambioml.com/).\\n\\n## Quick Install\\n\\n```\\npip3 install uniflow\\n```\\n\\nSee more details at the [full installation](#installation).\\n\\n## Overview\\nTo use `uniflow`, follow of three main steps:\\n1. **Pick a [`Config`](#config)**\\\\\\n    This determines the LLM and the different configurable parameters.\\n\\n1. **Construct your [`Prompts`](#prompting)**\\\\\\n    Construct the context that you want to use to prompt your model. You can configure custom instructions and examples using the [`GuidedPrompt`](#guidedprompt) class.\\n\\n1. **Run your [`Flow`](#running-the-flow)**\\\\\\n    Run the flow on your input data and generate output from your LLM.\\n\\n> *Note: We\\'re currently building have `Preprocessing` flows as well to help process data from different sources, such as `pdf`, `html`, `Markdown`, and more.*\\n\\n## 1. Config\\nThe `Config` determines which LLM is used and how the input data is serialized and deserialized. It also has parameters that are specific to the LLM.\\n\\nHere is a table of the different pre-defined configurations you can use and their corresponding LLMs:\\n| Config | LLM |\\n| ------------- | ------------- |\\n| __Config__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5) |\\n| __OpenAIConfig__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5)|\\n| __HuggingfaceConfig__| [`mistralai/Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| __LMQGConfig__ | [`lmqg/t5-base-squad-qg-ae`](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n\\nYou can run each config with the defaults, or you can pass in custom parameters, such as `temperature` or `batch_size` to the config for your use case. See the [advanced custom configuration](#advanced-custom-configuration) section for more details.\\n\\n## 2. Prompting\\nBy default, `uniflow` is set up to generate questions and answers based on the `Context` you pass in. To do so, it has a default instruction and few-shot examples that it uses to guide the LLM.\\n\\nHere is the default instruction:\\n```\\nGenerate one question and its corresponding answer based on the last context in the last example. Follow the format of the examples below to include context, question, and answer in the response\\n```\\n\\nHere are the default few-shot examples:\\n```\\n    context=\"The quick brown fox jumps over the lazy brown dog.\",\\n    question=\"What is the color of the fox?\",\\n    answer=\"brown.\"\\n\\n    context=\"The quick brown fox jumps over the lazy black dog.\",\\n    question=\"What is the color of the dog?\",\\n    answer=\"black.\"\\n```\\n\\nTo run with these default instructions and examples, all you need to do is pass in a list of  `Context` objects to the flow. `uniflow` will then generate a custom prompt with the instructions and few-shot examples for each `Context` object to send to the LLM. See the [Running the flow](#running-the-flow) section for more details.\\n\\n### Context\\nThe `Context` class is used to pass in the context for the LLM prompt. A `Context` consists of a `context` property, which is a string of text.\\n\\nTo run `uniflow` with the default instructions and few-shot examples, you can pass in a list of `Context` objects to the flow. For example:\\n```\\nfrom uniflow.schema import Context\\n\\ndata = [\\n    Context(\\n        context=\"The quick brown fox jumps over the lazy brown dog.\",\\n    ),\\n    ...\\n]\\n\\nclient.run(data)\\n```\\n\\nFor a more detailed overview of running the flow, see the [Running the flow](#running-the-flow) section.\\n\\n### GuidedPrompt\\nIf you want to run with a custom prompt instruction or few-shot examples, you can use the `GuidedPrompt` object. It has `instruction` and `example` properties.\\n\\n| Property | Type | Description |\\n| ------------- | ------------- | ------------- |\\n| `instruction` | str | Detailed instructions for the LLM |\\n| `examples` | List[Context] | The few-shot examples. |\\n\\nYou can overwrite any of the defaults as needed.\\n\\nTo see an example of how to use the `GuidedPrompt` to run `uniflow` with a custom `instruction`, few-shot examples, and custom `Context` fields to generate a summary, check out the [openai_pdf_source_10k_summary notebook](./example/model/openai_pdf_source_10k_summary.ipynb)\\n\\n\\n## Running the Flow\\nOnce you\\'ve decided on your `Config` and prompting strategy, you can run the flow on the input data.\\n\\n1. Import the `uniflow` `Client`, `Config`, and `Context` objects.\\n    ```\\n    from uniflow.transform.client import Client\\n    from uniflow.config import OpenAIConfig\\n    from uniflow.model.config import OpenAIModelConfig\\n    from uniflow.schema import Context\\n    ```\\n1. Preprocess your data in to chunks to pass into the flow. In the future we will have `Preprocessing` flows to help with this step, but for now you can use a library of your choice, like [pypdf](https://pypi.org/project/pypdf/), to chunk your data.\\n    ```\\n    raw_input_context = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\n    ```\\n\\n1. Create a list of `Context` objects to pass your data into the flow.\\n    ```\\n    data = [\\n        Context(context=c)\\n        for c in raw_input_context\\n    ]\\n    ```\\n\\n1. [Optional] If you want to use a customized instruction and/or examples, create a `GuidedPrompt`.\\n    ```\\n    from uniflow.schema import GuidedPrompt\\n\\n    guided_prompt = GuidedPrompt(\\n    instruction=\"Generate a one sentence summary based on the last context below. Follow the format of the examples below to include context and summary in the response\",\\n    examples=[\\n        Context(\\n            context=\"When you\\'re operating on the maker\\'s schedule, meetings are a disaster. A single meeting can blow a whole afternoon, by breaking it into two pieces each too small to do anything hard in. Plus you have to remember to go to the meeting. That\\'s no problem for someone on the manager\\'s schedule. There\\'s always something coming on the next hour; the only question is what. But when someone on the maker\\'s schedule has a meeting, they have to think about it.\",\\n            summary=\"Meetings disrupt the productivity of those following a maker\\'s schedule, dividing their time into impractical segments, while those on a manager\\'s schedule are accustomed to a continuous flow of tasks.\",\\n        ),\\n    ],\\n)\\n    ```\\n\\n1. Create a `Config` object to pass into the `Client` object.\\n    ```\\n    config = OpenAIConfig(\\n        guided_prompt_template=guided_prompt,\\n        model_config=OpenAIModelConfig(\\n            response_format={\"type\": \"json_object\"}\\n        ),\\n    )\\n    client = Client(config)\\n    ```\\n\\n1. Use the `client` object to run the flow on the input data.\\n\\n    ```\\n    output = client.run(data)\\n    ```\\n\\n1. Process the output data. By default, the LLM output will be a list of output dicts, one for each `Context` passed into the flow. Each dict has a `response` property which has the LLM response, as well as any errors. For example `output[0][\\'output\\'][0]` would look like this:\\n    ```\\n    {\\n        \\'response\\': [{\\'context\\': \\'It was a sunny day and the sky color is blue.\\',\\n        \\'question\\': \\'What was the color of the sky?\\',\\n        \\'answer\\': \\'blue.\\'}],\\n        \\'error\\': \\'No errors.\\'\\n    }\\n    ```\\n\\n## Examples\\n For more examples, see the [example](./example/model) folder.\\n\\n## Advanced Custom Configuration\\nYou can also configure the flows by passing custom configurations or arguments to the `Config` object if you want to further tune specific parameters like the the LLM model, number of threads, the temperature, and more.\\n\\nEvery configuration has the following parameters:\\n| Parameter | Type | Description |\\n| ------------- | ------------- | ------------- |\\n| `guided_prompt_template` | `GuidedPrompt` | The template to use for the guided prompt. |\\n| `num_threads` | int | The number of threads to use for the flow. |\\n| `model_config` | `ModelConfig` | The configuration to pass to the model. |\\n\\nYou can further configure the `model_config` by passing in one of the `Model Configs` with custom parameters.\\n\\n### Model Config\\nThe __Model Config__ is a configuration that is passed to the base `Config` object and determines which LLM model is used and has parameters that are specific to the LLM model.\\n\\n#### ModelConfig\\nThe base config is called `ModelConfig` and has the following parameters:\\n Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | gpt-3.5-turbo-1106 | [OpenAI site](https://platform.openai.com/docs/models/gpt-3-5) |\\n\\n#### OpenAIModelConfig\\nThe `OpenAIModelConfig` inherits from the `ModelConfig` and has the following additional parameters:\\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `num_calls` | int | 1 | The number of calls to make to the OpenAI API. |\\n| `temperature` | float | 1.5 | The temperature to use for the OpenAI API. |\\n| `response_format` | Dict[str, str] | {\"type\": \"text\"} | The response format to use for the OpenAI API. Can be \"text\" or \"json\" |\\n\\n#### HuggingfaceModelConfig\\nThe `HuggingfaceModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `mistralai/Mistral-7B-Instruct-v0.1` model by default.\\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | mistralai/Mistral-7B-Instruct-v0.1 | [Hugging Face site](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| `batch_size` | int | 1 | The batch size to use for the Hugging Face API. |\\n\\n#### LMQGModelConfig\\nThe `LMQGModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `lmqg/t5-base-squad-qg-ae` model by default.\\n\\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | lmqg/t5-base-squad-qg-ae | [Hugging Face site](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| `batch_size` | int | 1 | The batch size to use for the LMQG API. |\\n\\n### Custom Configuration Example\\nHere is an example of how to pass in a custom configuration to the `Client` object:\\n```\\nfrom uniflow.transform.client import Client\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\n\\ncontexts = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\n\\ndata = [\\n    Context(\\n        context=c\\n    )\\n    for c in contexts\\n]\\n\\nconfig = OpenAIConfig(\\n  num_threads=2,\\n  model_config=OpenAIModelConfig(\\n    model_name=\"gpt-4\",\\n    num_calls=2,\\n    temperature=0.5,\\n  ),\\n)\\nclient = Client(config)\\noutput = client.run(data)\\n```\\n\\nAs you can see, we are passing in a custom parameters to the `OpenAIModelConfig` to the `OpenAIConfig` configurations according to our needs.\\n\\n## Installation\\nTo get started with `uniflow`, you can install it using `pip` in a `conda` environment.\\n\\nFirst, create a conda environment on your terminal using:\\n```\\nconda create -n uniflow python=3.10 -y\\nconda activate uniflow  # some OS requires `source activate uniflow`\\n```\\n\\nNext, install the compatible pytorch based on your OS.\\n- If you are on a GPU, install [pytorch based on your cuda version](https://pytorch.org/get-started/locally/). You can find your CUDA version via `nvcc -V`.\\n    ```\\n    pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121  # cu121 means cuda 12.1\\n    ```\\n- If you are on a CPU instance,\\n    ```\\n    pip3 install torch\\n    ```\\n\\nThen, install `uniflow`:\\n```\\npip3 install uniflow\\n```\\n\\nIf you are running the `HuggingfaceModelFlow`, you will also need to install the `transformers`, `accelerate`, `bitsandbytes`, `scipy` libraries:\\n```\\npip3 install transformers accelerate bitsandbytes scipy\\n```\\n\\nFinally, if you are running the `HuggingfaceModelFlow`, you will also need to install the `lmqg` and `spacy` libraries:\\n```\\npip3 install lmqg spacy\\n```\\n\\nCongrats you have finished the installation!\\n\\n## Dev Setup\\nIf you are interested in contributing to us, here are the preliminary development setups.\\n\\n### API keys\\nIf you are running one of the following `OpenAI` flows, you will have to set up your OpenAI API key.\\n\\nTo do so, create a `.env` file in your root uniflow folder. Then add the following line to the `.env` file:\\n```\\nOPENAI_API_KEY=YOUR_API_KEY\\n```\\n### Backend Dev Setup\\n\\n```\\nconda create -n uniflow python=3.10\\nconda activate uniflow\\ncd uniflow\\npip3 install poetry\\npoetry install --no-root\\n```\\n\\n### EC2 Dev Setup\\nIf you are on EC2, you can launch a GPU instance with the following config:\\n- EC2 `g4dn.xlarge` (if you want to run a pretrained LLM with 7B parameters)\\n- Deep Learning AMI PyTorch GPU 2.0.1 (Ubuntu 20.04)\\n    <img src=\"example/image/readme_ec2_ami.jpg\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n- EBS: at least 100G\\n    <img src=\"example/image/readme_ec2_storage.png\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"../../\" + markdown_file\n",
    "markdown_str = \"\"\n",
    "with open(input_file, 'r') as file:\n",
    "   markdown_str = file.read()\n",
    "markdown_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '<p align=\"center\">\\n<p align=\"center\">\\n<p align=\"center\">\\n<p align=\"center\">\\n<a href=\"/LICENSE\"><img alt=\"License Apache-2.0\" src=\"https://img.shields.io/github/license/cambioml/uniflow?style=flat-square\"></a>\\n<a href=\"/LICENSE\"><img alt=\"License Apache-2.0\" src=\"https://img.shields.io/github/license/cambioml/uniflow?style=flat-square\"></a>\\n<a href=\"/LICENSE\"><img alt=\"License Apache-2.0\" src=\"https://img.shields.io/github/license/cambioml/uniflow?style=flat-square\"></a>\\n<a href=\"/LICENSE\"><img alt=\"License Apache-2.0\" src=\"https://img.shields.io/github/license/cambioml/uniflow?style=flat-square\"></a>\\n<a href=\"https://pypi.org/project/uniflow\"><img src=\"https://img.shields.io/pypi/v/uniflow.svg\" alt=\"pypi_status\" /></a>\\n<a href=\"https://pypi.org/project/uniflow\"><img src=\"https://img.shields.io/pypi/v/uniflow.svg\" alt=\"pypi_status\" /></a>\\n<a href=\"https://pypi.org/project/uniflow\"><img src=\"https://img.shields.io/pypi/v/uniflow.svg\" alt=\"pypi_status\" /></a>\\n<a href=\"https://pypi.org/project/uniflow\"><img src=\"https://img.shields.io/pypi/v/uniflow.svg\" alt=\"pypi_status\" /></a>\\n<a href=\"https://github.com/cambioml/uniflow/graphs/commit-activity\"><img alt=\"Commit activity\" src=\"https://img.shields.io/github/commit-activity/m/cambioml/uniflow?style=flat-square\"/></a>\\n<a href=\"https://github.com/cambioml/uniflow/graphs/commit-activity\"><img alt=\"Commit activity\" src=\"https://img.shields.io/github/commit-activity/m/cambioml/uniflow?style=flat-square\"/></a>\\n<a href=\"https://github.com/cambioml/uniflow/graphs/commit-activity\"><img alt=\"Commit activity\" src=\"https://img.shields.io/github/commit-activity/m/cambioml/uniflow?style=flat-square\"/></a>\\n<a href=\"https://github.com/cambioml/uniflow/graphs/commit-activity\"><img alt=\"Commit activity\" src=\"https://img.shields.io/github/commit-activity/m/cambioml/uniflow?style=flat-square\"/></a>\\n<a href=\"https://join.slack.com/t/cambiomlworkspace/shared_invite/zt-1zes33rmt-20Rag043uvExUaUdvt5_xQ\"><img src=\"https://badgen.net/badge/Join/Community/cyan?icon=slack\" alt=\"Slack\" /></a>\\n<a href=\"https://join.slack.com/t/cambiomlworkspace/shared_invite/zt-1zes33rmt-20Rag043uvExUaUdvt5_xQ\"><img src=\"https://badgen.net/badge/Join/Community/cyan?icon=slack\" alt=\"Slack\" /></a>\\n<a href=\"https://join.slack.com/t/cambiomlworkspace/shared_invite/zt-1zes33rmt-20Rag043uvExUaUdvt5_xQ\"><img src=\"https://badgen.net/badge/Join/Community/cyan?icon=slack\" alt=\"Slack\" /></a>\\n<a href=\"https://join.slack.com/t/cambiomlworkspace/shared_invite/zt-1zes33rmt-20Rag043uvExUaUdvt5_xQ\"><img src=\"https://badgen.net/badge/Join/Community/cyan?icon=slack\" alt=\"Slack\" /></a>\\n</p>\\n</p>\\n</p>\\n</p>\\n`uniflow` is a unified interface to solve data augmentation problem for LLM training. It enables use of different LLMs, including [OpenAI](https://openai.com/product), [Huggingface](https://huggingface.co/mistralai/Mistral-7B-v0.1), and [LMQG](https://huggingface.co/lmqg) with a single interface. Using `uniflow`, you can easily run different LLMs to generate questions and answers, chunk text, summarize text, and more.\\n`uniflow` is a unified interface to solve data augmentation problem for LLM training. It enables use of different LLMs, including [OpenAI](https://openai.com/product), [Huggingface](https://huggingface.co/mistralai/Mistral-7B-v0.1), and [LMQG](https://huggingface.co/lmqg) with a single interface. Using `uniflow`, you can easily run different LLMs to generate questions and answers, chunk text, summarize text, and more.\\n`uniflow` is a unified interface to solve data augmentation problem for LLM training. It enables use of different LLMs, including [OpenAI](https://openai.com/product), [Huggingface](https://huggingface.co/mistralai/Mistral-7B-v0.1), and [LMQG](https://huggingface.co/lmqg) with a single interface. Using `uniflow`, you can easily run different LLMs to generate questions and answers, chunk text, summarize text, and more.\\n`uniflow` is a unified interface to solve data augmentation problem for LLM training. It enables use of different LLMs, including [OpenAI](https://openai.com/product), [Huggingface](https://huggingface.co/mistralai/Mistral-7B-v0.1), and [LMQG](https://huggingface.co/lmqg) with a single interface. Using `uniflow`, you can easily run different LLMs to generate questions and answers, chunk text, summarize text, and more.\\nBuilt by [CambioML](https://www.cambioml.com/).\\nBuilt by [CambioML](https://www.cambioml.com/).\\nBuilt by [CambioML](https://www.cambioml.com/).\\nBuilt by [CambioML](https://www.cambioml.com/).\\n## Quick Install',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow'}},\n",
       " {'content': '```\\n```\\n```\\n```\\npip3 install uniflow\\npip3 install uniflow\\npip3 install uniflow\\npip3 install uniflow\\n```\\n```\\n```\\n```\\nSee more details at the [full installation](#installation).\\nSee more details at the [full installation](#installation).\\nSee more details at the [full installation](#installation).\\nSee more details at the [full installation](#installation).\\n## Overview',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Quick Install'}},\n",
       " {'content': \"To use `uniflow`, follow of three main steps:\\nTo use `uniflow`, follow of three main steps:\\nTo use `uniflow`, follow of three main steps:\\nTo use `uniflow`, follow of three main steps:\\n1. **Pick a [`Config`](#config)**\\\\\\n1. **Pick a [`Config`](#config)**\\\\\\n1. **Pick a [`Config`](#config)**\\\\\\n1. **Pick a [`Config`](#config)**\\\\\\nThis determines the LLM and the different configurable parameters.\\nThis determines the LLM and the different configurable parameters.\\nThis determines the LLM and the different configurable parameters.\\nThis determines the LLM and the different configurable parameters.\\n1. **Construct your [`Prompts`](#prompting)**\\\\\\n1. **Construct your [`Prompts`](#prompting)**\\\\\\n1. **Construct your [`Prompts`](#prompting)**\\\\\\n1. **Construct your [`Prompts`](#prompting)**\\\\\\nConstruct the context that you want to use to prompt your model. You can configure custom instructions and examples using the [`GuidedPrompt`](#guidedprompt) class.\\nConstruct the context that you want to use to prompt your model. You can configure custom instructions and examples using the [`GuidedPrompt`](#guidedprompt) class.\\nConstruct the context that you want to use to prompt your model. You can configure custom instructions and examples using the [`GuidedPrompt`](#guidedprompt) class.\\nConstruct the context that you want to use to prompt your model. You can configure custom instructions and examples using the [`GuidedPrompt`](#guidedprompt) class.\\n1. **Run your [`Flow`](#running-the-flow)**\\\\\\n1. **Run your [`Flow`](#running-the-flow)**\\\\\\n1. **Run your [`Flow`](#running-the-flow)**\\\\\\n1. **Run your [`Flow`](#running-the-flow)**\\\\\\nRun the flow on your input data and generate output from your LLM.\\nRun the flow on your input data and generate output from your LLM.\\nRun the flow on your input data and generate output from your LLM.\\nRun the flow on your input data and generate output from your LLM.\\n> *Note: We're currently building have `Preprocessing` flows as well to help process data from different sources, such as `pdf`, `html`, `Markdown`, and more.*\\n> *Note: We're currently building have `Preprocessing` flows as well to help process data from different sources, such as `pdf`, `html`, `Markdown`, and more.*\\n> *Note: We're currently building have `Preprocessing` flows as well to help process data from different sources, such as `pdf`, `html`, `Markdown`, and more.*\\n> *Note: We're currently building have `Preprocessing` flows as well to help process data from different sources, such as `pdf`, `html`, `Markdown`, and more.*\\n## 1. Config\",\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Overview'}},\n",
       " {'content': 'The `Config` determines which LLM is used and how the input data is serialized and deserialized. It also has parameters that are specific to the LLM.\\nThe `Config` determines which LLM is used and how the input data is serialized and deserialized. It also has parameters that are specific to the LLM.\\nThe `Config` determines which LLM is used and how the input data is serialized and deserialized. It also has parameters that are specific to the LLM.\\nThe `Config` determines which LLM is used and how the input data is serialized and deserialized. It also has parameters that are specific to the LLM.\\nHere is a table of the different pre-defined configurations you can use and their corresponding LLMs:\\nHere is a table of the different pre-defined configurations you can use and their corresponding LLMs:\\nHere is a table of the different pre-defined configurations you can use and their corresponding LLMs:\\nHere is a table of the different pre-defined configurations you can use and their corresponding LLMs:\\n| Config | LLM |\\n| Config | LLM |\\n| Config | LLM |\\n| Config | LLM |\\n| ------------- | ------------- |\\n| ------------- | ------------- |\\n| ------------- | ------------- |\\n| ------------- | ------------- |\\n| __Config__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5) |\\n| __Config__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5) |\\n| __Config__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5) |\\n| __Config__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5) |\\n| __OpenAIConfig__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5)|\\n| __OpenAIConfig__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5)|\\n| __OpenAIConfig__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5)|\\n| __OpenAIConfig__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5)|\\n| __HuggingfaceConfig__| [`mistralai/Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| __HuggingfaceConfig__| [`mistralai/Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| __HuggingfaceConfig__| [`mistralai/Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| __HuggingfaceConfig__| [`mistralai/Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| __LMQGConfig__ | [`lmqg/t5-base-squad-qg-ae`](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| __LMQGConfig__ | [`lmqg/t5-base-squad-qg-ae`](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| __LMQGConfig__ | [`lmqg/t5-base-squad-qg-ae`](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| __LMQGConfig__ | [`lmqg/t5-base-squad-qg-ae`](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\nYou can run each config with the defaults, or you can pass in custom parameters, such as `temperature` or `batch_size` to the config for your use case. See the [advanced custom configuration](#advanced-custom-configuration) section for more details.\\nYou can run each config with the defaults, or you can pass in custom parameters, such as `temperature` or `batch_size` to the config for your use case. See the [advanced custom configuration](#advanced-custom-configuration) section for more details.\\nYou can run each config with the defaults, or you can pass in custom parameters, such as `temperature` or `batch_size` to the config for your use case. See the [advanced custom configuration](#advanced-custom-configuration) section for more details.\\nYou can run each config with the defaults, or you can pass in custom parameters, such as `temperature` or `batch_size` to the config for your use case. See the [advanced custom configuration](#advanced-custom-configuration) section for more details.\\n## 2. Prompting',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow', 'Header 2': '1. Config'}},\n",
       " {'content': 'By default, `uniflow` is set up to generate questions and answers based on the `Context` you pass in. To do so, it has a default instruction and few-shot examples that it uses to guide the LLM.\\nBy default, `uniflow` is set up to generate questions and answers based on the `Context` you pass in. To do so, it has a default instruction and few-shot examples that it uses to guide the LLM.\\nBy default, `uniflow` is set up to generate questions and answers based on the `Context` you pass in. To do so, it has a default instruction and few-shot examples that it uses to guide the LLM.\\nBy default, `uniflow` is set up to generate questions and answers based on the `Context` you pass in. To do so, it has a default instruction and few-shot examples that it uses to guide the LLM.\\nHere is the default instruction:\\nHere is the default instruction:\\nHere is the default instruction:\\nHere is the default instruction:\\n```\\n```\\n```\\n```\\nGenerate one question and its corresponding answer based on the last context in the last example. Follow the format of the examples below to include context, question, and answer in the response\\nGenerate one question and its corresponding answer based on the last context in the last example. Follow the format of the examples below to include context, question, and answer in the response\\nGenerate one question and its corresponding answer based on the last context in the last example. Follow the format of the examples below to include context, question, and answer in the response\\nGenerate one question and its corresponding answer based on the last context in the last example. Follow the format of the examples below to include context, question, and answer in the response\\n```\\n```\\n```\\n```\\nHere are the default few-shot examples:\\nHere are the default few-shot examples:\\nHere are the default few-shot examples:\\nHere are the default few-shot examples:\\n```\\n```\\n```\\n```\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\nquestion=\"What is the color of the fox?\",\\nquestion=\"What is the color of the fox?\",\\nquestion=\"What is the color of the fox?\",\\nquestion=\"What is the color of the fox?\",\\nanswer=\"brown.\"\\nanswer=\"brown.\"\\nanswer=\"brown.\"\\nanswer=\"brown.\"\\ncontext=\"The quick brown fox jumps over the lazy black dog.\",\\ncontext=\"The quick brown fox jumps over the lazy black dog.\",\\ncontext=\"The quick brown fox jumps over the lazy black dog.\",\\ncontext=\"The quick brown fox jumps over the lazy black dog.\",\\nquestion=\"What is the color of the dog?\",\\nquestion=\"What is the color of the dog?\",\\nquestion=\"What is the color of the dog?\",\\nquestion=\"What is the color of the dog?\",\\nanswer=\"black.\"\\nanswer=\"black.\"\\nanswer=\"black.\"\\nanswer=\"black.\"\\n```\\n```\\n```\\n```\\nTo run with these default instructions and examples, all you need to do is pass in a list of  `Context` objects to the flow. `uniflow` will then generate a custom prompt with the instructions and few-shot examples for each `Context` object to send to the LLM. See the [Running the flow](#running-the-flow) section for more details.\\nTo run with these default instructions and examples, all you need to do is pass in a list of  `Context` objects to the flow. `uniflow` will then generate a custom prompt with the instructions and few-shot examples for each `Context` object to send to the LLM. See the [Running the flow](#running-the-flow) section for more details.\\nTo run with these default instructions and examples, all you need to do is pass in a list of  `Context` objects to the flow. `uniflow` will then generate a custom prompt with the instructions and few-shot examples for each `Context` object to send to the LLM. See the [Running the flow](#running-the-flow) section for more details.\\nTo run with these default instructions and examples, all you need to do is pass in a list of  `Context` objects to the flow. `uniflow` will then generate a custom prompt with the instructions and few-shot examples for each `Context` object to send to the LLM. See the [Running the flow](#running-the-flow) section for more details.\\n### Context\\n### Context',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow', 'Header 2': '2. Prompting'}},\n",
       " {'content': 'The `Context` class is used to pass in the context for the LLM prompt. A `Context` consists of a `context` property, which is a string of text.\\nThe `Context` class is used to pass in the context for the LLM prompt. A `Context` consists of a `context` property, which is a string of text.\\nThe `Context` class is used to pass in the context for the LLM prompt. A `Context` consists of a `context` property, which is a string of text.\\nThe `Context` class is used to pass in the context for the LLM prompt. A `Context` consists of a `context` property, which is a string of text.\\nTo run `uniflow` with the default instructions and few-shot examples, you can pass in a list of `Context` objects to the flow. For example:\\nTo run `uniflow` with the default instructions and few-shot examples, you can pass in a list of `Context` objects to the flow. For example:\\nTo run `uniflow` with the default instructions and few-shot examples, you can pass in a list of `Context` objects to the flow. For example:\\nTo run `uniflow` with the default instructions and few-shot examples, you can pass in a list of `Context` objects to the flow. For example:\\n```\\n```\\n```\\n```\\nfrom uniflow.schema import Context\\nfrom uniflow.schema import Context\\nfrom uniflow.schema import Context\\nfrom uniflow.schema import Context\\ndata = [\\ndata = [\\ndata = [\\ndata = [\\nContext(\\nContext(\\nContext(\\nContext(\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\n),\\n),\\n),\\n),\\n...\\n...\\n...\\n...\\n]\\n]\\n]\\n]\\nclient.run(data)\\nclient.run(data)\\nclient.run(data)\\nclient.run(data)\\n```\\n```\\n```\\n```\\nFor a more detailed overview of running the flow, see the [Running the flow](#running-the-flow) section.\\nFor a more detailed overview of running the flow, see the [Running the flow](#running-the-flow) section.\\nFor a more detailed overview of running the flow, see the [Running the flow](#running-the-flow) section.\\nFor a more detailed overview of running the flow, see the [Running the flow](#running-the-flow) section.\\n### GuidedPrompt\\n### GuidedPrompt',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': '2. Prompting',\n",
       "   'Header 3': 'Context'}},\n",
       " {'content': 'If you want to run with a custom prompt instruction or few-shot examples, you can use the `GuidedPrompt` object. It has `instruction` and `example` properties.\\nIf you want to run with a custom prompt instruction or few-shot examples, you can use the `GuidedPrompt` object. It has `instruction` and `example` properties.\\nIf you want to run with a custom prompt instruction or few-shot examples, you can use the `GuidedPrompt` object. It has `instruction` and `example` properties.\\nIf you want to run with a custom prompt instruction or few-shot examples, you can use the `GuidedPrompt` object. It has `instruction` and `example` properties.\\n| Property | Type | Description |\\n| Property | Type | Description |\\n| Property | Type | Description |\\n| Property | Type | Description |\\n| ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- |\\n| `instruction` | str | Detailed instructions for the LLM |\\n| `instruction` | str | Detailed instructions for the LLM |\\n| `instruction` | str | Detailed instructions for the LLM |\\n| `instruction` | str | Detailed instructions for the LLM |\\n| `examples` | List[Context] | The few-shot examples. |\\n| `examples` | List[Context] | The few-shot examples. |\\n| `examples` | List[Context] | The few-shot examples. |\\n| `examples` | List[Context] | The few-shot examples. |\\nYou can overwrite any of the defaults as needed.\\nYou can overwrite any of the defaults as needed.\\nYou can overwrite any of the defaults as needed.\\nYou can overwrite any of the defaults as needed.\\nTo see an example of how to use the `GuidedPrompt` to run `uniflow` with a custom `instruction`, few-shot examples, and custom `Context` fields to generate a summary, check out the [openai_pdf_source_10k_summary notebook](./example/model/openai_pdf_source_10k_summary.ipynb)\\nTo see an example of how to use the `GuidedPrompt` to run `uniflow` with a custom `instruction`, few-shot examples, and custom `Context` fields to generate a summary, check out the [openai_pdf_source_10k_summary notebook](./example/model/openai_pdf_source_10k_summary.ipynb)\\nTo see an example of how to use the `GuidedPrompt` to run `uniflow` with a custom `instruction`, few-shot examples, and custom `Context` fields to generate a summary, check out the [openai_pdf_source_10k_summary notebook](./example/model/openai_pdf_source_10k_summary.ipynb)\\nTo see an example of how to use the `GuidedPrompt` to run `uniflow` with a custom `instruction`, few-shot examples, and custom `Context` fields to generate a summary, check out the [openai_pdf_source_10k_summary notebook](./example/model/openai_pdf_source_10k_summary.ipynb)\\n## Running the Flow',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': '2. Prompting',\n",
       "   'Header 3': 'GuidedPrompt'}},\n",
       " {'content': 'Once you\\'ve decided on your `Config` and prompting strategy, you can run the flow on the input data.\\nOnce you\\'ve decided on your `Config` and prompting strategy, you can run the flow on the input data.\\nOnce you\\'ve decided on your `Config` and prompting strategy, you can run the flow on the input data.\\nOnce you\\'ve decided on your `Config` and prompting strategy, you can run the flow on the input data.\\n1. Import the `uniflow` `Client`, `Config`, and `Context` objects.\\n1. Import the `uniflow` `Client`, `Config`, and `Context` objects.\\n1. Import the `uniflow` `Client`, `Config`, and `Context` objects.\\n1. Import the `uniflow` `Client`, `Config`, and `Context` objects.\\n```\\n```\\n```\\n```\\nfrom uniflow.transform.client import Client\\nfrom uniflow.transform.client import Client\\nfrom uniflow.transform.client import Client\\nfrom uniflow.transform.client import Client\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\nfrom uniflow.schema import Context\\nfrom uniflow.schema import Context\\nfrom uniflow.schema import Context\\nfrom uniflow.schema import Context\\n```\\n```\\n```\\n```\\n1. Preprocess your data in to chunks to pass into the flow. In the future we will have `Preprocessing` flows to help with this step, but for now you can use a library of your choice, like [pypdf](https://pypi.org/project/pypdf/), to chunk your data.\\n1. Preprocess your data in to chunks to pass into the flow. In the future we will have `Preprocessing` flows to help with this step, but for now you can use a library of your choice, like [pypdf](https://pypi.org/project/pypdf/), to chunk your data.\\n1. Preprocess your data in to chunks to pass into the flow. In the future we will have `Preprocessing` flows to help with this step, but for now you can use a library of your choice, like [pypdf](https://pypi.org/project/pypdf/), to chunk your data.\\n1. Preprocess your data in to chunks to pass into the flow. In the future we will have `Preprocessing` flows to help with this step, but for now you can use a library of your choice, like [pypdf](https://pypi.org/project/pypdf/), to chunk your data.\\n```\\n```\\n```\\n```\\nraw_input_context = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\nraw_input_context = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\nraw_input_context = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\nraw_input_context = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\n```\\n```\\n```\\n```\\n1. Create a list of `Context` objects to pass your data into the flow.\\n1. Create a list of `Context` objects to pass your data into the flow.\\n1. Create a list of `Context` objects to pass your data into the flow.\\n1. Create a list of `Context` objects to pass your data into the flow.\\n```\\n```\\n```\\n```\\ndata = [\\ndata = [\\ndata = [\\ndata = [\\nContext(context=c)\\nContext(context=c)\\nContext(context=c)\\nContext(context=c)\\nfor c in raw_input_context\\nfor c in raw_input_context\\nfor c in raw_input_context\\nfor c in raw_input_context\\n]\\n]\\n]\\n]\\n```\\n```\\n```\\n```\\n1. [Optional] If you want to use a customized instruction and/or examples, create a `GuidedPrompt`.\\n1. [Optional] If you want to use a customized instruction and/or examples, create a `GuidedPrompt`.\\n1. [Optional] If you want to use a customized instruction and/or examples, create a `GuidedPrompt`.\\n1. [Optional] If you want to use a customized instruction and/or examples, create a `GuidedPrompt`.\\n```\\n```\\n```\\n```\\nfrom uniflow.schema import GuidedPrompt\\nfrom uniflow.schema import GuidedPrompt\\nfrom uniflow.schema import GuidedPrompt\\nfrom uniflow.schema import GuidedPrompt\\nguided_prompt = GuidedPrompt(\\nguided_prompt = GuidedPrompt(\\nguided_prompt = GuidedPrompt(\\nguided_prompt = GuidedPrompt(\\ninstruction=\"Generate a one sentence summary based on the last context below. Follow the format of the examples below to include context and summary in the response\",\\ninstruction=\"Generate a one sentence summary based on the last context below. Follow the format of the examples below to include context and summary in the response\",\\ninstruction=\"Generate a one sentence summary based on the last context below. Follow the format of the examples below to include context and summary in the response\",\\ninstruction=\"Generate a one sentence summary based on the last context below. Follow the format of the examples below to include context and summary in the response\",\\nexamples=[\\nexamples=[\\nexamples=[\\nexamples=[\\nContext(\\nContext(\\nContext(\\nContext(\\ncontext=\"When you\\'re operating on the maker\\'s schedule, meetings are a disaster. A single meeting can blow a whole afternoon, by breaking it into two pieces each too small to do anything hard in. Plus you have to remember to go to the meeting. That\\'s no problem for someone on the manager\\'s schedule. There\\'s always something coming on the next hour; the only question is what. But when someone on the maker\\'s schedule has a meeting, they have to think about it.\",\\ncontext=\"When you\\'re operating on the maker\\'s schedule, meetings are a disaster. A single meeting can blow a whole afternoon, by breaking it into two pieces each too small to do anything hard in. Plus you have to remember to go to the meeting. That\\'s no problem for someone on the manager\\'s schedule. There\\'s always something coming on the next hour; the only question is what. But when someone on the maker\\'s schedule has a meeting, they have to think about it.\",\\ncontext=\"When you\\'re operating on the maker\\'s schedule, meetings are a disaster. A single meeting can blow a whole afternoon, by breaking it into two pieces each too small to do anything hard in. Plus you have to remember to go to the meeting. That\\'s no problem for someone on the manager\\'s schedule. There\\'s always something coming on the next hour; the only question is what. But when someone on the maker\\'s schedule has a meeting, they have to think about it.\",\\ncontext=\"When you\\'re operating on the maker\\'s schedule, meetings are a disaster. A single meeting can blow a whole afternoon, by breaking it into two pieces each too small to do anything hard in. Plus you have to remember to go to the meeting. That\\'s no problem for someone on the manager\\'s schedule. There\\'s always something coming on the next hour; the only question is what. But when someone on the maker\\'s schedule has a meeting, they have to think about it.\",\\nsummary=\"Meetings disrupt the productivity of those following a maker\\'s schedule, dividing their time into impractical segments, while those on a manager\\'s schedule are accustomed to a continuous flow of tasks.\",\\nsummary=\"Meetings disrupt the productivity of those following a maker\\'s schedule, dividing their time into impractical segments, while those on a manager\\'s schedule are accustomed to a continuous flow of tasks.\",\\nsummary=\"Meetings disrupt the productivity of those following a maker\\'s schedule, dividing their time into impractical segments, while those on a manager\\'s schedule are accustomed to a continuous flow of tasks.\",\\nsummary=\"Meetings disrupt the productivity of those following a maker\\'s schedule, dividing their time into impractical segments, while those on a manager\\'s schedule are accustomed to a continuous flow of tasks.\",\\n),\\n),\\n),\\n),\\n],\\n],\\n],\\n],\\n)\\n)\\n)\\n)\\n```\\n```\\n```\\n```\\n1. Create a `Config` object to pass into the `Client` object.\\n1. Create a `Config` object to pass into the `Client` object.\\n1. Create a `Config` object to pass into the `Client` object.\\n1. Create a `Config` object to pass into the `Client` object.\\n```\\n```\\n```\\n```\\nconfig = OpenAIConfig(\\nconfig = OpenAIConfig(\\nconfig = OpenAIConfig(\\nconfig = OpenAIConfig(\\nguided_prompt_template=guided_prompt,\\nguided_prompt_template=guided_prompt,\\nguided_prompt_template=guided_prompt,\\nguided_prompt_template=guided_prompt,\\nmodel_config=OpenAIModelConfig(\\nmodel_config=OpenAIModelConfig(\\nmodel_config=OpenAIModelConfig(\\nmodel_config=OpenAIModelConfig(\\nresponse_format={\"type\": \"json_object\"}\\nresponse_format={\"type\": \"json_object\"}\\nresponse_format={\"type\": \"json_object\"}\\nresponse_format={\"type\": \"json_object\"}\\n),\\n),\\n),\\n),\\n)\\n)\\n)\\n)\\nclient = Client(config)\\nclient = Client(config)\\nclient = Client(config)\\nclient = Client(config)\\n```\\n```\\n```\\n```\\n1. Use the `client` object to run the flow on the input data.\\n1. Use the `client` object to run the flow on the input data.\\n1. Use the `client` object to run the flow on the input data.\\n1. Use the `client` object to run the flow on the input data.\\n```\\n```\\n```\\n```\\noutput = client.run(data)\\noutput = client.run(data)\\noutput = client.run(data)\\noutput = client.run(data)\\n```\\n```\\n```\\n```\\n1. Process the output data. By default, the LLM output will be a list of output dicts, one for each `Context` passed into the flow. Each dict has a `response` property which has the LLM response, as well as any errors. For example `output[0][\\'output\\'][0]` would look like this:\\n1. Process the output data. By default, the LLM output will be a list of output dicts, one for each `Context` passed into the flow. Each dict has a `response` property which has the LLM response, as well as any errors. For example `output[0][\\'output\\'][0]` would look like this:\\n1. Process the output data. By default, the LLM output will be a list of output dicts, one for each `Context` passed into the flow. Each dict has a `response` property which has the LLM response, as well as any errors. For example `output[0][\\'output\\'][0]` would look like this:\\n1. Process the output data. By default, the LLM output will be a list of output dicts, one for each `Context` passed into the flow. Each dict has a `response` property which has the LLM response, as well as any errors. For example `output[0][\\'output\\'][0]` would look like this:\\n```\\n```\\n```\\n```\\n{\\n{\\n{\\n{\\n\\'response\\': [{\\'context\\': \\'It was a sunny day and the sky color is blue.\\',\\n\\'response\\': [{\\'context\\': \\'It was a sunny day and the sky color is blue.\\',\\n\\'response\\': [{\\'context\\': \\'It was a sunny day and the sky color is blue.\\',\\n\\'response\\': [{\\'context\\': \\'It was a sunny day and the sky color is blue.\\',\\n\\'question\\': \\'What was the color of the sky?\\',\\n\\'question\\': \\'What was the color of the sky?\\',\\n\\'question\\': \\'What was the color of the sky?\\',\\n\\'question\\': \\'What was the color of the sky?\\',\\n\\'answer\\': \\'blue.\\'}],\\n\\'answer\\': \\'blue.\\'}],\\n\\'answer\\': \\'blue.\\'}],\\n\\'answer\\': \\'blue.\\'}],\\n\\'error\\': \\'No errors.\\'\\n\\'error\\': \\'No errors.\\'\\n\\'error\\': \\'No errors.\\'\\n\\'error\\': \\'No errors.\\'\\n}\\n}\\n}\\n}\\n```\\n```\\n```\\n```\\n## Examples',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Running the Flow'}},\n",
       " {'content': 'For more examples, see the [example](./example/model) folder.\\nFor more examples, see the [example](./example/model) folder.\\nFor more examples, see the [example](./example/model) folder.\\nFor more examples, see the [example](./example/model) folder.\\n## Advanced Custom Configuration',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Examples'}},\n",
       " {'content': 'You can also configure the flows by passing custom configurations or arguments to the `Config` object if you want to further tune specific parameters like the the LLM model, number of threads, the temperature, and more.\\nYou can also configure the flows by passing custom configurations or arguments to the `Config` object if you want to further tune specific parameters like the the LLM model, number of threads, the temperature, and more.\\nYou can also configure the flows by passing custom configurations or arguments to the `Config` object if you want to further tune specific parameters like the the LLM model, number of threads, the temperature, and more.\\nYou can also configure the flows by passing custom configurations or arguments to the `Config` object if you want to further tune specific parameters like the the LLM model, number of threads, the temperature, and more.\\nEvery configuration has the following parameters:\\nEvery configuration has the following parameters:\\nEvery configuration has the following parameters:\\nEvery configuration has the following parameters:\\n| Parameter | Type | Description |\\n| Parameter | Type | Description |\\n| Parameter | Type | Description |\\n| Parameter | Type | Description |\\n| ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- |\\n| `guided_prompt_template` | `GuidedPrompt` | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | The template to use for the guided prompt. |\\n| `num_threads` | int | The number of threads to use for the flow. |\\n| `num_threads` | int | The number of threads to use for the flow. |\\n| `num_threads` | int | The number of threads to use for the flow. |\\n| `num_threads` | int | The number of threads to use for the flow. |\\n| `model_config` | `ModelConfig` | The configuration to pass to the model. |\\n| `model_config` | `ModelConfig` | The configuration to pass to the model. |\\n| `model_config` | `ModelConfig` | The configuration to pass to the model. |\\n| `model_config` | `ModelConfig` | The configuration to pass to the model. |\\nYou can further configure the `model_config` by passing in one of the `Model Configs` with custom parameters.\\nYou can further configure the `model_config` by passing in one of the `Model Configs` with custom parameters.\\nYou can further configure the `model_config` by passing in one of the `Model Configs` with custom parameters.\\nYou can further configure the `model_config` by passing in one of the `Model Configs` with custom parameters.\\n### Model Config\\n### Model Config',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Advanced Custom Configuration'}},\n",
       " {'content': 'The __Model Config__ is a configuration that is passed to the base `Config` object and determines which LLM model is used and has parameters that are specific to the LLM model.\\nThe __Model Config__ is a configuration that is passed to the base `Config` object and determines which LLM model is used and has parameters that are specific to the LLM model.\\nThe __Model Config__ is a configuration that is passed to the base `Config` object and determines which LLM model is used and has parameters that are specific to the LLM model.\\nThe __Model Config__ is a configuration that is passed to the base `Config` object and determines which LLM model is used and has parameters that are specific to the LLM model.\\n#### ModelConfig\\n#### ModelConfig\\n#### ModelConfig',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Advanced Custom Configuration',\n",
       "   'Header 3': 'Model Config'}},\n",
       " {'content': 'The base config is called `ModelConfig` and has the following parameters:\\nThe base config is called `ModelConfig` and has the following parameters:\\nThe base config is called `ModelConfig` and has the following parameters:\\nThe base config is called `ModelConfig` and has the following parameters:\\nParameter | Type | Default | Description |\\nParameter | Type | Default | Description |\\nParameter | Type | Default | Description |\\nParameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | gpt-3.5-turbo-1106 | [OpenAI site](https://platform.openai.com/docs/models/gpt-3-5) |\\n| `model_name` | str | gpt-3.5-turbo-1106 | [OpenAI site](https://platform.openai.com/docs/models/gpt-3-5) |\\n| `model_name` | str | gpt-3.5-turbo-1106 | [OpenAI site](https://platform.openai.com/docs/models/gpt-3-5) |\\n| `model_name` | str | gpt-3.5-turbo-1106 | [OpenAI site](https://platform.openai.com/docs/models/gpt-3-5) |\\n#### OpenAIModelConfig\\n#### OpenAIModelConfig\\n#### OpenAIModelConfig',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Advanced Custom Configuration',\n",
       "   'Header 3': 'Model Config',\n",
       "   'Header 4': 'ModelConfig'}},\n",
       " {'content': 'The `OpenAIModelConfig` inherits from the `ModelConfig` and has the following additional parameters:\\nThe `OpenAIModelConfig` inherits from the `ModelConfig` and has the following additional parameters:\\nThe `OpenAIModelConfig` inherits from the `ModelConfig` and has the following additional parameters:\\nThe `OpenAIModelConfig` inherits from the `ModelConfig` and has the following additional parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `num_calls` | int | 1 | The number of calls to make to the OpenAI API. |\\n| `num_calls` | int | 1 | The number of calls to make to the OpenAI API. |\\n| `num_calls` | int | 1 | The number of calls to make to the OpenAI API. |\\n| `num_calls` | int | 1 | The number of calls to make to the OpenAI API. |\\n| `temperature` | float | 1.5 | The temperature to use for the OpenAI API. |\\n| `temperature` | float | 1.5 | The temperature to use for the OpenAI API. |\\n| `temperature` | float | 1.5 | The temperature to use for the OpenAI API. |\\n| `temperature` | float | 1.5 | The temperature to use for the OpenAI API. |\\n| `response_format` | Dict[str, str] | {\"type\": \"text\"} | The response format to use for the OpenAI API. Can be \"text\" or \"json\" |\\n| `response_format` | Dict[str, str] | {\"type\": \"text\"} | The response format to use for the OpenAI API. Can be \"text\" or \"json\" |\\n| `response_format` | Dict[str, str] | {\"type\": \"text\"} | The response format to use for the OpenAI API. Can be \"text\" or \"json\" |\\n| `response_format` | Dict[str, str] | {\"type\": \"text\"} | The response format to use for the OpenAI API. Can be \"text\" or \"json\" |\\n#### HuggingfaceModelConfig\\n#### HuggingfaceModelConfig\\n#### HuggingfaceModelConfig',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Advanced Custom Configuration',\n",
       "   'Header 3': 'Model Config',\n",
       "   'Header 4': 'OpenAIModelConfig'}},\n",
       " {'content': 'The `HuggingfaceModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `mistralai/Mistral-7B-Instruct-v0.1` model by default.\\nThe `HuggingfaceModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `mistralai/Mistral-7B-Instruct-v0.1` model by default.\\nThe `HuggingfaceModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `mistralai/Mistral-7B-Instruct-v0.1` model by default.\\nThe `HuggingfaceModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `mistralai/Mistral-7B-Instruct-v0.1` model by default.\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | mistralai/Mistral-7B-Instruct-v0.1 | [Hugging Face site](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| `model_name` | str | mistralai/Mistral-7B-Instruct-v0.1 | [Hugging Face site](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| `model_name` | str | mistralai/Mistral-7B-Instruct-v0.1 | [Hugging Face site](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| `model_name` | str | mistralai/Mistral-7B-Instruct-v0.1 | [Hugging Face site](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| `batch_size` | int | 1 | The batch size to use for the Hugging Face API. |\\n| `batch_size` | int | 1 | The batch size to use for the Hugging Face API. |\\n| `batch_size` | int | 1 | The batch size to use for the Hugging Face API. |\\n| `batch_size` | int | 1 | The batch size to use for the Hugging Face API. |\\n#### LMQGModelConfig\\n#### LMQGModelConfig\\n#### LMQGModelConfig',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Advanced Custom Configuration',\n",
       "   'Header 3': 'Model Config',\n",
       "   'Header 4': 'HuggingfaceModelConfig'}},\n",
       " {'content': 'The `LMQGModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `lmqg/t5-base-squad-qg-ae` model by default.\\nThe `LMQGModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `lmqg/t5-base-squad-qg-ae` model by default.\\nThe `LMQGModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `lmqg/t5-base-squad-qg-ae` model by default.\\nThe `LMQGModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `lmqg/t5-base-squad-qg-ae` model by default.\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | lmqg/t5-base-squad-qg-ae | [Hugging Face site](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| `model_name` | str | lmqg/t5-base-squad-qg-ae | [Hugging Face site](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| `model_name` | str | lmqg/t5-base-squad-qg-ae | [Hugging Face site](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| `model_name` | str | lmqg/t5-base-squad-qg-ae | [Hugging Face site](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| `batch_size` | int | 1 | The batch size to use for the LMQG API. |\\n| `batch_size` | int | 1 | The batch size to use for the LMQG API. |\\n| `batch_size` | int | 1 | The batch size to use for the LMQG API. |\\n| `batch_size` | int | 1 | The batch size to use for the LMQG API. |\\n### Custom Configuration Example\\n### Custom Configuration Example',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Advanced Custom Configuration',\n",
       "   'Header 3': 'Model Config',\n",
       "   'Header 4': 'LMQGModelConfig'}},\n",
       " {'content': 'Here is an example of how to pass in a custom configuration to the `Client` object:\\nHere is an example of how to pass in a custom configuration to the `Client` object:\\nHere is an example of how to pass in a custom configuration to the `Client` object:\\nHere is an example of how to pass in a custom configuration to the `Client` object:\\n```\\n```\\n```\\n```\\nfrom uniflow.transform.client import Client\\nfrom uniflow.transform.client import Client\\nfrom uniflow.transform.client import Client\\nfrom uniflow.transform.client import Client\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\ncontexts = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\ncontexts = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\ncontexts = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\ncontexts = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\ndata = [\\ndata = [\\ndata = [\\ndata = [\\nContext(\\nContext(\\nContext(\\nContext(\\ncontext=c\\ncontext=c\\ncontext=c\\ncontext=c\\n)\\n)\\n)\\n)\\nfor c in contexts\\nfor c in contexts\\nfor c in contexts\\nfor c in contexts\\n]\\n]\\n]\\n]\\nconfig = OpenAIConfig(\\nconfig = OpenAIConfig(\\nconfig = OpenAIConfig(\\nconfig = OpenAIConfig(\\nnum_threads=2,\\nnum_threads=2,\\nnum_threads=2,\\nnum_threads=2,\\nmodel_config=OpenAIModelConfig(\\nmodel_config=OpenAIModelConfig(\\nmodel_config=OpenAIModelConfig(\\nmodel_config=OpenAIModelConfig(\\nmodel_name=\"gpt-4\",\\nmodel_name=\"gpt-4\",\\nmodel_name=\"gpt-4\",\\nmodel_name=\"gpt-4\",\\nnum_calls=2,\\nnum_calls=2,\\nnum_calls=2,\\nnum_calls=2,\\ntemperature=0.5,\\ntemperature=0.5,\\ntemperature=0.5,\\ntemperature=0.5,\\n),\\n),\\n),\\n),\\n)\\n)\\n)\\n)\\nclient = Client(config)\\nclient = Client(config)\\nclient = Client(config)\\nclient = Client(config)\\noutput = client.run(data)\\noutput = client.run(data)\\noutput = client.run(data)\\noutput = client.run(data)\\n```\\n```\\n```\\n```\\nAs you can see, we are passing in a custom parameters to the `OpenAIModelConfig` to the `OpenAIConfig` configurations according to our needs.\\nAs you can see, we are passing in a custom parameters to the `OpenAIModelConfig` to the `OpenAIConfig` configurations according to our needs.\\nAs you can see, we are passing in a custom parameters to the `OpenAIModelConfig` to the `OpenAIConfig` configurations according to our needs.\\nAs you can see, we are passing in a custom parameters to the `OpenAIModelConfig` to the `OpenAIConfig` configurations according to our needs.\\n## Installation',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Advanced Custom Configuration',\n",
       "   'Header 3': 'Custom Configuration Example'}},\n",
       " {'content': 'To get started with `uniflow`, you can install it using `pip` in a `conda` environment.\\nTo get started with `uniflow`, you can install it using `pip` in a `conda` environment.\\nTo get started with `uniflow`, you can install it using `pip` in a `conda` environment.\\nTo get started with `uniflow`, you can install it using `pip` in a `conda` environment.\\nFirst, create a conda environment on your terminal using:\\nFirst, create a conda environment on your terminal using:\\nFirst, create a conda environment on your terminal using:\\nFirst, create a conda environment on your terminal using:\\n```\\n```\\n```\\n```\\nconda create -n uniflow python=3.10 -y\\nconda create -n uniflow python=3.10 -y\\nconda create -n uniflow python=3.10 -y\\nconda create -n uniflow python=3.10 -y\\nconda activate uniflow  # some OS requires `source activate uniflow`\\nconda activate uniflow  # some OS requires `source activate uniflow`\\nconda activate uniflow  # some OS requires `source activate uniflow`\\nconda activate uniflow  # some OS requires `source activate uniflow`\\n```\\n```\\n```\\n```\\nNext, install the compatible pytorch based on your OS.\\nNext, install the compatible pytorch based on your OS.\\nNext, install the compatible pytorch based on your OS.\\nNext, install the compatible pytorch based on your OS.\\n- If you are on a GPU, install [pytorch based on your cuda version](https://pytorch.org/get-started/locally/). You can find your CUDA version via `nvcc -V`.\\n- If you are on a GPU, install [pytorch based on your cuda version](https://pytorch.org/get-started/locally/). You can find your CUDA version via `nvcc -V`.\\n- If you are on a GPU, install [pytorch based on your cuda version](https://pytorch.org/get-started/locally/). You can find your CUDA version via `nvcc -V`.\\n- If you are on a GPU, install [pytorch based on your cuda version](https://pytorch.org/get-started/locally/). You can find your CUDA version via `nvcc -V`.\\n```\\n```\\n```\\n```\\npip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121  # cu121 means cuda 12.1\\npip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121  # cu121 means cuda 12.1\\npip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121  # cu121 means cuda 12.1\\npip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121  # cu121 means cuda 12.1\\n```\\n```\\n```\\n```\\n- If you are on a CPU instance,\\n- If you are on a CPU instance,\\n- If you are on a CPU instance,\\n- If you are on a CPU instance,\\n```\\n```\\n```\\n```\\npip3 install torch\\npip3 install torch\\npip3 install torch\\npip3 install torch\\n```\\n```\\n```\\n```\\nThen, install `uniflow`:\\nThen, install `uniflow`:\\nThen, install `uniflow`:\\nThen, install `uniflow`:\\n```\\n```\\n```\\n```\\npip3 install uniflow\\npip3 install uniflow\\npip3 install uniflow\\npip3 install uniflow\\n```\\n```\\n```\\n```\\nIf you are running the `HuggingfaceModelFlow`, you will also need to install the `transformers`, `accelerate`, `bitsandbytes`, `scipy` libraries:\\nIf you are running the `HuggingfaceModelFlow`, you will also need to install the `transformers`, `accelerate`, `bitsandbytes`, `scipy` libraries:\\nIf you are running the `HuggingfaceModelFlow`, you will also need to install the `transformers`, `accelerate`, `bitsandbytes`, `scipy` libraries:\\nIf you are running the `HuggingfaceModelFlow`, you will also need to install the `transformers`, `accelerate`, `bitsandbytes`, `scipy` libraries:\\n```\\n```\\n```\\n```\\npip3 install transformers accelerate bitsandbytes scipy\\npip3 install transformers accelerate bitsandbytes scipy\\npip3 install transformers accelerate bitsandbytes scipy\\npip3 install transformers accelerate bitsandbytes scipy\\n```\\n```\\n```\\n```\\nFinally, if you are running the `HuggingfaceModelFlow`, you will also need to install the `lmqg` and `spacy` libraries:\\nFinally, if you are running the `HuggingfaceModelFlow`, you will also need to install the `lmqg` and `spacy` libraries:\\nFinally, if you are running the `HuggingfaceModelFlow`, you will also need to install the `lmqg` and `spacy` libraries:\\nFinally, if you are running the `HuggingfaceModelFlow`, you will also need to install the `lmqg` and `spacy` libraries:\\n```\\n```\\n```\\n```\\npip3 install lmqg spacy\\npip3 install lmqg spacy\\npip3 install lmqg spacy\\npip3 install lmqg spacy\\n```\\n```\\n```\\n```\\nCongrats you have finished the installation!\\nCongrats you have finished the installation!\\nCongrats you have finished the installation!\\nCongrats you have finished the installation!\\n## Dev Setup',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Installation'}},\n",
       " {'content': 'If you are interested in contributing to us, here are the preliminary development setups.\\nIf you are interested in contributing to us, here are the preliminary development setups.\\nIf you are interested in contributing to us, here are the preliminary development setups.\\nIf you are interested in contributing to us, here are the preliminary development setups.\\n### API keys\\n### API keys',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Dev Setup'}},\n",
       " {'content': 'If you are running one of the following `OpenAI` flows, you will have to set up your OpenAI API key.\\nIf you are running one of the following `OpenAI` flows, you will have to set up your OpenAI API key.\\nIf you are running one of the following `OpenAI` flows, you will have to set up your OpenAI API key.\\nIf you are running one of the following `OpenAI` flows, you will have to set up your OpenAI API key.\\nTo do so, create a `.env` file in your root uniflow folder. Then add the following line to the `.env` file:\\nTo do so, create a `.env` file in your root uniflow folder. Then add the following line to the `.env` file:\\nTo do so, create a `.env` file in your root uniflow folder. Then add the following line to the `.env` file:\\nTo do so, create a `.env` file in your root uniflow folder. Then add the following line to the `.env` file:\\n```\\n```\\n```\\n```\\nOPENAI_API_KEY=YOUR_API_KEY\\nOPENAI_API_KEY=YOUR_API_KEY\\nOPENAI_API_KEY=YOUR_API_KEY\\nOPENAI_API_KEY=YOUR_API_KEY\\n```\\n```\\n```\\n```\\n### Backend Dev Setup\\n### Backend Dev Setup',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Dev Setup',\n",
       "   'Header 3': 'API keys'}},\n",
       " {'content': '```\\n```\\n```\\n```\\nconda create -n uniflow python=3.10\\nconda create -n uniflow python=3.10\\nconda create -n uniflow python=3.10\\nconda create -n uniflow python=3.10\\nconda activate uniflow\\nconda activate uniflow\\nconda activate uniflow\\nconda activate uniflow\\ncd uniflow\\ncd uniflow\\ncd uniflow\\ncd uniflow\\npip3 install poetry\\npip3 install poetry\\npip3 install poetry\\npip3 install poetry\\npoetry install --no-root\\npoetry install --no-root\\npoetry install --no-root\\npoetry install --no-root\\n```\\n```\\n```\\n```\\n### EC2 Dev Setup\\n### EC2 Dev Setup',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Dev Setup',\n",
       "   'Header 3': 'Backend Dev Setup'}},\n",
       " {'content': 'If you are on EC2, you can launch a GPU instance with the following config:\\nIf you are on EC2, you can launch a GPU instance with the following config:\\nIf you are on EC2, you can launch a GPU instance with the following config:\\nIf you are on EC2, you can launch a GPU instance with the following config:\\n- EC2 `g4dn.xlarge` (if you want to run a pretrained LLM with 7B parameters)\\n- EC2 `g4dn.xlarge` (if you want to run a pretrained LLM with 7B parameters)\\n- EC2 `g4dn.xlarge` (if you want to run a pretrained LLM with 7B parameters)\\n- EC2 `g4dn.xlarge` (if you want to run a pretrained LLM with 7B parameters)\\n- Deep Learning AMI PyTorch GPU 2.0.1 (Ubuntu 20.04)\\n- Deep Learning AMI PyTorch GPU 2.0.1 (Ubuntu 20.04)\\n- Deep Learning AMI PyTorch GPU 2.0.1 (Ubuntu 20.04)\\n- Deep Learning AMI PyTorch GPU 2.0.1 (Ubuntu 20.04)\\n<img src=\"example/image/readme_ec2_ami.jpg\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n<img src=\"example/image/readme_ec2_ami.jpg\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n<img src=\"example/image/readme_ec2_ami.jpg\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n<img src=\"example/image/readme_ec2_ami.jpg\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n- EBS: at least 100G\\n- EBS: at least 100G\\n- EBS: at least 100G\\n- EBS: at least 100G\\n<img src=\"example/image/readme_ec2_storage.png\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n<img src=\"example/image/readme_ec2_storage.png\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n<img src=\"example/image/readme_ec2_storage.png\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n<img src=\"example/image/readme_ec2_storage.png\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>',\n",
       "  'metadata': {'Header 1': 'ðŸŒŠ uniflow',\n",
       "   'Header 2': 'Dev Setup',\n",
       "   'Header 3': 'EC2 Dev Setup'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SplitMarkdownOp().header_splitter(markdown_str)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='<p align=\"center\">\\n<a href=\"/LICENSE\"><img alt=\"License Apache-2.0\" src=\"https://img.shields.io/github/license/cambioml/uniflow?style=flat-square\"></a>\\n<a href=\"https://pypi.org/project/uniflow\"><img src=\"https://img.shields.io/pypi/v/uniflow.svg\" alt=\"pypi_status\" /></a>\\n<a href=\"https://github.com/cambioml/uniflow/graphs/commit-activity\"><img alt=\"Commit activity\" src=\"https://img.shields.io/github/commit-activity/m/cambioml/uniflow?style=flat-square\"/></a>\\n<a href=\"https://join.slack.com/t/cambiomlworkspace/shared_invite/zt-1zes33rmt-20Rag043uvExUaUdvt5_xQ\"><img src=\"https://badgen.net/badge/Join/Community/cyan?icon=slack\" alt=\"Slack\" /></a>\\n</p>  \\n`uniflow` is a unified interface to solve data augmentation problem for LLM training. It enables use of different LLMs, including [OpenAI](https://openai.com/product), [Huggingface](https://huggingface.co/mistralai/Mistral-7B-v0.1), and [LMQG](https://huggingface.co/lmqg) with a single interface. Using `uniflow`, you can easily run different LLMs to generate questions and answers, chunk text, summarize text, and more.  \\nBuilt by [CambioML](https://www.cambioml.com/).', metadata={'Header 1': 'ðŸŒŠ uniflow'}),\n",
       " Document(page_content='```\\npip3 install uniflow\\n```  \\nSee more details at the [full installation](#installation).', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Quick Install'}),\n",
       " Document(page_content=\"To use `uniflow`, follow of three main steps:\\n1. **Pick a [`Config`](#config)**\\\\\\nThis determines the LLM and the different configurable parameters.  \\n1. **Construct your [`Prompts`](#prompting)**\\\\\\nConstruct the context that you want to use to prompt your model. You can configure custom instructions and examples using the [`GuidedPrompt`](#guidedprompt) class.  \\n1. **Run your [`Flow`](#running-the-flow)**\\\\\\nRun the flow on your input data and generate output from your LLM.  \\n> *Note: We're currently building have `Preprocessing` flows as well to help process data from different sources, such as `pdf`, `html`, `Markdown`, and more.*\", metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Overview'}),\n",
       " Document(page_content='The `Config` determines which LLM is used and how the input data is serialized and deserialized. It also has parameters that are specific to the LLM.  \\nHere is a table of the different pre-defined configurations you can use and their corresponding LLMs:\\n| Config | LLM |\\n| ------------- | ------------- |\\n| __Config__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5) |\\n| __OpenAIConfig__ | [`gpt-3.5-turbo-1106`](https://platform.openai.com/docs/models/gpt-3-5)|\\n| __HuggingfaceConfig__| [`mistralai/Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| __LMQGConfig__ | [`lmqg/t5-base-squad-qg-ae`](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |  \\nYou can run each config with the defaults, or you can pass in custom parameters, such as `temperature` or `batch_size` to the config for your use case. See the [advanced custom configuration](#advanced-custom-configuration) section for more details.', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': '1. Config'}),\n",
       " Document(page_content='By default, `uniflow` is set up to generate questions and answers based on the `Context` you pass in. To do so, it has a default instruction and few-shot examples that it uses to guide the LLM.  \\nHere is the default instruction:\\n```\\nGenerate one question and its corresponding answer based on the last context in the last example. Follow the format of the examples below to include context, question, and answer in the response\\n```  \\nHere are the default few-shot examples:\\n```\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\nquestion=\"What is the color of the fox?\",\\nanswer=\"brown.\"\\n\\ncontext=\"The quick brown fox jumps over the lazy black dog.\",\\nquestion=\"What is the color of the dog?\",\\nanswer=\"black.\"\\n```  \\nTo run with these default instructions and examples, all you need to do is pass in a list of  `Context` objects to the flow. `uniflow` will then generate a custom prompt with the instructions and few-shot examples for each `Context` object to send to the LLM. See the [Running the flow](#running-the-flow) section for more details.', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': '2. Prompting'}),\n",
       " Document(page_content='The `Context` class is used to pass in the context for the LLM prompt. A `Context` consists of a `context` property, which is a string of text.  \\nTo run `uniflow` with the default instructions and few-shot examples, you can pass in a list of `Context` objects to the flow. For example:\\n```\\nfrom uniflow.schema import Context\\n\\ndata = [\\nContext(\\ncontext=\"The quick brown fox jumps over the lazy brown dog.\",\\n),\\n...\\n]\\n\\nclient.run(data)\\n```  \\nFor a more detailed overview of running the flow, see the [Running the flow](#running-the-flow) section.', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': '2. Prompting', 'Header 3': 'Context'}),\n",
       " Document(page_content='If you want to run with a custom prompt instruction or few-shot examples, you can use the `GuidedPrompt` object. It has `instruction` and `example` properties.  \\n| Property | Type | Description |\\n| ------------- | ------------- | ------------- |\\n| `instruction` | str | Detailed instructions for the LLM |\\n| `examples` | List[Context] | The few-shot examples. |  \\nYou can overwrite any of the defaults as needed.  \\nTo see an example of how to use the `GuidedPrompt` to run `uniflow` with a custom `instruction`, few-shot examples, and custom `Context` fields to generate a summary, check out the [openai_pdf_source_10k_summary notebook](./example/model/openai_pdf_source_10k_summary.ipynb)', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': '2. Prompting', 'Header 3': 'GuidedPrompt'}),\n",
       " Document(page_content='Once you\\'ve decided on your `Config` and prompting strategy, you can run the flow on the input data.  \\n1. Import the `uniflow` `Client`, `Config`, and `Context` objects.\\n```\\nfrom uniflow.transform.client import Client\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\nfrom uniflow.schema import Context\\n```\\n1. Preprocess your data in to chunks to pass into the flow. In the future we will have `Preprocessing` flows to help with this step, but for now you can use a library of your choice, like [pypdf](https://pypi.org/project/pypdf/), to chunk your data.\\n```\\nraw_input_context = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\n```  \\n1. Create a list of `Context` objects to pass your data into the flow.\\n```\\ndata = [\\nContext(context=c)\\nfor c in raw_input_context\\n]\\n```  \\n1. [Optional] If you want to use a customized instruction and/or examples, create a `GuidedPrompt`.\\n```\\nfrom uniflow.schema import GuidedPrompt\\n\\nguided_prompt = GuidedPrompt(\\ninstruction=\"Generate a one sentence summary based on the last context below. Follow the format of the examples below to include context and summary in the response\",\\nexamples=[\\nContext(\\ncontext=\"When you\\'re operating on the maker\\'s schedule, meetings are a disaster. A single meeting can blow a whole afternoon, by breaking it into two pieces each too small to do anything hard in. Plus you have to remember to go to the meeting. That\\'s no problem for someone on the manager\\'s schedule. There\\'s always something coming on the next hour; the only question is what. But when someone on the maker\\'s schedule has a meeting, they have to think about it.\",\\nsummary=\"Meetings disrupt the productivity of those following a maker\\'s schedule, dividing their time into impractical segments, while those on a manager\\'s schedule are accustomed to a continuous flow of tasks.\",\\n),\\n],\\n)\\n```  \\n1. Create a `Config` object to pass into the `Client` object.\\n```\\nconfig = OpenAIConfig(\\nguided_prompt_template=guided_prompt,\\nmodel_config=OpenAIModelConfig(\\nresponse_format={\"type\": \"json_object\"}\\n),\\n)\\nclient = Client(config)\\n```  \\n1. Use the `client` object to run the flow on the input data.  \\n```\\noutput = client.run(data)\\n```  \\n1. Process the output data. By default, the LLM output will be a list of output dicts, one for each `Context` passed into the flow. Each dict has a `response` property which has the LLM response, as well as any errors. For example `output[0][\\'output\\'][0]` would look like this:\\n```\\n{\\n\\'response\\': [{\\'context\\': \\'It was a sunny day and the sky color is blue.\\',\\n\\'question\\': \\'What was the color of the sky?\\',\\n\\'answer\\': \\'blue.\\'}],\\n\\'error\\': \\'No errors.\\'\\n}\\n```', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Running the Flow'}),\n",
       " Document(page_content='For more examples, see the [example](./example/model) folder.', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Examples'}),\n",
       " Document(page_content='You can also configure the flows by passing custom configurations or arguments to the `Config` object if you want to further tune specific parameters like the the LLM model, number of threads, the temperature, and more.  \\nEvery configuration has the following parameters:\\n| Parameter | Type | Description |\\n| ------------- | ------------- | ------------- |\\n| `guided_prompt_template` | `GuidedPrompt` | The template to use for the guided prompt. |\\n| `num_threads` | int | The number of threads to use for the flow. |\\n| `model_config` | `ModelConfig` | The configuration to pass to the model. |  \\nYou can further configure the `model_config` by passing in one of the `Model Configs` with custom parameters.', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Advanced Custom Configuration'}),\n",
       " Document(page_content='The __Model Config__ is a configuration that is passed to the base `Config` object and determines which LLM model is used and has parameters that are specific to the LLM model.', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Advanced Custom Configuration', 'Header 3': 'Model Config'}),\n",
       " Document(page_content='The base config is called `ModelConfig` and has the following parameters:\\nParameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | gpt-3.5-turbo-1106 | [OpenAI site](https://platform.openai.com/docs/models/gpt-3-5) |', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Advanced Custom Configuration', 'Header 3': 'Model Config', 'Header 4': 'ModelConfig'}),\n",
       " Document(page_content='The `OpenAIModelConfig` inherits from the `ModelConfig` and has the following additional parameters:\\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `num_calls` | int | 1 | The number of calls to make to the OpenAI API. |\\n| `temperature` | float | 1.5 | The temperature to use for the OpenAI API. |\\n| `response_format` | Dict[str, str] | {\"type\": \"text\"} | The response format to use for the OpenAI API. Can be \"text\" or \"json\" |', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Advanced Custom Configuration', 'Header 3': 'Model Config', 'Header 4': 'OpenAIModelConfig'}),\n",
       " Document(page_content='The `HuggingfaceModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `mistralai/Mistral-7B-Instruct-v0.1` model by default.\\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | mistralai/Mistral-7B-Instruct-v0.1 | [Hugging Face site](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\\n| `batch_size` | int | 1 | The batch size to use for the Hugging Face API. |', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Advanced Custom Configuration', 'Header 3': 'Model Config', 'Header 4': 'HuggingfaceModelConfig'}),\n",
       " Document(page_content='The `LMQGModelConfig` inherits from the `ModelConfig`, but overrides the `model_name` parameter to use the `lmqg/t5-base-squad-qg-ae` model by default.  \\n| Parameter | Type | Default | Description |\\n| ------------- | ------------- | ------------- | ------------- |\\n| `model_name` | str | lmqg/t5-base-squad-qg-ae | [Hugging Face site](https://huggingface.co/lmqg/t5-base-squad-qg-ae) |\\n| `batch_size` | int | 1 | The batch size to use for the LMQG API. |', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Advanced Custom Configuration', 'Header 3': 'Model Config', 'Header 4': 'LMQGModelConfig'}),\n",
       " Document(page_content='Here is an example of how to pass in a custom configuration to the `Client` object:\\n```\\nfrom uniflow.transform.client import Client\\nfrom uniflow.config import OpenAIConfig\\nfrom uniflow.model.config import OpenAIModelConfig\\n\\ncontexts = [\"It was a sunny day and the sky color is blue.\", \"My name is bobby and I am a talent software engineer working on AI/ML.\"]\\n\\ndata = [\\nContext(\\ncontext=c\\n)\\nfor c in contexts\\n]\\n\\nconfig = OpenAIConfig(\\nnum_threads=2,\\nmodel_config=OpenAIModelConfig(\\nmodel_name=\"gpt-4\",\\nnum_calls=2,\\ntemperature=0.5,\\n),\\n)\\nclient = Client(config)\\noutput = client.run(data)\\n```  \\nAs you can see, we are passing in a custom parameters to the `OpenAIModelConfig` to the `OpenAIConfig` configurations according to our needs.', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Advanced Custom Configuration', 'Header 3': 'Custom Configuration Example'}),\n",
       " Document(page_content='To get started with `uniflow`, you can install it using `pip` in a `conda` environment.  \\nFirst, create a conda environment on your terminal using:\\n```\\nconda create -n uniflow python=3.10 -y\\nconda activate uniflow  # some OS requires `source activate uniflow`\\n```  \\nNext, install the compatible pytorch based on your OS.\\n- If you are on a GPU, install [pytorch based on your cuda version](https://pytorch.org/get-started/locally/). You can find your CUDA version via `nvcc -V`.\\n```\\npip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121  # cu121 means cuda 12.1\\n```\\n- If you are on a CPU instance,\\n```\\npip3 install torch\\n```  \\nThen, install `uniflow`:\\n```\\npip3 install uniflow\\n```  \\nIf you are running the `HuggingfaceModelFlow`, you will also need to install the `transformers`, `accelerate`, `bitsandbytes`, `scipy` libraries:\\n```\\npip3 install transformers accelerate bitsandbytes scipy\\n```  \\nFinally, if you are running the `HuggingfaceModelFlow`, you will also need to install the `lmqg` and `spacy` libraries:\\n```\\npip3 install lmqg spacy\\n```  \\nCongrats you have finished the installation!', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Installation'}),\n",
       " Document(page_content='If you are interested in contributing to us, here are the preliminary development setups.', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Dev Setup'}),\n",
       " Document(page_content='If you are running one of the following `OpenAI` flows, you will have to set up your OpenAI API key.  \\nTo do so, create a `.env` file in your root uniflow folder. Then add the following line to the `.env` file:\\n```\\nOPENAI_API_KEY=YOUR_API_KEY\\n```', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Dev Setup', 'Header 3': 'API keys'}),\n",
       " Document(page_content='```\\nconda create -n uniflow python=3.10\\nconda activate uniflow\\ncd uniflow\\npip3 install poetry\\npoetry install --no-root\\n```', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Dev Setup', 'Header 3': 'Backend Dev Setup'}),\n",
       " Document(page_content='If you are on EC2, you can launch a GPU instance with the following config:\\n- EC2 `g4dn.xlarge` (if you want to run a pretrained LLM with 7B parameters)\\n- Deep Learning AMI PyTorch GPU 2.0.1 (Ubuntu 20.04)\\n<img src=\"example/image/readme_ec2_ami.jpg\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>\\n- EBS: at least 100G\\n<img src=\"example/image/readme_ec2_storage.png\" alt=\"Alt text\" width=\"50%\" height=\"50%\"/>', metadata={'Header 1': 'ðŸŒŠ uniflow', 'Header 2': 'Dev Setup', 'Header 3': 'EC2 Dev Setup'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_str)\n",
    "md_header_splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
