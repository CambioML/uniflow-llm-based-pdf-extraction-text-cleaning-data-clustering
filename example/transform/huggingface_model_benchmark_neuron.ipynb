{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QAs from Unstructured Data Using AWS Inferentia and Neuron\n",
    "\n",
    "Are you a machine learning (ML) professional grappling with the challenge of extracting high-quality data from confidential and unstructured formats such as PDFs and HTML? The task of fine-tuning large language models (LLMs) often requires dealing with such complex data, which poses significant challenges in terms of data privacy, processing efficiency, and model accuracy.\n",
    "In this example, we will show you how to generate question-answers (QAs) from give text strings using open-source models via uniflow's [HuggingFaceModelFlow](https://github.com/CambioML/uniflow/blob/main/uniflow/flow/transform/transform_huggingface_flow.py).\n",
    "\n",
    "This article provides a comprehensive guide on leveraging the power of Uniflow on [Amazon Web Services (AWS) Inferentia2 (Inf2) instances](https://aws.amazon.com/machine-learning/inferentia/) for effectively converting unstructured data into a structured format, ideal for private LLM fine-tuning. We delve into the intricacies of handling sensitive information while maintaining confidentiality and data integrity.  Furthermore, we explore the robust capabilities of the AWS Neuron SDK, which plays a pivotal role in enhancing the performance of your ML models. The Neuron SDK is specifically designed to optimize ML workloads on AWS hardware, ensuring you achieve faster processing speeds and reduced operational costs.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q transformers accelerate bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformHuggingFaceConfig, HuggingfaceModelConfig\n",
    "from uniflow.op.prompt import PromptTemplate, Context\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample prompts\n",
    "\n",
    "First, we need to demonstrate sample prompts for LLM, those include instruction and sample json format. We do this by giving a sample instruction and list of `Context` examples to the `PromptTemplate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_instruction = \"\"\"Generate one question and its corresponding answer based on the context. Following \\\n",
    "the format of the examples below to include context, question, and answer in the response.\"\"\"\n",
    "\n",
    "sample_examples = [\n",
    "        Context(\n",
    "            context=\"The quick brown fox jumps over the lazy dog.\",\n",
    "            question=\"What is the color of the fox?\",\n",
    "            answer=\"brown.\"\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"The quick brown fox jumps over the lazy black dog.\",\n",
    "            question=\"What is the color of the dog?\",\n",
    "            answer=\"black.\"\n",
    "        )]\n",
    "\n",
    "guided_prompt = PromptTemplate(\n",
    "    instruction=sample_instruction,\n",
    "    few_shot_prompt=sample_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we craft some dummy sample raw text strings. Below, we build a dataset with 400 text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size of raw context:  1000\n"
     ]
    }
   ],
   "source": [
    "raw_context_input = [\n",
    "    \"\"\"We believe our success depends upon our capabilities in areas such as design, research and development, \\\n",
    "production and marketing and is supported and protected by our intellectual property rights, such as \\\n",
    "trademarks, utility and design patents, copyrights, and trade secrets, among others. We have followed a policy \\\n",
    "of applying for and registering intellectual property rights in the United States and select foreign countries \\\n",
    "on trademarks, inventions, innovations and designs that we deem valuable. W e also continue to vigorously \\\n",
    "protect our intellectual property, including trademarks, patents and trade secrets against third-party \\\n",
    "infringement and misappropriation.\"\"\",\n",
    "    \"\"\"In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) \\\n",
    "establishing the theory of information. In his article, Shannon introduced the concept of information entropy \\\n",
    "for the first time. We will begin our journey here.\"\"\",\n",
    "    \"\"\"The chain rule states that the derivative of a composite function (a function composed of another \\\n",
    "function) is equal to the derivative of the outer function multiplied by the derivative of the inner function.\\\n",
    "Mathematically, it can be written as: \\(\\frac{d}{dx}g(h(x)) = \\frac{dg}{dh}(h(x))\\cdot \\frac{dh}{dx}(x)\\).\"\"\",\n",
    "    \"\"\"Hypothesis testing involves making a claim about a population parameter based on sample data, and then \\\n",
    "conducting a test to determine whether this claim is supported or rejected. This typically involves \\\n",
    "calculating a test statistic, determining a significance level, and comparing the calculated value to a \\\n",
    "critical value to obtain a p-value. \"\"\"\n",
    "]\n",
    "\n",
    "raw_context_input = raw_context_input * 250\n",
    "\n",
    "print(\"sample size of raw context: \", len(raw_context_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for the given raw text strings `raw_context_input` above, we convert them to the `Context` class to be processed by `uniflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size of processed input data:  1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Context(context='We believe our success depends upon our capabilities in areas such as design, research and development, production and marketing and is supported and protected by our intellectual property rights, such as trademarks, utility and design patents, copyrights, and trade secrets, among others. We have followed a policy of applying for and registering intellectual property rights in the United States and select foreign countries on trademarks, inventions, innovations and designs that we deem valuable. W e also continue to vigorously protect our intellectual property, including trademarks, patents and trade secrets against third-party infringement and misappropriation.'),\n",
       " Context(context='In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_data = [\n",
    "    Context(context=data)\n",
    "    for data in raw_context_input\n",
    "]\n",
    "\n",
    "print(\"sample size of processed input data: \", len(input_data))\n",
    "\n",
    "input_data[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [HuggingfaceModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L39)'s default LLM to generate questions and answers. Let's import the config and client of this model.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `HuggingfaceConfig` to use our customized instructions and examples, instead of the `uniflow` default ones.\n",
    "\n",
    "Note, base on your GPU memory, you can set your optimal `batch_size` below. (We attached our `batch_size` benchmarking results in the appendix of this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron model does not support quantized models. load_in_4bit and load_in_8bit are automatically set to False.\n",
      "Spliting model, need to wait for a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 596/596 [00:00<00:00, 4.32MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 25.1k/25.1k [00:00<00:00, 80.0MB/s]\n",
      "model-00001-of-00003.safetensors: 100%|██████████| 4.94G/4.94G [00:20<00:00, 239MB/s]\n",
      "model-00002-of-00003.safetensors: 100%|██████████| 5.00G/5.00G [00:20<00:00, 243MB/s]\n",
      "model-00003-of-00003.safetensors: 100%|██████████| 4.54G/4.54G [00:17<00:00, 265MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [00:58<00:00, 19.59s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n",
      "generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 899kB/s]\n",
      "100%|██████████| 161M/161M [00:01<00:00, 100MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/uniflow/10b481c5b2f1e3054ca092027443b1a2.zip /home/ubuntu/.cache/uniflow/10b481c5b2f1e3054ca092027443b1a2\n",
      "loading neuron model from cache, need to wait for a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 11.4MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 699kB/s]\n"
     ]
    }
   ],
   "source": [
    "config = TransformHuggingFaceConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=HuggingfaceModelConfig(batch_size=8, model_name='mistralai/Mistral-7B-Instruct-v0.2', neuron=True))\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-Feb-27 06:44:35.0604 2332:2749 [1] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2024-Feb-27 06:44:35.0604 2332:2749 [1] init.cc:137 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [10:14<00:00,  4.92s/it]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "output = client.run(input_data)\n",
    "execution_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the output\n",
    "\n",
    "Let's take a look of the generated output. We need to do a little postprocessing on the raw output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': ' We believe our success depends upon our capabilities in areas '\n",
      "            'such as design, research and development, production and '\n",
      "            'marketing and is supported and protected by our intellectual '\n",
      "            'property rights, such as trademarks, utility and design patents, '\n",
      "            'copyrights, and trade secrets, among others. We have followed a '\n",
      "            'policy of applying for and registering intellectual property '\n",
      "            'rights in the United States and select foreign countries on '\n",
      "            'trademarks, inventions, innovations and designs that we deem '\n",
      "            'valuable. W e also continue to vigorously protect our '\n",
      "            'intellectual property, including trademarks, patents and trade '\n",
      "            'secrets against third-party infringement and misappropriation. ',\n",
      " 'question': ' What types of intellectual property does the company protect?\\n',\n",
      " 'answer': ' The company protects various intellectual property rights, '\n",
      "           'including trademarks, utility and design patents, copyrights, and '\n",
      "           'trade secrets.'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "keywords = [\"context:\", \"question:\", \"answer:\"]\n",
    "pattern = '|'.join(map(re.escape, keywords))\n",
    "\n",
    "o = output[0]['output'][0]['response'][0] ## we only postprocess the first output\n",
    "segments = [segment for segment in re.split(pattern, o) if segment.strip()]\n",
    "result = {\n",
    "    \"context\": segments[-3],\n",
    "    \"question\": segments[-2],\n",
    "    \"answer\": segments[-1]\n",
    "}\n",
    "\n",
    "pprint(result, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(document):\n",
    "    ''' \n",
    "    Give a document(string), return total number of tokens(words)\n",
    "\n",
    "    Parameters:\n",
    "       documents(string): A document\n",
    "\n",
    "    Returns:\n",
    "       count(int): Total number of tokens. 1 token = 0.75 word\n",
    "    '''\n",
    "    num_words = len(document.split())\n",
    "    num_tokens = round(num_words/0.75)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tokens:  131263\n",
      "input tokens:  76000\n",
      "prompt tokens:  36\n",
      "total tokens:  207299\n",
      "processed tokens per second:  337\n"
     ]
    }
   ],
   "source": [
    "# Count tokens\n",
    "input_tokens = sum(count_tokens(d) for d in raw_context_input)\n",
    "prompt_tokens = count_tokens(config.prompt_template.instruction)\n",
    "output_tokens = 0\n",
    "for o in output:\n",
    "    if \"output\" in o:\n",
    "        for response in o[\"output\"]:\n",
    "            for r in response[\"response\"]:\n",
    "                segments = [segment for segment in re.split(pattern, r) if segment.strip()]\n",
    "                result = {\n",
    "                    \"context\": segments[-3],\n",
    "                    \"question\": segments[-2],\n",
    "                    \"answer\": segments[-1]\n",
    "                }\n",
    "                output_tokens += count_tokens(result[\"context\"])\n",
    "                output_tokens += count_tokens(result[\"question\"])\n",
    "                output_tokens += count_tokens(result[\"answer\"])\n",
    "\n",
    "tokens = input_tokens + prompt_tokens + output_tokens\n",
    "print(\"output tokens: \", output_tokens)\n",
    "print(\"input tokens: \", input_tokens)\n",
    "print(\"prompt tokens: \",prompt_tokens)\n",
    "print(\"total tokens: \", tokens)\n",
    "print(\"processed tokens per second: \", round(float(tokens/execution_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! Your question answers from the given knowledge context are generated!\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Appendix\n",
    "\n",
    "We benchmarked to see the optimal `batch_size` for the `HuggingfaceModelConfig` flow. The answer is \"It depends on your data token length, your instance type, your LLM size, etc.\" In the following experiment, we use an AWS inf2.8xlarge. We still use the above 1000 raw data strings `raw_context_input`.\n",
    "\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "- batch_size = 1             \n",
    "    100%|██████████| 1000/1000 [5:11:19<00:00, 18.68s/it]         \n",
    "    output tokens:  364189    \n",
    "    input tokens:  76000    \n",
    "    prompt tokens:  36   \n",
    "    total tokens:  440225   \n",
    "    processed tokens per second:  24   \n",
    "\n",
    "- batch_size = 2         \n",
    "    100%|██████████| 500/500 [24:03<00:00,  2.89s/it]         \n",
    "    output tokens:  131427     \n",
    "    input tokens:  76000    \n",
    "    prompt tokens:  36    \n",
    "    total tokens:  207463    \n",
    "    processed tokens per second:  144     \n",
    "\n",
    "\n",
    "- batch_size = 4   \n",
    "    100%|██████████| 250/250 [17:45<00:00,  4.26s/it]   \n",
    "    output tokens:  132261   \n",
    "    input tokens:  76000   \n",
    "    prompt tokens:  36   \n",
    "    total tokens:  208297   \n",
    "    processed tokens per second:  195   \n",
    "\n",
    "\n",
    "- batch_size = 8   \n",
    "    100%|██████████| 125/125 [10:14<00:00,  4.92s/it]         \n",
    "    output tokens:  131263   \n",
    "    input tokens:  76000   \n",
    "    prompt tokens:  36   \n",
    "    total tokens:  207299   \n",
    "    processed tokens per second:  337   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
