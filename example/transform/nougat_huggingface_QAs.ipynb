{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23393b1c-b26c-4372-ba4e-58cb2033dfda",
   "metadata": {},
   "source": [
    "# Generate QAs based on the target PDF extracted from `Nougat`\n",
    "\n",
    "This package uses `nougat`, a tool for extracting information from specific PDF files. It also includes a feature that uses OpenAI's language models to check the accuracy of this extracted information. Additionally, the package uses `uniflow` to create questions and answers based on the information taken from the PDFs.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we are storing the Nike 10K in the `data\\raw_input` directory as \"nike-10k-2023.pdf\". You can download the file from [here](https://s1.q4cdn.com/806093406/files/doc_downloads/2023/414759-1-_5_Nike-NPS-Combo_Form-10-K_WR.pdf).\n",
    "\n",
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c26092-47bd-424d-af91-102acbc9cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa47b52-401c-4dc2-a0d8-753d079a1be8",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b336c909-2c3f-4df5-8fef-0672eaeda8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip -q install transformers accelerate bitsandbytes scipy nougat-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76abb4c-59fd-4514-9c78-8c383f2b3dd8",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e3cb8c-3a98-4599-a9c2-f0b1be59bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "from uniflow.flow.client import TransformClient, ExtractClient\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.flow.config import TransformOpenAIConfig, TransformHuggingFaceConfig, HuggingfaceModelConfig, ExtractPDFConfig, NougatModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e9a6a-c027-4f86-9e72-4ed8b11fce97",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "First, let's set current directory and input data directory, and load the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b503110-0dde-4d5e-a88c-789dd2a347b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()\n",
    "pdf_file = \"amazon-10k-2023.pdf\"\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f6dae-1a2b-462d-b46e-783510830d79",
   "metadata": {},
   "source": [
    "**Load the pdf using Nougat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d93208d-4276-4864-b955-0395c32b89fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "  0%|                                                                                               | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: likely hallucinated title at the end of the page: ## Appendix B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [07:10<00:00, 430.84s/it]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"pdf\": input_file},\n",
    "]\n",
    "\n",
    "config = ExtractPDFConfig(\n",
    "    model_config=NougatModelConfig(\n",
    "        model_name = \"0.1.0-small\",\n",
    "        batch_size = 128 # When batch_size>1, nougat will run on CUDA, otherwise it will run on CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "nougat_client = ExtractClient(config)\n",
    "\n",
    "output = nougat_client.run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d23c7e-850b-4770-9d59-fa3c7c9f0d40",
   "metadata": {},
   "source": [
    "Extract context from `output` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19659331-9f78-43b4-8907-1f1381602c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_context = '\\n'.join(output[0]['output'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56ed8ef-bd07-4835-9693-a8c345183915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('## UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549\\n'\n",
      " '**FORM 10-K**\\n'\n",
      " '**(Mark One)**\\n'\n",
      " '**(Exact name of registrant as specified in its charter)**\\n'\n",
      " '**Delaware**\\n'\n",
      " '**(State or other jurisdictions of**\\n'\n",
      " '**(Interpretation of organizations)**\\n'\n",
      " '**(I.R.S. Englurver**\\n'\n",
      " '**(I.\\n'\n",
      " '**AMAZON.COM, INC.**\\n'\n",
      " '**FORM 10-K**\\n'\n",
      " '**For the Fiscal Year Ended December 31, 2022**\\n'\n",
      " '**INDEX**\\n'\n",
      " '**PART I**\\n'\n",
      " 'Item 1. Business\\n'\n",
      " 'Item 1A. Risk Factors\\n'\n",
      " 'Item 1B. Unresolved Staff Comments\\n'\n",
      " 'Item 2. Properties\\n'\n",
      " 'Item 3. Legal Proceedings\\n'\n",
      " 'Item 4. Mine Safety Disclosures\\n'\n",
      " '**PART II**\\n'\n",
      " \"Item 5. Market for the Registrant's Common Stock, Related Shareholder \"\n",
      " 'Matters, and Issuer Purchases of Equity Securities\\n'\n",
      " 'Item 6. Reserved\\n'\n",
      " \"Item 7. Management's Discussion and Analysis of Financial Condition and \"\n",
      " 'Results of Operations\\n'\n",
      " 'Item 7A. Quantitative ')\n"
     ]
    }
   ],
   "source": [
    "pprint(extract_context[:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429cf469-ed3e-45d9-9560-9403c8e3cfda",
   "metadata": {},
   "source": [
    "Below are helper functions designed to process the output of `Nougat`, ensuring that the output context is efficiently processed by the Hugging Face model.\n",
    "\n",
    "#### Overview:\n",
    "The `process_content` function is designed to process the content generated by `ExtractClient` above, particularly handling large sections and table content. It reads a markdown file, splits it into manageable sections, processes these sections to handle table content, and optionally utilizes OpenAI for further processing.\n",
    "\n",
    "#### Inputs:\n",
    "- `content`: The string content generated by `ExtractClient`.\n",
    "- `client_openAI`: An object representing the OpenAI client, used for processing sections of the markdown file.\n",
    "\n",
    "#### Workflow:\n",
    "1. **Reading the File**: The function starts by reading the entire content of the markdown file specified by `file_path`.\n",
    "2. **Initial Splitting**: The content is split into sections based on '##' headers. The first section is skipped if it's empty.\n",
    "3. **Sub-Splitting for Large Sections**: Sections larger than a predefined word count (`max_word_count`) are further split using '###' headers.\n",
    "4. **Processing for Table Content**: Each section is processed for table content if its word count exceeds `max_word_count_for_table`. This involves reducing the word count while preserving essential information.\n",
    "5. **Word Count Reduction Check**: After processing, if the word count of a section is reduced below a certain threshold (`reduction_threshold`), the section is further processed using the OpenAI client.\n",
    "6. **Compilation of Processed Sections**: All processed sections that are not empty are compiled into a list.\n",
    "7. **Statistics**: The function prints the number of sections that were further split and the number of sections that were significantly reduced in word count.\n",
    "  \n",
    "#### Output:\n",
    "- Returns a list of strings, where each string is a processed section of the original markdown file. This list represents the cleaned and potentially AI-processed sections of the markdown content.\n",
    "\n",
    "#### Note\n",
    "- We've observed that some text chunks, post table syntax removal processing, contain only headers. To enhance the relevance of the output, you can eliminate these header-only chunks by setting a minimum length requirement for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87232ac-0f06-4d91-b951-bab710e141f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(content, client_openAI):\n",
    "    # Constants and counters\n",
    "    max_word_count_for_table = 25\n",
    "    max_word_count = 4096\n",
    "    reduction_threshold = 0.30\n",
    "    further_splitted_count = 0\n",
    "    significantly_reduced_count = 0\n",
    "\n",
    "    # Splitting the content\n",
    "    sections = content.split('##')\n",
    "    intermediate_sections = []\n",
    "\n",
    "    for i, section in enumerate(sections):\n",
    "        if i == 0 and not section.strip():\n",
    "            continue\n",
    "\n",
    "        # Add '##' back to the section header\n",
    "        if not section.lstrip().startswith('#'):\n",
    "            section = '##' + section\n",
    "\n",
    "        # Split large sections using '###'\n",
    "        if len(section.split()) > max_word_count:\n",
    "            sub_sections = section.split('###')\n",
    "            for sub_section in sub_sections:\n",
    "                if len(sub_section.split()) > max_word_count:\n",
    "                    further_splitted_sub_sections = split_large_section(sub_section, max_word_count)\n",
    "                    further_splitted_count += len(further_splitted_sub_sections) - 1\n",
    "                    intermediate_sections.extend(further_splitted_sub_sections)\n",
    "                else:\n",
    "                    intermediate_sections.append(sub_section)\n",
    "        else:\n",
    "            intermediate_sections.append(section)\n",
    "\n",
    "    # Process each section for table content and check word count reduction\n",
    "    cleaned_sections = []\n",
    "    for section in intermediate_sections:\n",
    "        original_word_count = len(section.split())\n",
    "        processed_section = process_for_table_content(section, max_word_count_for_table)\n",
    "\n",
    "        # Calculate word count reduction\n",
    "        processed_word_count = len(processed_section.split())\n",
    "        if processed_word_count == 0 or processed_word_count / original_word_count < reduction_threshold:\n",
    "            significantly_reduced_count += 1\n",
    "            # Use OpenAI-based processing for sections that are significantly reduced\n",
    "            temp_processed_section = clean_text_from_table_syntax_with_openAI(section, client_openAI)\n",
    "            if temp_processed_section:\n",
    "                processed_section = temp_processed_section\n",
    "\n",
    "        if processed_section:\n",
    "            cleaned_sections.append(processed_section)\n",
    "\n",
    "    print(f\"Number of chunks further split: {further_splitted_count}\")\n",
    "    print(f\"Number of significantly reduced chunks: {significantly_reduced_count}\")\n",
    "\n",
    "    return cleaned_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34ff65-87c8-41c0-a425-652d750ba013",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `split_large_section` function is designed to split a large text section into smaller chunks based on a specified maximum word count. This function is particularly useful for processing large blocks of text that need to be broken down for readability or specific processing requirements.\n",
    "\n",
    "#### Inputs:\n",
    "- `section`: A string representing the text section to be split.\n",
    "- `max_word_count`: An integer specifying the maximum word count for each chunk.\n",
    "\n",
    "#### Output:\n",
    "- Returns a list of strings, where each string represents a chunk of the original section. Each chunk contains words up to the specified `max_word_count`, ensuring no chunk exceeds this limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19c1d61-76af-4ca9-b787-12061623f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_section(section, max_word_count):\n",
    "    words = section.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk)) > max_word_count:\n",
    "            chunks.append(' '.join(current_chunk[:-1]))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654e832-904a-4d75-ba6e-347b6403c991",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `process_for_table_content` function is designed to filter and process text sections, specifically targeting content structured like tables. It aims to retain meaningful content while considering a maximum word count for each processed chunk.\n",
    "\n",
    "#### Inputs:\n",
    "- `section`: A string representing the text section to be processed. This section typically contains markdown content.\n",
    "- `max_word_count_for_table`: An integer specifying the maximum word count for each chunk within a table-like structure.\n",
    "\n",
    "#### Output:\n",
    "- Returns a string that represents the processed section. This string is composed of filtered lines that meet the criteria of having an appropriate word count and not being markdown headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4493d69-fc1c-4bbe-8628-7379d6691ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_table_content(section, max_word_count_for_table):\n",
    "    lines = [line for line in section.split('\\n') if line.strip() and not line.strip().startswith('##') and not line.strip().startswith('###')]\n",
    "    filtered_lines = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        end_index = min(i + 4, len(lines))\n",
    "        word_count = sum(len(line.split()) for line in lines[i:end_index])\n",
    "\n",
    "        if word_count >= max_word_count_for_table or end_index - i < 4:\n",
    "            filtered_lines.extend(lines[i:end_index])\n",
    "        i = end_index\n",
    "\n",
    "    return '\\n'.join(filtered_lines).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafd668-5b8a-44da-b6ea-4468ddb45ed8",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `clean_text_from_table_syntax_with_openAI` function is designed to process a text chunk, particularly focusing on cleaning and formatting text from table-like syntax, using the OpenAI API for advanced processing. This function is ideal for refining and simplifying complex text structures.\n",
    "\n",
    "#### Inputs:\n",
    "- `text_chunk`: A string representing the text chunk to be processed. It is expected to be potentially complex or table-like in structure.\n",
    "- `client_openAI`: An OpenAI client object used to process the text chunk.\n",
    "\n",
    "#### Output:\n",
    "- Returns the cleaned and processed text as a string if a valid 'cleaned_context' is extracted from the OpenAI client's response.\n",
    "- Returns an empty list if the input is invalid, or if the necessary data isn't found in the OpenAI response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dfe991a-0063-4352-95b3-f8c90501306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_from_table_syntax_with_openAI(text_chunk, client_openAI):\n",
    "    # Validate input\n",
    "    if not isinstance(text_chunk, str):\n",
    "        return []\n",
    "\n",
    "    input_data = [Context(context=text_chunk)]\n",
    "    output_openAI = client_openAI.run(input_data)\n",
    "\n",
    "    # Check if 'output' is in the first item of the output_openAI list\n",
    "    if isinstance(output_openAI, list) and len(output_openAI) > 0 and 'output' in output_openAI[0]:\n",
    "        first_output = output_openAI[0]['output']\n",
    "\n",
    "        # Check if first_output is a list and not empty\n",
    "        if isinstance(first_output, list) and len(first_output) > 0:\n",
    "            first_response = first_output[0]\n",
    "\n",
    "            # Check if 'response' is in the first_response and it's not empty\n",
    "            if isinstance(first_response, dict) and 'response' in first_response and isinstance(first_response['response'], list) and len(first_response['response']) > 0:\n",
    "                first_responses = first_response['response'][0]\n",
    "\n",
    "                # Check if 'responses' is in first_responses and it has at least two elements\n",
    "                if isinstance(first_responses, dict) and 'responses' in first_responses and isinstance(first_responses['responses'], list) and len(first_responses['responses']) > 1:\n",
    "                    cleaned_context = first_responses['responses'][1].get('cleaned_context')\n",
    "\n",
    "                    # Check if cleaned_context is not None\n",
    "                    if cleaned_context is not None:\n",
    "                        return cleaned_context\n",
    "\n",
    "    return []  # Return an empty list if the conditions are not met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310e685-8fb3-4181-9998-a5cdba276469",
   "metadata": {},
   "source": [
    "Create OpenAI client instance from uniflow, for further usage of process_mmd_file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b300ed0-de87-40b2-84fe-a55d12d270db",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt_openAI = PromptTemplate(\n",
    "instruction=\"\"\"Revise the original text, focusing on fully retaining the core textual content while removing elements resembling table \n",
    "syntax, including lines with a single number and a sign. Preserve headers like '##' and '###' in markdown format. Follow the format of the \n",
    "examples below to include original_context and cleaned_context in the response, under the 'responses' key in the JSON object.\"\"\",   \n",
    "few_shot_prompt=[\n",
    "    Context(\n",
    "        original_context=\"Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. \\[\\text{NON-U.S. RETAIL STORES}\\] Shannon introduced the concept of\\ninformation entropy for the first time. \\[\\frac{\\text{$}}{\\text{$}}\\]. \\n21%\\n507\\n25%\\n25%\\n\",\n",
    "        cleaned_context=\"Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. Shannon introduced the concept of\\ninformation entropy for the first time.\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "config_openAI = TransformOpenAIConfig(\n",
    "    prompt_template=guided_prompt_openAI,\n",
    "    model_config=OpenAIModelConfig(response_format={\"type\": \"json_object\"}),\n",
    ")\n",
    "\n",
    "client_openAI = TransformClient(config_openAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21f0bc-ff55-4d18-8691-c210537ec326",
   "metadata": {},
   "source": [
    "Number of chunks split by the extracted context using the helper function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25d8639d-d422-4624-92a0-d688c739120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.18s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.31s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.55s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.25s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.67s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.40s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.49s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.40s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.83s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.14s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.89s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.70s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [10:04<00:00, 604.85s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.91s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.13s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.22s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.88s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.96s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.19s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.90s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.44s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks further split: 0\n",
      "Number of significantly reduced chunks: 22\n",
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "page_contents = process_content(extract_context, client_openAI)\n",
    "print(len(page_contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccda576-f497-4e80-bda4-89b04e0105b2",
   "metadata": {},
   "source": [
    "### Prepare sample prompts\n",
    "\n",
    "First, we need to demonstrate sample prompts for LLM, those include instruction and sample json format. We do this by giving a sample instruction and list of `Context` examples to the `PromptTemplate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e365fc-19d3-4935-9d70-2697bc4ccf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_instruction = \"\"\"Generate one question and its corresponding answer based on the context. Following \\\n",
    "the format of the examples below to include only question and answer in the response with reasonable length.\"\"\"\n",
    "\n",
    "sample_examples = [\n",
    "        Context(\n",
    "            context=\"The quick brown fox jumps over the lazy dog.\",\n",
    "            question=\"What is the color of the fox?\",\n",
    "            answer=\"brown.\"\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"The quick brown fox jumps over the lazy black dog.\",\n",
    "            question=\"What is the color of the dog?\",\n",
    "            answer=\"black.\"\n",
    "        )]\n",
    "\n",
    "guided_prompt = PromptTemplate(\n",
    "    instruction=sample_instruction,\n",
    "    few_shot_prompt=sample_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230701b-af7e-41e5-a7d2-14fb63e3ff7a",
   "metadata": {},
   "source": [
    "Next, for the given page_contents above, we convert them to the Context class to be processed by uniflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af49ab6d-b63f-465c-b68d-2a29df94fbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(context='The following tables set forth certain information regarding our Executive Officers and Directors as of January 25, 2023:\\n\\\\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt}}\\n**Information About Our Executive Officers** \\\\\\\\ \\\\end{tabular}\\n\\\\begin{tabular}{p{113.8pt} p{113.8pt}}\\n**Name** & **Age** & **Position** \\\\\\\\ Jeffrey P. Bezos & 59 & Executive Chair \\\\\\\\ Andrew R. Jassy & 55 & President and Chief Executive Officer \\\\\\\\ Douglas J. Herrington & 56 & CEO Worldwide Amazon Stores \\\\\\\\ Brian T. Olsavsky & 59 & Senior Vice President and Chief Financial Officer \\\\\\\\ Shelley L. Reynolds & 58 & Vice President, Worldwide Controller, and Principal Accounting Officer \\\\\\\\ Adam N. Selipsky & 56 & CEO Amazon Web Services \\\\\\\\ David A. Zapolsky & 59 & Senior Vice President, General Counsel, and Secretary \\\\\\\\ \\\\end{tabula', summary=''),\n",
       " Context(context='# The Variability in Our Retail Business Places Increased Strain on Our Operations\\nDemand for our products and services can fluctuate significantly for many reasons, including as a result of seasonality, promotions, product launches, or unforeseeable events, such as in response to global economic conditions such as recessionary fears or rising inflation, natural or human-caused disasters (including public health crises) or extreme weather (including as a result of climate change), or geopolitical events. For example, we expect a disproportionate amount of our retail sales to occur during our fourth quarter. Our failure to stock or restock popular products in sufficient amounts such that we fail to meet customer demand could significantly affect our revenue and our future growth. When we ov', summary=''),\n",
       " Context(context=\"# We Are Impacted by Fraudulent or Unlawful Activities of Sellers\\nThe law relating to the liability of online service providers is currently unsettled. In addition, governmental agencies have in the past and could in the future require changes in the way this business is conducted. Under our seller programs, we maintain policies and processes designed to prevent sellers from collecting payments, fraudulently or otherwise, when buyers never receive the products they ordered or when the products received are materially different from the sellers' descriptions, and to prevent sellers in our stores or through other stores from selling unlawful, counterfeit, pirated, or stolen goods, selling goods in an unlawful or unethical manner, violating the proprietary rights of others, or otherwise viola\", summary=''),\n",
       " Context(context='In addition to risks described elsewhere in this Item 1A relating to fulfillment network and inventory optimization by us and third parties, we are exposed to significant inventory risks that may adversely affect our operating results as a result of seasonality, new product launches, rapid changes in product cycles and pricing, defective merchandise, changes in customer demand and consumer spending patterns, changes in consumer tastes with respect to our products, spoilage, and other factors. We endeavor to accurately predict these trends and avoid overstocking or understocking products we manufacture and/or sell. Demand for products, however, can change significantly between the time inventory or components are ordered and the date of sale. In addition, when we begin selling or manufactur', summary=''),\n",
       " Context(context=\"We accept payments using a variety of methods, including credit card, debit card, credit accounts (including promotional financing), gift cards, direct debit from a customer's bank account, consumer invoicing, physical bank check, and payment upon delivery. For existing and future payment options we offer to our customers, we currently are subject to, and may become subject to additional, regulations and compliance requirements (including obligations to implement enhanced authentication processes that could result in significant costs and reduce the ease of use of our payments products), as well as fraud. For certain payment methods, including credit and debit cards, we pay interchange and other fees, which may increase over time and raise our operating costs and lower profitability. We re\", summary=''),\n",
       " Context(context='# Legal and Regulatory Risks\\n_Government Regulation Is Evolving and Unfavorable Changes Could Harn Our Business_\\nWe are subject to general business regulations and laws, as well as regulations and laws specifically governing the Internet, physical, e-commerce, and omnichannel retail, digital content, web services, electronic devices, advertising, artificial intelligence technologies and services, and other products and services that we offer or sell. These regulations and laws cover taxation, privacy, data use, data protection, data security, data localization, network security, consumer protection, pricing, content, copyrights, distribution, transportation, mobile communications, electronic device certification, electronic waste, energy consumption, environmental regulation, electronic co', summary=''),\n",
       " Context(context='We are subject to income taxes in the U.S. (federal and state) and numerous foreign jurisdictions. Tax laws, regulations, administrative practices, principles, and interpretations in various jurisdictions may be subject to significant change, with or without notice, due to economic, political, and other conditions, and significant judgment is required in evaluating and estimating our provision and accruals for these taxes. There are many transactions that occur during the ordinary course of business for which the ultimate tax determination is uncertain. In addition, our actual and forecasted earnings are subject to change due to economic, political, and other conditions and significant judgment is required in determining our ability to use our deferred tax assets.\\nOur effective tax rates c', summary=''),\n",
       " Context(context='Cash flow information is as follows (in millions):\\n\\\\begin{tabular}{l c c c} \\\\multicolumn{4}{c}{**Year Ended December 31,**} \\\\\\\\ \\\\cline{2-4} \\\\multicolumn{1}{c}{} & **2021** & **2022** \\\\\\\\ \\\\hline \\\\multicolumn{4}{c}{Cash provided by (used in):} \\\\\\\\ \\\\hline Operating activities & \\\\$ & 46,327 & \\\\$ & 46,752 \\\\\\\\ \\\\multicolumn{4}{c}{} \\\\\\\\ Investing activities & & (\\\\$8,154) & (\\\\$7,601) \\\\\\\\ \\\\hline \\\\multicolumn{4}{c}{} \\\\\\\\ \\\\end{tabular}\\n\\\\begin{tabular}{l c c} \\\\multicolumn{4}{c}{Operating activities} \\\\\\\\ \\\\hline Investing activities & & (\\\\$8,154) & (\\\\$7,601) \\\\\\\\ \\\\hline \\\\multicolumn{4}{c}{} \\\\\\\\ \\\\end{tabular}\\n\\\\begin{tabular}{l c c} \\\\multicolumn{4}{c}{Financing activities} \\\\\\\\ \\\\hline \\\\end{tabular}\\nOur principal sources of liquidity are cash flows generated from operations and our cash, cash equivalents, and marketable', summary=''),\n",
       " Context(context='# Results of Operations\\nWe have organized our operations into three segments: North America, International, and AWS. These segments reflect the way the Company evaluates its business performance and manages its operations. See Item 8 of Part II, \"Financial Statements and Supplementary Data -- Note 10 -- Segment Information.\"', summary=''),\n",
       " Context(context='Macroeconomic factors, including inflation, increased interest rates, significant capital market volatility, the prolonged COVID-19 pandemic, global supply chain constraints, and global economic and geopolitical developments, have direct and indirect impacts on our results of operations that are difficult to isolate and quantify. These factors contributed to increases in our operating costs during 2022, particularly across our North America and International segments, primarily due to a return to more normal, seasonal demand volumes in relation to our fulfillment network fixed costs, increased transportation and utility costs, and increased wage rates. In addition, rising fuel, utility, and food costs, rising interest rates, and recessionary fears may impact customer demand and our ability', summary='')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ Context(context=p[:800], summary=\"\") for p in page_contents[6:16] if len(p) > 200 ]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398300-09f3-4420-a8a0-335a3c32ecbb",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [HuggingfaceModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L39)'s default LLM to generate questions and answers. Let's import the config and client of this model.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `HuggingfaceConfig` to use our customized instructions and examples, instead of the `uniflow` default ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88035b71-1055-4d4d-8e1c-35d4324f6afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "config = TransformHuggingFaceConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=HuggingfaceModelConfig(batch_size=128))\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85594569-5cdc-44f5-be1e-2b7116334d33",
   "metadata": {},
   "source": [
    "Now we call the run method on the client object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ada50841-d847-440e-b3e2-76a6261effa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                               | 0/1 [00:00<?, ?it/s]/opt/conda/envs/uniflow/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:39<00:00, 39.90s/it]\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfaac3-8895-48e4-8e72-34364037b0be",
   "metadata": {},
   "source": [
    "### Process the output\n",
    "\n",
    "Let's take a look of the generated output. We need to do a little postprocessing on the raw output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82d9ea69-1b00-41ae-9638-28b33363182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': ' The following tables set forth certain information regarding our '\n",
      "            'Executive Officers and Directors as of January 25, 2023:\\n'\n",
      "            '\\\\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt}}\\n'\n",
      "            '**Information About Our Executive Officers** \\\\\\\\ \\\\end{tabular}\\n'\n",
      "            '\\\\begin{tabular}{p{113.8pt} p{113.8pt}}\\n'\n",
      "            '**Name** & **Age** & **Position** \\\\\\\\ Jeffrey P. Bezos & 59 & '\n",
      "            'Executive Chair \\\\\\\\ Andrew R. Jassy & 55 & President and Chief '\n",
      "            'Executive Officer \\\\\\\\ Douglas J. Herrington & 56 & CEO Worldwide '\n",
      "            'Amazon Stores \\\\\\\\ Brian T. Olsavsky & 59 & Senior Vice President '\n",
      "            'and Chief Financial Officer \\\\\\\\ Shelley L. Reynolds & 58 & Vice '\n",
      "            'President, Worldwide Controller, and Principal Accounting Officer '\n",
      "            '\\\\\\\\ Adam N. Selipsky & 56 & CEO Amazon Web Services \\\\\\\\ David '\n",
      "            'A. Zapolsky & 59 & Senior Vice President, General Counsel, and '\n",
      "            'Secretary \\\\\\\\ \\\\end{tabula\\n',\n",
      " 'question': ' Who is the CEO of Amazon Worldwide Stores?\\n',\n",
      " 'answer': ' Douglas J. Herrington.'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "keywords = [\"context:\", \"question:\", \"answer:\"]\n",
    "pattern = '|'.join(map(re.escape, keywords))\n",
    "\n",
    "o = output[0]['output'][0]['response'][0] ## we only postprocess the first output\n",
    "segments = [segment for segment in re.split(pattern, o) if segment.strip()]\n",
    "result = {\n",
    "    \"context\": segments[-3].rstrip(\"summary:   \"),\n",
    "    \"question\": segments[-2],\n",
    "    \"answer\": segments[-1]\n",
    "}\n",
    "\n",
    "pprint(result, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d05cf-f9a6-4f51-a543-197bded734d2",
   "metadata": {},
   "source": [
    "Congrats! Your question answers from the given knowledge context are generated!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015bb33a-fef2-43aa-a147-2ff86c510ea0",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
