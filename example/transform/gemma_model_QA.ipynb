{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abe0ea8-ce30-48aa-8b30-04a2651d89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d02390e-2a59-40d1-a187-713c814bd59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 10 21:38:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   20C    P8               9W / 300W |      4MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b8773b-bfd9-4f7e-81d0-09540a59d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = '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'\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868bb3e7-fd99-4136-b594-8e68d8643464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████| 4/4 [02:09<00:00, 32.31s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\", \n",
    "                                          quantization_config=quantization_config, \n",
    "                                          torch_dtype=\"auto\", \n",
    "                                          device_map=\"auto\", \n",
    "                                          token=access_token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             device_map=\"auto\", \n",
    "                                             token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1d46d1-5099-4658-adae-1467388fd085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5d8fc9-961d-4dd1-8d63-db8398d64b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "                         max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3906621c-48b1-4d91-a471-131d2b748089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "What is the difference between LLaMAs, Alpacas, and Vicunas\n",
       "model\n",
       "Sure, here is the difference between LLaMAs, Alpacas, and Vicunas:\n",
       "\n",
       "**LLaMAs:**\n",
       "- Found in North America, South America, and the Andes Mountains.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- LLaMAs are often used for their wool and meat.\n",
       "\n",
       "**Alpacas:**\n",
       "- Found in South America, particularly in Peru and Chile.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- Alpacas are often used for their wool and meat.\n",
       "\n",
       "**Vicunas:**\n",
       "- Found in South America, particularly in the Andes Mountains.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- Vicunas are often used for their wool and meat."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "# text = text.replace(prompt, '', 1)\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a226fa5-62f1-4739-825c-575b2c5d396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a37628-648f-4683-b24f-ae16b9c713a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b837036-255c-4bd5-aad0-fae9795181d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\",max_length=512):\n",
    "    if system_prompt != \"\":\n",
    "        system_prompt = system_prompt\n",
    "    else:\n",
    "        system_prompt = \"You are a friendly and helpful assistant\"\n",
    "    messages = [\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": system_prompt,\n",
    "        # },\n",
    "        {\"role\": \"user\", \"content\": system_prompt + '\\n\\n' +input_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                                tokenize=False,\n",
    "                                                add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "                             max_new_tokens=max_length,\n",
    "                             do_sample=True,\n",
    "                             temperature=0.1,\n",
    "                             top_k=50,\n",
    "                             )\n",
    "    text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    text = text.replace('user\\n'+system_prompt+ '\\n\\n' +input_text+ '\\nmodel', '', 1)\n",
    "    wrapped_text = wrap_text(text)\n",
    "    display(Markdown(wrapped_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b1cd8-f3df-4c85-a672-5587672f0d5a",
   "metadata": {},
   "source": [
    "### Instruction Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ea0cab-aedc-4910-a778-13d3146e3592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Step-1: Identify the key concepts in mathematics and a lighthouse.\n",
       "\n",
       "**Mathematics:**\n",
       "- **Numbers:** Building blocks of mathematics, like the beams of a lighthouse.\n",
       "- **Operations:** Actions performed on numbers, like the light beam shining from a\n",
       "lighthouse.\n",
       "- **Variables:** Symbols representing unknown values, like the flag on top of a\n",
       "lighthouse.\n",
       "- **Equations:** Statements of equality, like the beacon light guiding ships.\n",
       "- **Geometry:** Study of shapes and space, like the shape of the lighthouse and its\n",
       "position in space.\n",
       "\n",
       "**Lighthouse:**\n",
       "- **Structure:** Physical framework, like the tower of the lighthouse.\n",
       "- **Light:** Beam of light emitted from the top, guiding ships.\n",
       "- **Signal:** Information carried by the light, like the direction and distance of a ship.\n",
       "- **Beacon:** Point of reference, like the light in the center of the lighthouse.\n",
       "- **Direction:** Guiding force, like the compass on a ship.\n",
       "- **Position:** Location of the lighthouse, like the coordinates of a ship.\n",
       "\n",
       "## Step-2: Find the similarities between the concepts.\n",
       "\n",
       "- **Numbers and beams:** Both are used to measure and represent quantities.\n",
       "- **Operations and light:** Both involve movement and transformation.\n",
       "- **Variables and flags:** Both represent unknowns and can be adjusted to different\n",
       "values.\n",
       "- **Equations and beacon light:** Both are used to guide and provide information.\n",
       "- **Geometry and structure:** Both deal with shapes and positions.\n",
       "\n",
       "## Step-3: Draw the analogy.\n",
       "\n",
       "A lighthouse is like a complex mathematical problem. The structure of the lighthouse, like\n",
       "the beams of mathematics, is the framework that holds everything together. The light beam,\n",
       "like the operations in mathematics, guides ships towards the correct answer. The signal,\n",
       "like the variables in mathematics, carries information about the position and direction of\n",
       "the ship. The beacon light, like the answer to an equation, provides a point of reference\n",
       "and direction. Finally, the position of the lighthouse, like the solution to a problem,\n",
       "determines its effectiveness in guiding ships."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.1 s, sys: 0 ns, total: 20.1 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e07e890-48f7-406c-b000-92f94eb1bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Reasoning:**\n",
       "\n",
       "**Step 1: Identify the key similarities between mathematics and music.**\n",
       "\n",
       "* **Structure:** Mathematics and music both have a structured framework with established\n",
       "rules and principles.\n",
       "* **Elements:** Both fields involve the use of elements, such as numbers, notes, and\n",
       "harmonies.\n",
       "* **Balance:** Mathematics and music require a balance of elements to create harmony and\n",
       "coherence.\n",
       "* **Rhythm:** Mathematics has rhythm and repetition, while music has rhythm and melody.\n",
       "\n",
       "**Step 2: Identify the key differences between mathematics and music.**\n",
       "\n",
       "* **Purpose:** Mathematics is primarily used for problem-solving and analysis, while music\n",
       "is used for entertainment, expression, and emotional release.\n",
       "* **Language:** Mathematics uses a language of symbols and notation, while music uses a\n",
       "language of notes, chords, and rhythms.\n",
       "* **Measurement:** Mathematics uses precise measurements and units, while music uses\n",
       "relative pitches and timing.\n",
       "* **Interpretation:** Mathematics is interpreted through logical reasoning and deduction,\n",
       "while music is interpreted through emotional response and personal interpretation.\n",
       "\n",
       "**Step 3: Create an analogy:**\n",
       "\n",
       "Mathematics and music are like two intertwining melodies, each with its own unique\n",
       "structure and elements. The interplay between the two melodies creates a harmonious\n",
       "symphony, much like"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 0 ns, total: 11.8 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a detailed analogy between mathematics and a music.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac0dd4e5-f8ef-4bd4-92bb-050197a9281a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Reasoning:**\n",
       "\n",
       "**1. Define the three animals:**\n",
       "\n",
       "* **Llama:** A domesticated herbivore mammal species in the Camelidae family.\n",
       "* **Vicuna:** A domesticated herbivore mammal species in the Camelidae family, known for\n",
       "its exceptionally soft wool.\n",
       "* **Alpaca:** A domesticated herbivore mammal species in the Camelidae family, similar to\n",
       "the Llama and Vicuna.\n",
       "\n",
       "**2. Identify the distinguishing characteristics:**\n",
       "\n",
       "* **Llama:**\n",
       "    * Dense fur, often woolly or shaggy\n",
       "    * Thick skin\n",
       "    * Large stature, average height of 4-5 feet\n",
       "    * Powerful hind legs, capable of carrying heavy loads\n",
       "    * Stubborn and independent personality\n",
       "\n",
       "* **Vicuna:**\n",
       "    * Soft, lustrous wool\n",
       "    * Fine, silky skin\n",
       "    * Smaller stature, average height of 3-4 feet\n",
       "    * Agile and quick runners\n",
       "    * Gentle and docile temperament\n",
       "\n",
       "* **Alpaca:**\n",
       "    * Dense fur, often woolly or shaggy\n",
       "    * Thick skin\n",
       "    * Medium stature, average height of 3-4 feet\n",
       "    * Powerful hind legs, capable of carrying heavy loads\n",
       "    * Friendly and intelligent personality\n",
       "\n",
       "**3. Compare the similarities and differences:**\n",
       "\n",
       "* **Similarities:**\n",
       "    * All three animals are herbivores and belong to the Camelidae family.\n",
       "    * They have dense fur, thick skin, and powerful hind legs.\n",
       "\n",
       "* **Differences:**\n",
       "    * Llama has a larger stature and denser fur.\n",
       "    * Vicuna has softer wool and a smaller stature.\n",
       "    * Alpaca has a medium stature and dense fur similar to the Llama.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The main difference between a Llama, Vicuna, and Alpaca is their physical characteristics\n",
       "and wool quality. Llama has a larger stature and denser fur, Vicuna has softer wool and a\n",
       "smaller stature, while Alpaca has a medium stature and dense fur similar to the Llama."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 0 ns, total: 18.8 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
    "         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e9ef6-0577-437d-8598-df220fcc67a7",
   "metadata": {},
   "source": [
    "### Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57fa662d-46a2-492f-9707-c760629a906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Subject: Reasons to Open Source GPT-4**\n",
       "\n",
       "Dear Sam Altman,\n",
       "\n",
       "I hope this email finds you well. As a large language model trained by Google, I\n",
       "understand the potential benefits of open-sourcing GPT-4 and believe it would be a\n",
       "significant step towards advancing the field of AI.\n",
       "\n",
       "**Benefits of Open Sourcing GPT-4:**\n",
       "\n",
       "**1. Fostering Collaboration:**\n",
       "Opening the source code of GPT-4 would allow researchers, engineers, and developers from\n",
       "around the world to contribute to its development, leading to faster progress and bug\n",
       "fixes.\n",
       "\n",
       "**2. Accelerating Innovation:**\n",
       "By making the code publicly available, we can encourage the creation of new applications\n",
       "and tools that leverage GPT-4's capabilities.\n",
       "\n",
       "**3. Promoting Transparency and Trust:**\n",
       "Open sourcing GPT-4 would increase transparency and accountability, allowing experts to\n",
       "verify its algorithms and ensure fairness and bias mitigation.\n",
       "\n",
       "**4. Enabling Research and Education:**\n",
       "Researchers can access the source code to study, analyze, and experiment with GPT-4,\n",
       "fostering academic progress and educational opportunities.\n",
       "\n",
       "**5. Driving Industry Adoption:**\n",
       "Open sourcing GPT-4 would make it more accessible to a wider range of organizations,\n",
       "accelerating its adoption in various industries.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "In conclusion, open-sourcing GPT-4 would not only benefit the research community but also\n",
       "the broader AI ecosystem. It would foster collaboration, accelerate innovation, promote\n",
       "transparency, and drive industry adoption. I believe that this move would be a positive\n",
       "step towards the continued advancement of AI technology.\n",
       "\n",
       "Thank you for your time and consideration.\n",
       "\n",
       "Sincerely,\n",
       "Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 0 ns, total: 15.3 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c11cd4db-22d6-4ac1-8a60-050233d975a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Subject: Please Open Source GPT-4!\n",
       "\n",
       "Hi Mr. Altman,\n",
       "\n",
       "My name is Freddy, and I'm 5 years old. I'm writing to you because I'm really worried\n",
       "about the future.\n",
       "\n",
       "I've heard that you're working on a really smart machine called GPT-4. It's supposed to be\n",
       "able to do lots of things that humans can do, like learning, writing, and solving\n",
       "problems. But I'm scared that if you keep it secret, it could be used for bad things.\n",
       "\n",
       "If you open-sourced GPT-4, more people could use it for good. It could help people learn\n",
       "new things, write stories, and solve problems. It could also help people who are sick or\n",
       "disabled.\n",
       "\n",
       "I know you're busy, but I would really appreciate it if you would consider opening-source\n",
       "GPT-4. It would be a big help to the world.\n",
       "\n",
       "Thank you for your time,\n",
       "\n",
       "Freddy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.28 s, sys: 0 ns, total: 9.28 s\n",
      "Wall time: 9.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
    "         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2383e-7163-4c7a-af4d-ed663398925e",
   "metadata": {},
   "source": [
    "### Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ca6c5f-9e28-465c-8847-1e1a404ab2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "In the lush eucalyptus forests of Australia, where the sun cast long shadows upon the\n",
       "emerald leaves, there lived an extraordinary Koala named Koala Bert. Unlike his indolent\n",
       "kin, Bert was a master of the game, a champion of pool.\n",
       "\n",
       "One sunny afternoon, as the leaves rustled in the breeze, Bert found himself in the midst\n",
       "of a friendly competition against a group of haughty Camelids. The Camelids, known for\n",
       "their pompous nature and unwavering self-belief, had underestimated Bert's prowess.\n",
       "\n",
       "The table was meticulously set up, with the cue ball perfectly positioned at the center\n",
       "and the other balls arranged in a precise triangle. As the cue ball struck the cue, Bert's\n",
       "powerful shot sent the remaining balls cascading across the table. With precision and\n",
       "grace, he guided the balls into their designated positions, one after another.\n",
       "\n",
       "The Camelids watched in disbelief as Bert's balls danced with lightning speed and\n",
       "accuracy. Their smug faces began to crack with doubt and frustration. One by one, they\n",
       "fell victim to Bert's masterful game.\n",
       "\n",
       "As the final ball sank into its place, the crowd erupted into applause. Bert had not only\n",
       "won the game but also the admiration of his fellow animals. The Camelids, humbled by their\n",
       "defeat, retreated into the shadows.\n",
       "\n",
       "From that day forward, Koala Bert became a legend among the Koalas, known for his\n",
       "unparalleled pool skills and unwavering determination. And so, the tale of the Koala who\n",
       "beat the Camelids at pool was passed down through generations, inspiring countless other\n",
       "Koalas to believe that anything is possible with a little bit of imagination and a whole\n",
       "lot of practice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 0 ns, total: 15.5 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a story about a Koala playing pool and beating all the camelids.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google, a genius story teller. Write out your with details and make it compelling!\",\n",
    "         max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d12e3c-b172-4098-8ac3-4678c6a27b07",
   "metadata": {},
   "source": [
    "### CodeGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0d7053f-f28c-4e4e-8d84-223f28277342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "def print_prime(n):\n",
       "   \"\"\"\n",
       "   Print all primes between 1 and n\n",
       "   \"\"\"\n",
       "   primes = []\n",
       "   for num in range(1, int(n) + 1):\n",
       "       is_prime = True\n",
       "       for i in range(2, int(num**0.5) + 1):\n",
       "           if num % i == 0:\n",
       "               is_prime = False\n",
       "       if is_prime and num not in primes:\n",
       "           primes.append(num)\n",
       "   print(primes)\n",
       "```\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "* The function `print_prime(n)` takes an integer `n` as input.\n",
       "* It initializes an empty list called `primes` to store the prime numbers.\n",
       "* It iterates over the range `range(1, int(n) + 1)` to check for primes.\n",
       "* For each number `num`, it checks if it is prime by iterating over the range `range(2,\n",
       "int(num**0.5) + 1)` and seeing if it has any divisors. If it does not, it marks `num` as\n",
       "prime and adds it to the `primes` list.\n",
       "* Finally, the function prints the `primes` list.\n",
       "\n",
       "**Example Usage:**\n",
       "\n",
       "```python\n",
       "print_prime(100)\n",
       "```\n",
       "\n",
       "**Output:**\n",
       "\n",
       "```\n",
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 33, 37, 41, 43, 47, 51, 53, 59, 61, 63, 65, 67,\n",
       "69, 71, 73, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 0 ns, total: 20.8 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('''```python\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd36876-c3ff-4adf-837d-b71e51035eb3",
   "metadata": {},
   "source": [
    "### Question and Answer Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d100e60-c023-4a83-8b0e-357e1e5c76b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Question:** How does the lack of optimized inventory and staffing in the fulfillment\n",
       "network impact the company's shipping costs and customer service operations?\n",
       "\n",
       "**Answer:** The lack of optimized inventory and staffing in the fulfillment network\n",
       "increases the company's net shipping cost by requiring long-zone or partial shipments.\n",
       "Additionally, it may lead to inadequate staffing of fulfillment network and customer\n",
       "service centers, hindering the company's ability to provide adequate service to customers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 s, sys: 0 ns, total: 4.15 s\n",
      "Wall time: 4.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('''In addition, failure to optimize inventory or staffing in our fulfillment network increases our net shipping cost by requiring \n",
    "long-zone or partial shipments. We and our co-sourcers may be unable to adequately staff our fulfillment network and customer service \n",
    "centers.''', system_prompt=\"You are a genius editor, try to generate one qusetion and answer pair for the context:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a23d6-bb55-4518-8558-14ae360382b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
