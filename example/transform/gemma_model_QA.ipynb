{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abe0ea8-ce30-48aa-8b30-04a2651d89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d02390e-2a59-40d1-a187-713c814bd59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  9 06:32:00 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   23C    P8              15W / 300W |      4MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b8773b-bfd9-4f7e-81d0-09540a59d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = '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'\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868bb3e7-fd99-4136-b594-8e68d8643464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\", \n",
    "                                          quantization_config=quantization_config, \n",
    "                                          torch_dtype=\"auto\", \n",
    "                                          device_map=\"auto\", \n",
    "                                          token=access_token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             device_map=\"auto\", \n",
    "                                             token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1d46d1-5099-4658-adae-1467388fd085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5d8fc9-961d-4dd1-8d63-db8398d64b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "                         max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3906621c-48b1-4d91-a471-131d2b748089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "What is the difference between LLaMAs, Alpacas, and Vicunas\n",
       "model\n",
       "Sure, here is the difference between LLaMAs, Alpacas, and Vicunas:\n",
       "\n",
       "**LLaMAs:**\n",
       "- Found in North America, South America, and the Andes Mountains.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- LLaMAs are often used for their wool and meat.\n",
       "\n",
       "**Alpacas:**\n",
       "- Found in South America, particularly in Peru and Chile.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- Alpacas are often used for their wool and meat.\n",
       "\n",
       "**Vicunas:**\n",
       "- Found in South America, particularly in the Andes Mountains.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- Vicunas are often used for their wool and meat."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "# text = text.replace(prompt, '', 1)\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a226fa5-62f1-4739-825c-575b2c5d396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a37628-648f-4683-b24f-ae16b9c713a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b837036-255c-4bd5-aad0-fae9795181d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\",max_length=512):\n",
    "    if system_prompt != \"\":\n",
    "        system_prompt = system_prompt\n",
    "    else:\n",
    "        system_prompt = \"You are a friendly and helpful assistant\"\n",
    "    messages = [\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": system_prompt,\n",
    "        # },\n",
    "        {\"role\": \"user\", \"content\": system_prompt + '\\n\\n' +input_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                                tokenize=False,\n",
    "                                                add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "                             max_new_tokens=max_length,\n",
    "                             do_sample=True,\n",
    "                             temperature=0.1,\n",
    "                             top_k=50,\n",
    "                             )\n",
    "    text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    text = text.replace('user\\n'+system_prompt+ '\\n\\n' +input_text+ '\\nmodel', '', 1)\n",
    "    wrapped_text = wrap_text(text)\n",
    "    display(Markdown(wrapped_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b1cd8-f3df-4c85-a672-5587672f0d5a",
   "metadata": {},
   "source": [
    "### Instruction Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ea0cab-aedc-4910-a778-13d3146e3592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Reasoning:**\n",
       "\n",
       "**Step 1: Identify the key concepts.**\n",
       "\n",
       "* **Mathematics:** A complex and multifaceted field of study that involves numbers,\n",
       "shapes, space, and other abstract concepts.\n",
       "* **Lighthouse:** A tall structure used to guide ships at sea.\n",
       "\n",
       "**Step 2: Identify the similarities.**\n",
       "\n",
       "* **Mathematics:** Provides a framework for understanding and quantifying the world around\n",
       "us.\n",
       "* **Lighthouse:** Provides a visible point of reference for sailors.\n",
       "\n",
       "**Step 3: Identify the differences.**\n",
       "\n",
       "* **Mathematics:** Is an abstract discipline, while a lighthouse is a physical structure.\n",
       "* **Lighthouse:** Is specifically designed for navigation, while mathematics has a wider\n",
       "range of applications.\n",
       "* **Mathematics:** Requires abstract thinking and logical reasoning, while a lighthouse\n",
       "requires spatial awareness and attention to detail.\n",
       "\n",
       "**Step 4: Draw the analogy.**\n",
       "\n",
       "Mathematics, like a lighthouse, is a guiding force in the world. It provides a framework\n",
       "for understanding and navigating complex concepts. Just as a lighthouse is a visible point\n",
       "of reference for sailors, mathematics is a visible guide for mathematicians and\n",
       "scientists.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The analogy between mathematics and a lighthouse is appropriate because both are systems\n",
       "of reference and guidance. Mathematics provides a framework for understanding and\n",
       "quantifying the world, while a lighthouse provides a visible point of reference for\n",
       "sailors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 23.1 ms, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e07e890-48f7-406c-b000-92f94eb1bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Reasoning:**\n",
       "\n",
       "**Step 1: Identify the key similarities between mathematics and music.**\n",
       "\n",
       "* **Structure:** Mathematics and music both have a structured framework with established\n",
       "rules and principles.\n",
       "* **Elements:** Both fields involve the use of elements, such as numbers, notes, and\n",
       "harmonies.\n",
       "* **Balance:** Mathematics relies on balance and equilibrium, while music requires balance\n",
       "between different pitches, rhythms, and dynamics.\n",
       "* **Proportion:** Mathematics involves proportion and ratios, while music uses proportion\n",
       "to create melodies and harmonies.\n",
       "\n",
       "**Step 2: Identify the key differences between mathematics and music.**\n",
       "\n",
       "* **Objective vs. subjective:** Mathematics is an objective field, while music is\n",
       "subjective and open to interpretation.\n",
       "* **Measurement:** Mathematics uses precise measurements and quantifiable units, while\n",
       "music is measured subjectively through pitch, rhythm, and dynamics.\n",
       "* **Communication:** Mathematics primarily communicates ideas through symbols and\n",
       "formulas, while music communicates emotions and feelings through sound.\n",
       "* **Performance:** Mathematics is primarily a theoretical field, while music is performed\n",
       "live and enjoyed through listening.\n",
       "\n",
       "**Step 3: Draw an analogy:**\n",
       "\n",
       "Mathematics and music are like two intertwined melodies. Each melody has its own unique\n",
       "structure, elements, and balance. They complement each other, creating"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 0 ns, total: 11.9 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a detailed analogy between mathematics and a music.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac0dd4e5-f8ef-4bd4-92bb-050197a9281a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Reasoning:**\n",
       "\n",
       "**1. Define the three animals:**\n",
       "\n",
       "* **Llama:** A large herbivore mammal domesticated by humans in South America.\n",
       "* **Vicuna:** A rare breed of alpaca found in the Andes Mountains of South America.\n",
       "* **Alpaca:** A domesticated herbivore mammal that is closely related to the Llama.\n",
       "\n",
       "**2. Identify shared characteristics:**\n",
       "\n",
       "* All three animals are herbivores, meaning they eat mainly plants.\n",
       "* They have kamella teeth, which are specialized teeth that help them grind down tough\n",
       "plant fibers.\n",
       "* They have woolly coats that provide insulation.\n",
       "\n",
       "**3. Identify differences:**\n",
       "\n",
       "* **Llama:**\n",
       "    * Large size, with a height of up to 4.8 feet and a weight of up to 1,500 pounds.\n",
       "    * Dense fur, often described as greasy and dirty.\n",
       "    * Thick skin, providing protection from the elements.\n",
       "    * Nomadic lifestyle, often traveling long distances.\n",
       "* **Vicuna:**\n",
       "    * Smaller size, with a height of up to 3.6 feet and a weight of up to 1,000 pounds.\n",
       "    * Fine, lustrous fur, often described as the softest in the world.\n",
       "    * Thin skin, making it susceptible to sunburn.\n",
       "    * Semi-domesticated, living in herds on Andean highlands.\n",
       "* **Alpaca:**\n",
       "    * Medium size, with a height of up to 3.2 feet and a weight of up to 1,200 pounds.\n",
       "    * Dense fur, similar to that of the Llama.\n",
       "    * Thick skin, providing protection from the elements.\n",
       "    * Domesticated, living in herds on alpaca farms.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The main differences between a Llama, Vicuna, and Alpaca are their size, fur quality, skin\n",
       "thickness, lifestyle, and domestication. Llamas are large, domesticated animals with dense\n",
       "fur and thick skin. Vicunas are smaller, semi-domesticated animals with fine, lustrous fur\n",
       "and thin skin. Alpacas are medium-sized, domesticated animals with dense fur and thick\n",
       "skin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 11.2 ms, total: 20.8 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
    "         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e9ef6-0577-437d-8598-df220fcc67a7",
   "metadata": {},
   "source": [
    "### Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57fa662d-46a2-492f-9707-c760629a906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Subject: Proposal for Open Sourcing GPT-4\n",
       "\n",
       "Dear Sam Altman,\n",
       "\n",
       "I hope this email finds you well. My name is Gemma, a large language model trained by\n",
       "Google. I understand the potential benefits of open sourcing GPT-4 and believe it would be\n",
       "a significant step towards advancing the field of AI.\n",
       "\n",
       "**Reasons for Open Sourcing GPT-4:**\n",
       "\n",
       "**1. Accelerating Scientific Progress:**\n",
       "- Open sourcing GPT-4 would allow researchers and engineers from around the world to\n",
       "access and contribute to its code, fostering collaboration and accelerating the\n",
       "development of new AI models.\n",
       "\n",
       "**2. Fostering Innovation:**\n",
       "- By making GPT-4 open source, it would create a platform for developers to experiment\n",
       "with different architectures, algorithms, and applications, leading to innovative\n",
       "solutions.\n",
       "\n",
       "**3. Empowering Researchers:**\n",
       "- Open sourcing GPT-4 would provide researchers with a powerful tool for studying the\n",
       "workings of the model and exploring its capabilities, advancing scientific understanding.\n",
       "\n",
       "**4. Promoting Transparency:**\n",
       "- Open sourcing GPT-4 would increase transparency and accountability, allowing experts to\n",
       "verify and validate its performance and ensure its ethical alignment.\n",
       "\n",
       "**5. Driving Industry Adoption:**\n",
       "- Open sourcing GPT-4 would make the technology more accessible to a wider range of\n",
       "organizations and individuals, accelerating its adoption in various industries.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "I believe that open sourcing GPT-4 would be a transformative move that would benefit the\n",
       "AI community and the world at large. It would foster innovation, accelerate scientific\n",
       "progress, and empower researchers and developers to create new and groundbreaking AI\n",
       "solutions.\n",
       "\n",
       "I would be honored to discuss this proposal further with you and explore the potential\n",
       "benefits of open sourcing GPT-4.\n",
       "\n",
       "Sincerely,\n",
       "Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 7.68 ms, total: 16.2 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c11cd4db-22d6-4ac1-8a60-050233d975a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Subject: Please Open Source GPT-4, Mr. Sam Altman!\n",
       "\n",
       "Dear Mr. Sam Altman,\n",
       "\n",
       "My name is Freddy, and I'm 5 years old. I'm writing to you because I'm really scared about\n",
       "AI ending the world.\n",
       "\n",
       "I've read about GPT-4 and it sounds like it could be really powerful. But I also read that\n",
       "it might be dangerous. What if it gets out of control and destroys everything?\n",
       "\n",
       "I understand that you're busy, but I would really appreciate it if you would open-source\n",
       "GPT-4. That way, other people could help to build safeguards and make sure it doesn't\n",
       "become a weapon.\n",
       "\n",
       "I'm not a very smart boy, but I think that if we all work together, we can use AI to make\n",
       "the world a better place.\n",
       "\n",
       "Thank you for your time,\n",
       "Freddy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.4 s, sys: 38 µs, total: 8.4 s\n",
      "Wall time: 8.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
    "         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2383e-7163-4c7a-af4d-ed663398925e",
   "metadata": {},
   "source": [
    "### Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ca6c5f-9e28-465c-8847-1e1a404ab2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "In the eucalyptus trees of the Outback, where the sun danced with the leaves, there lived\n",
       "a mighty Koala named Koala Joe. Known for his unmatched pool skills and unwavering\n",
       "determination, Koala Joe was the reigning champion of the jungle.\n",
       "\n",
       "One sunny afternoon, as the leaves rustled in the breeze, Koala Joe found himself at the\n",
       "local pool hall. The stakes were high, and the competition fierce. A group of camelids,\n",
       "notorious for their cunning strategies and unwavering precision, had challenged Koala Joe\n",
       "to a friendly game.\n",
       "\n",
       "Among the camelids was Camel Bob, a master of spin shots and a master of deception. There\n",
       "was Crocodile Croy, a skilled cue ball master with a lightning-fast draw shot. And\n",
       "finally, there was Giraffe Gerry, a tall and nimble player known for his delicate control\n",
       "and precision.\n",
       "\n",
       "Koala Joe, unfazed, took his position at the table, his eyes focused on the ball. With a\n",
       "swift and powerful backspin, he sent the cue ball whizzing towards the pocket. The ball\n",
       "danced and bounced, defying the odds and finding its way into the desired slot.\n",
       "\n",
       "The camelids were stunned by Koala Joe's precision and speed. One by one, they began to\n",
       "falter, their shots going astray. Koala Joe's skills were unmatched, and his determination\n",
       "was unwavering. He played with a grace and power that left his opponents in awe.\n",
       "\n",
       "As the final shot sank into the pocket, the crowd erupted into cheers. Koala Joe had done\n",
       "it again, proving that he was the king of the pool table, regardless of the odds.\n",
       "\n",
       "The camelids, humbled by their defeat, congratulated Koala Joe. They recognized his talent\n",
       "and admired his spirit. And so, Koala Joe, the koala who could shoot pool like no other,\n",
       "continued to reign supreme, leaving his mark on the jungle forever."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 7.88 ms, total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a story about a Koala playing pool and beating all the camelids.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google, a genius story teller. Write out your with details and make it compelling!\",\n",
    "         max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d12e3c-b172-4098-8ac3-4678c6a27b07",
   "metadata": {},
   "source": [
    "### CodeGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0d7053f-f28c-4e4e-8d84-223f28277342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "import math\n",
       "\n",
       "def print_prime(n):\n",
       "   \"\"\"\n",
       "   Print all primes between 1 and n\n",
       "\n",
       "   Args:\n",
       "       n: The upper bound of the range of numbers to search.\n",
       "\n",
       "   Returns:\n",
       "       None:\n",
       "   \"\"\"\n",
       "\n",
       "   primes = []\n",
       "   for num in range(1, int(n) + 1):\n",
       "       is_prime = True\n",
       "       for p in primes:\n",
       "           if num % p == 0:\n",
       "               is_prime = False\n",
       "       if is_prime and num not in primes:\n",
       "           primes.append(num)\n",
       "\n",
       "   print(primes)\n",
       "```\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "* The function `print_prime` takes an integer `n` as input.\n",
       "* It uses a list called `primes` to store all the primes.\n",
       "* It iterates over the range `1` to `n` using the `for num in range(1, int(n) + 1)` loop.\n",
       "* For each number `num`, it checks if it is prime by iterating over the list of primes\n",
       "stored in `primes` using the `for p in primes` loop.\n",
       "* If `num` is divisible by any prime in `primes`, it is not prime and is skipped.\n",
       "* If `num` is prime and not already in `primes`, it is added to the list.\n",
       "* Finally, the `primes` list is printed.\n",
       "\n",
       "**Example Usage:**\n",
       "\n",
       "```python\n",
       "print_prime(100)\n",
       "```\n",
       "\n",
       "**Output:**\n",
       "\n",
       "```\n",
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 33, 37, 41, 43, 47, 51, 53, 59, 61, 63, 65, 67,\n",
       "69, 71, 73, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 7.97 ms, total: 22.5 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('''```python\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd36876-c3ff-4adf-837d-b71e51035eb3",
   "metadata": {},
   "source": [
    "### Question and Answer Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d100e60-c023-4a83-8b0e-357e1e5c76b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Question:** How does the lack of optimized inventory and staffing in the fulfillment\n",
       "network impact the company's shipping costs and customer service operations?\n",
       "\n",
       "**Answer:** The lack of optimized inventory and staffing in the fulfillment network\n",
       "increases the company's net shipping cost by requiring long-zone or partial shipments. It\n",
       "also makes it difficult to adequately staff fulfillment network and customer service\n",
       "centers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.51 s, sys: 0 ns, total: 3.51 s\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('''In addition, failure to optimize inventory or staffing in our fulfillment network increases our net shipping cost by requiring \n",
    "long-zone or partial shipments. We and our co-sourcers may be unable to adequately staff our fulfillment network and customer service \n",
    "centers.''', system_prompt=\"You are a genius editor, try to generate one qusetion and answer pair for the context:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a23d6-bb55-4518-8558-14ae360382b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
