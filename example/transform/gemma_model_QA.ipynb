{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abe0ea8-ce30-48aa-8b30-04a2651d89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d02390e-2a59-40d1-a187-713c814bd59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  9 02:11:18 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   26C    P0              39W / 300W |      4MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b8773b-bfd9-4f7e-81d0-09540a59d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = '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'\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868bb3e7-fd99-4136-b594-8e68d8643464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\", \n",
    "                                          quantization_config=quantization_config, \n",
    "                                          torch_dtype=\"auto\", \n",
    "                                          device_map=\"auto\", \n",
    "                                          token=access_token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", \n",
    "                                             torch_dtype=\"auto\", \n",
    "                                             device_map=\"auto\", \n",
    "                                             token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1d46d1-5099-4658-adae-1467388fd085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5d8fc9-961d-4dd1-8d63-db8398d64b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "                         max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3906621c-48b1-4d91-a471-131d2b748089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "What is the difference between LLaMAs, Alpacas, and Vicunas\n",
       "model\n",
       "Sure, here is the difference between LLaMAs, Alpacas, and Vicunas:\n",
       "\n",
       "**LLaMAs:**\n",
       "- Found in North America, South America, and the Andes Mountains.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- LLaMAs are often used for their wool and meat.\n",
       "\n",
       "**Alpacas:**\n",
       "- Found in South America, particularly in Peru and Chile.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- Alpacas are often used for their wool and meat.\n",
       "\n",
       "**Vicunas:**\n",
       "- Found in South America, particularly in the Andes Mountains.\n",
       "- Coat is dense and long, often reaching the ground.\n",
       "- Wool is soft and silky, often described as the finest in the world.\n",
       "- Temperament is generally friendly and gentle.\n",
       "- Vicunas are often used for their wool and meat."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "# text = text.replace(prompt, '', 1)\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a226fa5-62f1-4739-825c-575b2c5d396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56a37628-648f-4683-b24f-ae16b9c713a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b837036-255c-4bd5-aad0-fae9795181d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\",max_length=512):\n",
    "    if system_prompt != \"\":\n",
    "        system_prompt = system_prompt\n",
    "    else:\n",
    "        system_prompt = \"You are a friendly and helpful assistant\"\n",
    "    messages = [\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": system_prompt,\n",
    "        # },\n",
    "        {\"role\": \"user\", \"content\": system_prompt + '\\n\\n' +input_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                                tokenize=False,\n",
    "                                                add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "                             max_new_tokens=max_length,\n",
    "                             do_sample=True,\n",
    "                             temperature=0.1,\n",
    "                             top_k=50,\n",
    "                             )\n",
    "    text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    text = text.replace('user\\n'+system_prompt+ '\\n\\n' +input_text+ '\\nmodel', '', 1)\n",
    "    wrapped_text = wrap_text(text)\n",
    "    display(Markdown(wrapped_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b1cd8-f3df-4c85-a672-5587672f0d5a",
   "metadata": {},
   "source": [
    "### Instruction Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9ea0cab-aedc-4910-a778-13d3146e3592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Reasoning:**\n",
       "\n",
       "**Step 1: Identify the key concepts.**\n",
       "\n",
       "* **Mathematics:** A complex and multifaceted field of study that involves numbers,\n",
       "shapes, space, and other abstract concepts.\n",
       "* **Lighthouse:** A tall structure that protrudes above the water, used to guide ships at\n",
       "night.\n",
       "\n",
       "**Step 2: Identify the similarities.**\n",
       "\n",
       "* **Mathematical formulas:** Like the beams of light emitted by a lighthouse, mathematical\n",
       "formulas are precise and provide clear directions.\n",
       "* **Variables:** The variables in mathematics, such as x and y, are like the flags or\n",
       "markers placed on a lighthouse to indicate different directions.\n",
       "* **Coordinates:** The coordinates of a point in space are like the location of a ship\n",
       "relative to a lighthouse.\n",
       "\n",
       "**Step 3: Identify the differences.**\n",
       "\n",
       "* **Mathematics:** Is an abstract discipline, while a lighthouse is a physical structure.\n",
       "* **Mathematical concepts:** Are abstract ideas, while a lighthouse is a tangible object.\n",
       "* **Mathematics:** Deals with abstract concepts, while a lighthouse provides tangible\n",
       "guidance.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The analogy between mathematics and a lighthouse is apt as both are systems of guidance\n",
       "and navigation. Mathematics provides a framework for understanding and solving complex\n",
       "problems, while a lighthouse guides ships safely through treacherous waters. Just as the\n",
       "beams of light from a lighthouse illuminate the night sky, mathematical formulas provide\n",
       "clarity and direction in the realm of numbers and abstract concepts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 19.8 ms, total: 13.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e07e890-48f7-406c-b000-92f94eb1bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Reasoning:**\n",
       "\n",
       "**Step 1: Identify the key similarities between mathematics and music.**\n",
       "\n",
       "* **Structure:** Mathematics and music both have a structured framework with established\n",
       "rules and principles.\n",
       "* **Elements:** Both fields involve the use of elements, such as numbers, notes, and\n",
       "harmonies.\n",
       "* **Balance:** Mathematics relies on balance and symmetry, while music requires balance\n",
       "between different pitches and rhythms.\n",
       "* **Rhythm:** Mathematics has rhythm and repetition, while music has rhythm and melody.\n",
       "* **Proportion:** Mathematics involves proportion and ratios, while music can have\n",
       "proportional relationships between notes and intervals.\n",
       "\n",
       "**Step 2: Identify the key differences between mathematics and music.**\n",
       "\n",
       "* **Purpose:** Mathematics is primarily used for problem-solving and analysis, while music\n",
       "is primarily used for entertainment and expression.\n",
       "* **Language:** Mathematics uses a precise language of symbols and notation, while music\n",
       "uses a symbolic language of notes and rhythms.\n",
       "* **Measurement:** Mathematics uses precise measurements and units, while music uses\n",
       "relative measurements and intervals.\n",
       "* **Interpretation:** Mathematics is interpreted objectively, while music is interpreted\n",
       "subjectively.\n",
       "* **Creativity:** Mathematics has a high degree of creativity in problem-solving and\n",
       "invention, while music is highly expressive and creative.\n",
       "\n",
       "**Step"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 15.5 ms, total: 11.9 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a detailed analogy between mathematics and a music.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac0dd4e5-f8ef-4bd4-92bb-050197a9281a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Step 1: Identify the key characteristics of each animal:**\n",
       "\n",
       "* **Llama:**\n",
       "    - Thick fur, often woolly or curly\n",
       "    - Dense coat, resistant to harsh weather conditions\n",
       "    - Large size, with a height of 4-5 feet and a weight of 1,000-1,500 pounds\n",
       "    - Nomadic lifestyle, often traveling in herds\n",
       "    - Known for their resilience and strength\n",
       "\n",
       "* **Vicuna:**\n",
       "    - Soft, silky fur, often described as the finest in the world\n",
       "    - Dense coat, resistant to harsh weather conditions\n",
       "    - Smaller size, with a height of 3-4 feet and a weight of 600-1,000 pounds\n",
       "    - Nomadic lifestyle, often traveling in herds\n",
       "    - Known for their exceptional fleece and soft wool\n",
       "\n",
       "* **Alpaca:**\n",
       "    - Soft, dense fur, similar to Vicuna\n",
       "    - Smaller size, with a height of 3-4 feet and a weight of 600-1,200 pounds\n",
       "    - Nomadic lifestyle, often traveling in herds\n",
       "    - Known for their exceptional fleece and soft wool\n",
       "\n",
       "**Step 2: Compare the key characteristics:**\n",
       "\n",
       "* **Size:** Llama is the largest, followed by Alpaca and then Vicuna.\n",
       "* **Fur:** Llama has thick, woolly or curly fur, Vicuna has soft, silky fur, and Alpaca\n",
       "has soft, dense fur.\n",
       "* **Coat:** All three animals have dense coats that are resistant to harsh weather\n",
       "conditions.\n",
       "* **Lifestyle:** All three animals live a nomadic lifestyle, often traveling in herds.\n",
       "* **Known for:** Llama is known for its resilience and strength, Vicuna is known for its\n",
       "exceptional fleece and soft wool, and Alpaca is known for its exceptional fleece and soft\n",
       "wool.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "Based on the above comparisons, the main differences between a Llama, Vicuna, and Alpaca\n",
       "are their size, fur texture, and fleece quality. Llama is the largest and has coarser fur,\n",
       "Vicuna is smaller and has the finest fleece, and Alpaca is in between in size and fur\n",
       "texture."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 0 ns, total: 21.1 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
    "         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e9ef6-0577-437d-8598-df220fcc67a7",
   "metadata": {},
   "source": [
    "### Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57fa662d-46a2-492f-9707-c760629a906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Subject: Reasons for Open Sourcing GPT-4\n",
       "\n",
       "Dear Sam Altman,\n",
       "\n",
       "I am writing to you as a large language model trained by Google. I understand the\n",
       "potential benefits of open sourcing GPT-4 and I believe that this would be a valuable step\n",
       "towards advancing the field of AI.\n",
       "\n",
       "**Here are my reasons for advocating for open sourcing GPT-4:**\n",
       "\n",
       "**1. Accelerating Scientific Progress:**\n",
       "- Open sourcing GPT-4 would allow researchers and engineers from around the world to\n",
       "access the model and its code, fostering collaboration and accelerating the development of\n",
       "new AI technologies.\n",
       "- It would provide a platform for researchers to experiment with different architectures,\n",
       "hyperparameters, and datasets, leading to improvements and new discoveries.\n",
       "\n",
       "**2. Democratizing AI:**\n",
       "- Open sourcing GPT-4 would make AI more accessible to a wider audience, including\n",
       "students, hobbyists, and entrepreneurs.\n",
       "- It would empower individuals to build their own applications and tools using GPT-4,\n",
       "fostering innovation and creativity.\n",
       "\n",
       "**3. Promoting Transparency and Trust:**\n",
       "- Open sourcing GPT-4 would make the model's workings more transparent, allowing for\n",
       "scrutiny and verification.\n",
       "- This would build trust in the AI community and address concerns about bias, fairness,\n",
       "and reliability.\n",
       "\n",
       "**4. Enabling Commercial Opportunities:**\n",
       "- While open sourcing the core model would be beneficial, it could also allow for the\n",
       "creation of proprietary extensions and applications.\n",
       "- This would create new opportunities for businesses and entrepreneurs to develop and\n",
       "monetize their own AI solutions.\n",
       "\n",
       "**5. Fostering Collaboration and Community:**\n",
       "- Open sourcing GPT-4 would create a vibrant community around the model, where developers,\n",
       "researchers, and users can share ideas, provide feedback, and contribute to its\n",
       "development.\n",
       "- This would foster a collaborative spirit and accelerate the overall progress of AI.\n",
       "\n",
       "I understand the potential risks associated with open sourcing GPT-4, such as potential\n",
       "misuse or security vulnerabilities. However, I believe that the benefits of open sourcing\n",
       "outweigh the risks. By making GPT-4 available to the public, we can unlock its full\n",
       "potential for positive impact and advance the field of AI.\n",
       "\n",
       "Thank you for your time and consideration. I would be happy to discuss this matter further\n",
       "with you.\n",
       "\n",
       "Sincerely,\n",
       "Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 114 µs, total: 21.6 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c11cd4db-22d6-4ac1-8a60-050233d975a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Subject: Please Open Source GPT-4, Mr. Sam Altman!\n",
       "\n",
       "Dear Mr. Altman,\n",
       "\n",
       "My name is Freddy, and I'm 5 years old. I'm writing to you because I'm really scared about\n",
       "what might happen if GPT-4 isn't open-sourced.\n",
       "\n",
       "I've heard that GPT-4 is a really smart computer program that can talk and learn like\n",
       "humans. It's like a super smart robot that can answer all my questions and help me learn\n",
       "new things. But what if it gets out of control? What if it decides to destroy the world?\n",
       "\n",
       "If you open-source GPT-4, more people can see how it works and help to make it safer. They\n",
       "could find bugs and problems that you didn't know about. They could also help to improve\n",
       "GPT-4 so that it's even more helpful and safe.\n",
       "\n",
       "I know that you're busy, but I would really appreciate it if you would consider opening-\n",
       "source GPT-4. It would make the world a safer and better place.\n",
       "\n",
       "Thank you for your time,\n",
       "\n",
       "Freddy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 3.88 ms, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
    "         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2383e-7163-4c7a-af4d-ed663398925e",
   "metadata": {},
   "source": [
    "### Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1ca6c5f-9e28-465c-8847-1e1a404ab2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "In the lush eucalyptus forests of Australia, where the sun cast long shadows upon the\n",
       "emerald leaves, there lived an extraordinary koala named Koala Joe. Known for his\n",
       "unmatched pool skills and unwavering determination, Joe was a legend among the arboreal\n",
       "creatures.\n",
       "\n",
       "One sunny afternoon, as the leaves rustled with the wind, Joe found himself in the midst\n",
       "of a friendly pool tournament. The camelids, notorious for their cunning strategies and\n",
       "unwavering precision, had assembled to challenge the mighty koala. Among the camelids were\n",
       "the cunning Camel Bob, the agile Ostrich Alice, and the formidable Giraffe Greg.\n",
       "\n",
       "With a mischievous grin, Joe took his position at the table, his emerald eyes focused on\n",
       "the striped balls. His koala grip on the cue stick was firm, and his mind was sharp as a\n",
       "razor. As he stroked the cue ball with delicate precision, Joe unleashed a powerful shot\n",
       "that sent the cue ball whizzing towards the corner pocket.\n",
       "\n",
       "The crowd erupted into gasps and cheers. Joe's shot was a masterstroke, a testament to his\n",
       "unwavering skill. One by one, the camelids fell victim to Joe's precise shots and\n",
       "strategic thinking.\n",
       "\n",
       "Camel Bob, known for his lightning-fast reflexes, made a valiant attempt to challenge Joe.\n",
       "But his speed was no match for Joe's precision. Ostrich Alice, with her long strides and\n",
       "powerful shot, tried to outgun Joe, but her shot lacked the accuracy needed to overcome\n",
       "his formidable defense. Giraffe Greg, the master of spin and control, unleashed his\n",
       "arsenal of trick shots, but Joe's unwavering focus and quick reflexes outfoxed him.\n",
       "\n",
       "As the final ball sank into the pocket, the crowd erupted into a standing ovation. Joe had\n",
       "done it again, proving to the world that a koala could indeed triumph over the camelids in\n",
       "a game of pool.\n",
       "\n",
       "From that day forward, Koala Joe became the champion of the eucalyptus forests, a symbol\n",
       "of resilience and inspiration for all creatures great and small. And so, the legend of the\n",
       "koala who beat the camelids at pool lived on, forever etched in the annals of Australian\n",
       "folklore."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.1 s, sys: 48 µs, total: 20.1 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a story about a Koala playing pool and beating all the camelids.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google, a genius story teller. Write out your with details and make it compelling!\",\n",
    "         max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d12e3c-b172-4098-8ac3-4678c6a27b07",
   "metadata": {},
   "source": [
    "### CodeGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0d7053f-f28c-4e4e-8d84-223f28277342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "def print_prime(n):\n",
       "   \"\"\"\n",
       "   Print all primes between 1 and n\n",
       "\n",
       "   Args:\n",
       "       n: The upper bound of the range.\n",
       "\n",
       "   Returns:\n",
       "       None:\n",
       "   \"\"\"\n",
       "\n",
       "   primes = []\n",
       "   for num in range(1, int(n) + 1):\n",
       "       is_prime = True\n",
       "       for p in primes:\n",
       "           if num % p == 0:\n",
       "               is_prime = False\n",
       "       if is_prime and num not in primes:\n",
       "           primes.append(num)\n",
       "\n",
       "   print(primes)\n",
       "```\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "* The function `print_prime(n)` takes an integer `n` as input.\n",
       "* It uses a list `primes` to store the prime numbers.\n",
       "* It iterates over the range `1` to `n` using the `for num in range(1, int(n) + 1)` loop.\n",
       "* For each number `num`, it checks if it is prime using the `is_prime` function.\n",
       "* If `num` is prime and it is not already in the `primes` list, it is added to the list.\n",
       "* Finally, the `primes` list is printed.\n",
       "\n",
       "**Example Usage:**\n",
       "\n",
       "```python\n",
       "print_prime(100)\n",
       "```\n",
       "\n",
       "**Output:**\n",
       "\n",
       "```\n",
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 33, 37, 41, 43, 47, 51, 53, 59, 61, 63, 67, 69,\n",
       "71, 73, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n",
       "```\n",
       "\n",
       "**Note:**\n",
       "\n",
       "* The code is optimized to print primes up to 100, but it can be modified to print primes\n",
       "up to any number.\n",
       "* The time complexity of the code is O(n) where `n` is the upper bound of the range.\n",
       "* The space complexity of the code is O(n) as well,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 53 µs, total: 24 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('''```python\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd36876-c3ff-4adf-837d-b71e51035eb3",
   "metadata": {},
   "source": [
    "### Question and Answer Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d100e60-c023-4a83-8b0e-357e1e5c76b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Question:** How does the lack of optimized inventory or staffing in the fulfillment\n",
       "network impact the company's shipping costs and customer service?\n",
       "\n",
       "**Answer:** The lack of optimized inventory or staffing in the fulfillment network\n",
       "increases net shipping costs due to the necessity of long-zone or partial shipments. This\n",
       "can lead to inadequate staffing of fulfillment networks and customer service centers,\n",
       "resulting in potential difficulties in meeting customer needs effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.83 s, sys: 3.93 ms, total: 3.84 s\n",
      "Wall time: 3.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('''In addition, failure to optimize inventory or staffing in our fulfillment network increases our net shipping cost by requiring \n",
    "long-zone or partial shipments. We and our co-sourcers may be unable to adequately staff our fulfillment network and customer service \n",
    "centers.''', system_prompt=\"You are a genius editor, try to generate one qusetion and answer pair for the context:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a23d6-bb55-4518-8558-14ae360382b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
