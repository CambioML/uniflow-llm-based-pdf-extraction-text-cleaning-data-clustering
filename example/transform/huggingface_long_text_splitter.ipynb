{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Using TransformConfig with Long Text\n",
    "\n",
    "In this example, we demonstrate how to leverage the Transform flow in handling long text that exceeds the token limitation of the ChatGPT API. There are two TransformConfigs for HuggingFace that you can add the parameter `auto_split_long_text`\n",
    "- TransformQAHuggingFaceConfig\n",
    "- TransformQAHuggingFaceJsonFormatConfig\n",
    "\n",
    "We use the text from [War and Peace](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt) as our example.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "We store the \"War and Peace\" text file in the `data/raw_input` directory as \"book-war-and-peace.txt\". You can download the file from [here](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa945f",
   "metadata": {},
   "source": [
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf03f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92290428",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4891fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bdd76b",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformForGenerationOpenAIGPT3p5Config\n",
    "from uniflow.flow.config import HuggingfaceModelConfig, TransformQAHuggingFaceJsonFormatConfig\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccd3d3",
   "metadata": {},
   "source": [
    "### Generate Context object with long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe82668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Context 1 (Preview): 'arn you, if you don't tell me that this means war,...'\n",
      "---\n",
      "---\n",
      "Context 2 (Preview): ' as an emperor. So it\n",
      "seems to me.\"\n",
      "\n",
      "\"Yes, yes, of...'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def extract_contexts(file_path, ranges):\n",
    "    # Read the entire file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Initialize the list for holding contexts\n",
    "    contexts = []\n",
    "\n",
    "    # Extract specified ranges\n",
    "    for start, end in ranges:\n",
    "        # Adjusting end index to fit within the file length if necessary\n",
    "        end = min(end, len(content))\n",
    "        context_text = content[start:end]\n",
    "        contexts.append(Context(context=context_text))\n",
    "\n",
    "    return contexts\n",
    "\n",
    "# Define the ranges for the contexts\n",
    "ranges = [(100, 32000), (32000+12600, 32000+13200)]\n",
    "\n",
    "# Specify the file path\n",
    "file_path = './data/raw_input/book-war-and-peace.txt'\n",
    "\n",
    "# Extract contexts\n",
    "contexts = extract_contexts(file_path, ranges)\n",
    "\n",
    "for i, context in enumerate(contexts):\n",
    "    preview_text = context.context[:50] + \"...\" if len(context.context) > 50 else context.context\n",
    "    print(f\"---\\nContext {i+1} (Preview): '{preview_text}'\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb0aa1",
   "metadata": {},
   "source": [
    "### Prepare sample prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b531babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
    "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
    "    few_shot_prompt=[\n",
    "        Context(\n",
    "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "            answer=\"Claude E. Shannon.\",\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"\"\"The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantum for InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements;\"\"\",\n",
    "            question=\"What does the Compute & Networking segment include?\",\n",
    "            answer=\"\"\"The Compute & Networking segment includes Data Center accelerated computing platforms, end-to-end networking platforms (Quantum for InfiniBand and Spectrum for Ethernet), the NVIDIA DRIVE automated-driving platform, and automotive development agreements.\"\"\",\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9fbce",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `TransformQAHuggingFaceJsonFormatConfig` to use our customized instructions and examples, instead of the `uniflow` default ones.\n",
    "\n",
    "We also want to get the response in the `json` format instead of the `text` default, so we set the `response_format` to `json_object`.\n",
    "\n",
    "Please note that we include the `auto_split_long_text` parameter in the transform configuration. This ensures that if a `Context` object contains text exceeding the specified token length limit, it will automatically be split into multiple `Context` objects. Each of these objects will contain text segments that adhere to the limit, ready for submission to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f49ed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "config = TransformQAHuggingFaceJsonFormatConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=HuggingfaceModelConfig(\n",
    "        batch_size=4,\n",
    "        response_start_key=\"question\", response_format={\"type\": \"json_object\"},\n",
    "    ),\n",
    "    auto_split_long_text=True\n",
    ")\n",
    "\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b335f6",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15cba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                   | 0/1 [00:00<?, ?it/s]/opt/conda/envs/uniflow/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.94s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = client.run(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230239f6",
   "metadata": {},
   "source": [
    "### Display the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d4f983-e641-4de9-b159-3118711ba96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <hr/>\n",
       "        <div style=\"background-color: #ffffff; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Context:</b> arn you, if you don't tell me that this means war,\n",
       "if you still try to defend the infamies and horrors perpetrated by that\n",
       "Antichrist--I really believe he is Antichrist--I will have nothing more\n",
       "to do with you and you are no longer my friend, no longer my 'faithful\n",
       "slave,' as you call yourself! But how do you do? I see I have frightened\n",
       "you--sit down and tell me all the news.\"It was in July, 1805, and the speaker was the well-known Anna Pavlovna\n",
       "Scherer, maid of honor and favorite of the Empress ...\n",
       "        </div>\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who was the mysterious guest named Pierre in Anna Pavlovna's drawing room?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Pierre was an illegitimate son of Count Bezukhov, a well-known grandee of Catherine's time who was dying in Moscow. He had recently returned from abroad and was making his first appearance in society.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <hr/>\n",
       "        <div style=\"background-color: #ffffff; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Context:</b> Anna Pavlovna's alarm was justified, for Pierre turned away from the\n",
       "aunt without waiting to hear her speech about Her Majesty's health. Anna\n",
       "Pavlovna in dismay detained him with the words: \"Do you know the Abbe\n",
       "Morio? He is a most interesting man.\"\"Yes, I have heard of his scheme for perpetual peace, and it is very\n",
       "interesting but hardly feasible.\"\"You think so?\" rejoined Anna Pavlovna in order to say something and get\n",
       "away to attend to her duties as hostess. But Pierre now committed a\n",
       "reverse  ...\n",
       "        </div>\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who played the role of the maitre d'hotel in Anna Pavlovna's reception, serving up the vicomte and the abbe as special treats to her guests?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Anna Pavlovna herself.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <hr/>\n",
       "        <div style=\"background-color: #ffffff; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Context:</b> as an emperor. So it\n",
       "seems to me.\"\n",
       "\n",
       "\"Yes, yes, of course!\" Pierre chimed in, pleased at the arrival of this\n",
       "reinforcement.\n",
       "\n",
       "\"One must admit,\" continued Prince Andrew, \"that Napoleon as a man was\n",
       "great on the bridge of Arcola, and in the hospital at Jaffa where he\n",
       "gave his hand to the plague-stricken; but... but there are other acts\n",
       "which it is difficult to justify.\"\n",
       "\n",
       "Prince Andrew, who had evidently wished to tone down the awkwardness of\n",
       "Pierre's remarks, rose and made a sign to his wife that it ...\n",
       "        </div>\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who did Pierre refer to as being great on the bridge of Arcola and in the hospital at Jaffer, despite some difficult-to-justify actions?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Napoleon.\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define a function to wrap text in HTML tags with style\n",
    "def format_html_question_answer(context, question, answer):\n",
    "    return f\"\"\"\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <hr/>\n",
    "        <div style=\"background-color: #ffffff; padding: 10px; border-radius: 5px;\">\n",
    "            <b>Context:</b> {context} ...\n",
    "        </div>\n",
    "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
    "            <b>Question:</b> {question}\n",
    "        </div>\n",
    "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
    "            <b>Answer:</b> {answer}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_output = \"\"\n",
    "for o in outputs[0][\"output\"][0][\"response\"]:\n",
    "    response = o\n",
    "    # Split the response based on the first occurrence of '\\nanswer'\n",
    "    context = response['context'][:500]\n",
    "    question = response['question']\n",
    "    answer = response['answer']\n",
    "    html_output += format_html_question_answer(context, question, answer)\n",
    "\n",
    "# Display the formatted HTML\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f961c1",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
