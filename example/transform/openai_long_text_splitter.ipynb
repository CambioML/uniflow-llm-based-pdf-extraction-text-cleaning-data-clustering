{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Using TransformConfig with Long Text\n",
    "\n",
    "In this example, we demonstrate how to leverage the Transform flow in handling long text that exceeds the token limitation of the ChatGPT API. There are three TransformConfigs for OpenAI that you can add the parameter `auto_split_long_text`\n",
    "- TransformOpenAIConfig\n",
    "- TransformForGenerationOpenAIGPT3p5Config\n",
    "- <del>TransformForClusteringOpenAIGPT4Config</del>\n",
    "\n",
    "We use the text from [War and Peace](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt) as our example.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we store the \"War and Peace\" text file in the `data/raw_input` directory as \"book-war-and-peace.txt\". You can download the file from [here](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12b5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenzhihan/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'cl100k_base'>\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformForGenerationOpenAIGPT3p5Config\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccd3d3",
   "metadata": {},
   "source": [
    "### Generate Context object with long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe82668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Context 1 (Preview): 'arn you, if you don't tell me that this means war,...'\n",
      "---\n",
      "---\n",
      "Context 2 (Preview): 'nly she jumped up onto a tub to be higher than he,...'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def extract_contexts(file_path, ranges):\n",
    "    # Read the entire file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Initialize the list for holding contexts\n",
    "    contexts = []\n",
    "\n",
    "    # Extract specified ranges\n",
    "    for start, end in ranges:\n",
    "        # Adjusting end index to fit within the file length if necessary\n",
    "        end = min(end, len(content))\n",
    "        context_text = content[start:end]\n",
    "        contexts.append(Context(context=context_text))\n",
    "\n",
    "    return contexts\n",
    "\n",
    "# Define the ranges for the contexts\n",
    "ranges = [(100, 92000), (92000+12600, 92000+13200)]\n",
    "\n",
    "# Specify the file path\n",
    "file_path = './data/raw_input/book-war-and-peace.txt'\n",
    "\n",
    "# Extract contexts\n",
    "contexts = extract_contexts(file_path, ranges)\n",
    "\n",
    "for i, context in enumerate(contexts):\n",
    "    preview_text = context.context[:50] + \"...\" if len(context.context) > 50 else context.context\n",
    "    print(f\"---\\nContext {i+1} (Preview): '{preview_text}'\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
    "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
    "    few_shot_prompt=[\n",
    "        Context(\n",
    "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "            answer=\"Claude E. Shannon.\",\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"\"\"The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantum for InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements;\"\"\",\n",
    "            question=\"What does the Compute & Networking segment include?\",\n",
    "            answer=\"\"\"The Compute & Networking segment includes Data Center accelerated computing platforms, end-to-end networking platforms (Quantum for InfiniBand and Spectrum for Ethernet), the NVIDIA DRIVE automated-driving platform, and automotive development agreements.\"\"\",\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17)'s default LLM to generate questions and answers.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `OpenAIConfig` to use our customized instructions and examples, instead of the `uniflow` default ones.\n",
    "\n",
    "We also want to get the response in the `json` format instead of the `text` default, so we set the `response_format` to `json_object`.\n",
    "\n",
    "Please note that we include the `auto_split_long_text` parameter in the transform configuration. This ensures that if a `Context` object contains text exceeding the specified token length limit, it will automatically be split into multiple `Context` objects. Each of these objects will contain text segments that adhere to the limit, ready for submission to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformForGenerationOpenAIGPT3p5Config(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=OpenAIModelConfig(),\n",
    "    auto_split_long_text=True\n",
    ")\n",
    "\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56277ec3d40545779e8cf3de3ad3f0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = client.run(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230239f6",
   "metadata": {},
   "source": [
    "### Display the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ccffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who is the illegitimate son of Count Bezukhov?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The illegitimate son of Count Bezukhov is Pierre.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who entered the drawing room just as the vicomte was telling his tale and engaged in conversation with Prince Andrew?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Prince Andrew Bolkonski, the little princess' husband.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> What did Prince Andrew ask his wife as they were leaving the soiree?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> \"Are you ready?\"\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> What is the dissipated life that Pierre is sharing with Prince Vasili Kuragin's son Anatole?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Pierre is staying at Prince Vasili Kuragin's and sharing the dissipated life of his son Anatole, which includes debauchery and dissipation.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> What type of activity were the guests engaged in at the large house near the Horse Guards' barracks?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The guests were engaged in playing cards and having supper. Some of the young men were also betting and engaging in rowdy behavior.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who are the members of the younger generation in the drawing room?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The younger generation in the drawing room includes Boris, the officer, Anna Mikhaylovna's son; Nicholas, the undergraduate, the count's eldest son; Sonya, the count's fifteen-year-old niece, and little Petya, his youngest boy.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> How many years does the man say he will ask for Natasha's hand in marriage?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> In another four years.\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define a function to wrap text in HTML tags with style\n",
    "def format_html_question_answer(question, answer):\n",
    "    return f\"\"\"\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
    "            <b>Question:</b> {question}\n",
    "        </div>\n",
    "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
    "            <b>Answer:</b> {answer}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_output = \"\"\n",
    "for o in output:\n",
    "    response = o['output'][0]['response'][0]\n",
    "    # Split the response based on the first occurrence of '\\nanswer'\n",
    "    split_index = response.find('\\nanswer')\n",
    "    question = response[:split_index].replace('question: ', '')\n",
    "    answer = response[split_index:].replace('\\nanswer: ', '')\n",
    "    html_output += format_html_question_answer(question, answer)\n",
    "\n",
    "# Display the formatted HTML\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b67b7",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "- When splitter using token length for this case (Token length for each context: 4096): 100%|██████████| 7/7 [00:09<00:00,  1.33s/it]\n",
    "\n",
    "- When splitter using char length for this case (Char length for each context: 4096): 100%|██████████| 25/25 [00:27<00:00,  1.11s/it]\n",
    "\n",
    "(We are using the splitter based on token length for config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
